---
title: 'Supervised Learning in R: Classification'
author: "Hanh Nguyen"
date: "4/28/2020"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```


- Machine learning utilizes computers to turn data into insight and action.
- This course focuses on training a machine to learn from prior examples

- *Classification*: the task of learning a concept which is a set of categories.
    ex: classification tasks for driverless cars: when a vehicle's camera observes an object (a stop sign),
                                                    it must classify object before it can react.

___________________________________________
***Classification with Nearest Neighbors***
___________________________________________


- A nearest neighbor classifiers take advantage of the fact that signs that look alike should be similar to, 
  or 'nearby' other signs of the same type.
- A nearest neighbor learner decide whether two signs are similar by *measuring the distance btween them in feature space*
    ex: color of each sign
- k-Nearest Neighbors (kNN) - an algorithm uses the principle of nearest neighbor to classify unlabeled examples.
- By default, in R, *knn()* searches a dataset for the historic observation most similar to the newly-observed one.

***
library(class)
pred <- knn(training_data, testing_data, training_labels)
***

**Practice**

After several trips with a human behind the wheel, it is time for the self-driving car to attempt the test course alone.
As it begins to drive away, its camera captures the following image:

![knn_stop_99](http://s3.amazonaws.com/assets.datacamp.com/production/course_2906/datasets/knn_stop_99.gif)

Can you apply a kNN classifier to help the car recognize this sign?

```{r}
# load packages: library(class) and library(tidyverse)
# load dataset:
signs <- read.csv("https://assets.datacamp.com/production/repositories/718/datasets/c274ea22cc3d7e12d7bb9fdc9c2bdabe9ab025f4/knn_traffic_signs.csv")
signs <- signs[, !names(signs) %in% c('id', 'sample')]
signs
# dataframe
next_sign = signs[nrow(signs), -1] 
next_sign
# Create a vector of sign labels to use with kNN by extracting the column *sign_type* from *signs*.
sign_types <- signs$sign_type
sign_types
# Classify the next sign observed
knn(signs[-1], next_sign, cl=sign_types)
```

To better understand how the knn() function was able to classify the stop sign, it 
may help to examine the training dataset it used.
Each previous observed street sign was divided into a 4x4 grid, and the red, green, and blue level for each of the 16 center pixels is recorded as illustrated here.

![knn_sign_data](http://s3.amazonaws.com/assets.datacamp.com/production/course_2906/datasets/knn_sign_data.png)

The result is a dataset that records the sign_type as well as 16x3=48 colors properties of each sign

```{r}
signs <- read.csv("https://assets.datacamp.com/production/repositories/718/datasets/c274ea22cc3d7e12d7bb9fdc9c2bdabe9ab025f4/knn_traffic_signs.csv")
signs <- signs[, !names(signs) %in% c('id', 'sample')]
signs
#Examine the structure of the signs dataset
str(signs)
# Count the number of signs of each type
table(signs$sign_type)
# Check r10's average red level by sign type
aggregate(r10 ~ sign_type, data = signs, mean)
```

Now that the autonomous vehicle has successfully stopped on its own, your team 
feels confident allowing the car to continue the test course.
The test course includes 59 additional road signs divided into three types:

![knn_stop_28](http://s3.amazonaws.com/assets.datacamp.com/production/course_2906/datasets/knn_stop_28.gif)

![knn_speed_55](http://s3.amazonaws.com/assets.datacamp.com/production/course_2906/datasets/knn_speed_55.gif)

![knn_peds_47](http://s3.amazonaws.com/assets.datacamp.com/production/course_2906/datasets/knn_peds_47.gif)

At the conclusion of the trial, you are asked to measure the car's overall performance 
at recognizeing these signs.

The class package and the dataset *signs* are already loaded in your workspace. So is the dataframe *test_signs*, which holds a set of observations you'll test your model on.
- Classify the test_signs data using knn().
    . Set train equal to the observations in signs without labels.
    . Use test_signs for the test argument, again without labels.
    . For the cl argument, use the vector of labels provided for you.
- Use table() to explore the classifier's performance at identifying the three 
  sign types (the confusion matrix).
    . Create the vector signs_actual by extracting the labels from test_signs.
    . Pass the vector of predictions and the vector of actual signs to table() 
      to cross tabulate them.
- Compute the overall accuracy of the kNN learner using the mean() function.

```{r}
signs <- read.csv("https://assets.datacamp.com/production/repositories/718/datasets/c274ea22cc3d7e12d7bb9fdc9c2bdabe9ab025f4/knn_traffic_signs.csv", header=TRUE)
(test_signs <- signs[signs$sample == "test", ])                            # remove all item when sample column has "test" value
test_signs <- test_signs [, !names(test_signs) %in% c('id','sample')]      # remove column id and sample in signs
signs <- signs[,!names(signs) %in% c('id', 'sample')]

# Use kNN to identify the test road signs
signs
sign_types <- signs$sign_type                                               # signs$sign_type is column targeted for prediction
signs_pred <- knn(train = signs[-1], test = test_signs[-1], cl = sign_types) # train=original-data, test=test-dataframe, cl=target-column-for-prediction

# Create a confusion matrix of the predicted versus actual values
test_signs
signs_actual <- test_signs$sign_type
signs_actual
table(signs_pred, signs_actual)

# Compute the accuracy
mean(signs_pred == signs_actual)
```

________________
***'k' in kNN***
________________

- The letter k is a variable that specifies the number of neighbors to consider when 
  making the classification -> determining the size of the neighborhoods.
- By default, k=1 in R, meaning only the single nearest, most similar neighbor was used to 
  classify the unlabeled example.
- Choosing the value of k may have a substantial impact on the performance of our classifier.
- Bigger 'k' is not always better
      . A small 'k' creates very small neighborhoods, the classifier is able to discover very 
        subtle pattern.
      . A larger 'k' can ignore some potentially-noisy points in an effort to discover a
        broader, more general pattern
- Choosing 'k':
      . Choosing 'k' by the square root of the number of observaions in the training data.
            ~ ex: if the car has observed 100 previous road signs, you might set k to 10.
      . Choosing 'k' by testing several different values of k and compare the performance on
        data it has not seen before.
        
**Practice**
. Testing other 'k' values .
Setting a k parameter allows the algorithm to consider additional nearby neighbors. 
This enlarges the collection of neighbors which will vote on the predicted class.

Compare k values of 1, 7, and 15 to examine the impact on traffic sign classification accuracy.
  - Compute the accuracy of the default k = 1 model using the given code, 
    then find the accuracy of the model using mean() to compare signs_actual 
    and the model's predictions.
  - Modify the knn() function call by setting k = 7 and again find accuracy value.
  - Revise the code once more by setting k = 15, plus find the accuracy value one more time.

```{r}
# library(class) to load knn() function
signs <- read.csv("https://assets.datacamp.com/production/repositories/718/datasets/c274ea22cc3d7e12d7bb9fdc9c2bdabe9ab025f4/knn_traffic_signs.csv", header=TRUE)
(test_signs <- signs[signs$sample == "test", ])                            # remove all item when sample column has "test" value
test_signs <- test_signs [, !names(test_signs) %in% c('id','sample')]      # remove column id and sample in signs
signs <- signs[,!names(signs) %in% c('id', 'sample')]
sign_types <- signs$sign_type   

# Compute the accuracy of the baseline model (default k = 1)
k_1 <- knn(train = signs[-1], test = test_signs[-1], cl = sign_types)
mean(signs_actual == k_1)

# Modify the above to set k = 7
k_7 <- knn(train = signs[-1], test = test_signs[-1], cl = sign_types, k = 7)
mean(signs_actual == k_7)

# Set k = 15 and compare to the above
k_15 <- knn(train = signs[-1], test = test_signs[-1], cl = sign_types, k = 15)
mean(signs_actual == k_15)

```
. Seeing how the neighbors voted .
When multiple nearest neighbors hold a vote, it can sometimes be useful to examine
whether the voters were unanimous or widely separated.

For example, knowing more about the voters' confidence in the classification could
allow an autonomous vehicle to use caution in the case there is any chance at all
that a stop sign is ahead.

In this exercise, you will learn how to obtain the voting results from the knn() function.

  - Build a kNN model with the prob = TRUE parameter to compute the vote
    proportions. Set k = 7.
  - Use the attr() function to obtain the vote proportions for the predicted
    class. These are stored in the attribute "prob".
  - Examine the first several vote outcomes and percentages using the head()
    function to see how the confidence varies from sign to sign.
```{r}
signs <- read.csv("https://assets.datacamp.com/production/repositories/718/datasets/c274ea22cc3d7e12d7bb9fdc9c2bdabe9ab025f4/knn_traffic_signs.csv", header=TRUE)
(test_signs <- signs[signs$sample == "test", ])                            # remove all item when sample column has "test" value
test_signs <- test_signs [, !names(test_signs) %in% c('id','sample')]      # remove column id and sample in signs
signs <- signs[,!names(signs) %in% c('id', 'sample')]
sign_types <- signs$sign_type   

# Use the prob parameter to get the proportion of votes for the winning class
sign_pred <- knn(train=signs[-1], test=test_signs[-1], cl=sign_types, prob=TRUE, k=7)

# Get the "prob" attribute from the predicted classes
sign_prob <- attr(sign_pred, "prob")

# Examine the first several predictions
head(sign_pred)

# Examine the proportion of votes for the winning class
head(sign_prob)
```

________________________________
*** Data preparation for kNN ***
________________________________
- kNN assumes your data in numeric format as it is difficult to define the data
  distance between catefories.
    . Dummy coding - using 1/10 indicators to represent the categories 
        ~ setting '1' if the category applies
        ~ setting '0' if the category does not apply
        --> categories with the same coding will be closer in distance than the others.
- When calculating distance, each  feature of the input data should be measured with the
  same range of values
- Normalizing data in R - rescale data to a given range - performing min-max normalization.
```{r}
    # define a min-max normalize() function
    normalize <- function(x) { return( (x - min(x)) / (max(x) - min(x)) ) }
    
    # normalized version of r1
    summary(normalize(signs$r1))
    
    # un-normalized version of r1
    summary(signs$r1)
```

