---
title: 'Supervised Learning in R: Classification'
author: "Hanh Nguyen"
date: "4/28/2020"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```


- Machine learning utilizes computers to turn data into insight and action.
- This course focuses on training a machine to learn from prior examples

- *Classification*: the task of learning a concept which is a set of categories.
    ex: classification tasks for driverless cars: when a vehicle's camera observes an object (a stop sign),
                                                    it must classify object before it can react.


***
### PART 1: KNN ###
***

___________________________________________
***Classification with Nearest Neighbors***
___________________________________________

- A nearest neighbor classifiers take advantage of the fact that signs that look alike should be similar to, 
  or 'nearby' other signs of the same type.
- A nearest neighbor learner decide whether two signs are similar by *measuring the distance btween them in feature space*
    ex: color of each sign
- k-Nearest Neighbors (kNN) - an algorithm uses the principle of nearest neighbor to classify unlabeled examples.
- By default, in R, *knn()* searches a dataset for the historic observation most similar to the newly-observed one.

***
library(class)
pred <- knn(training_data, testing_data, training_labels)
***

**Practice**

After several trips with a human behind the wheel, it is time for the self-driving car to attempt the test course alone.
As it begins to drive away, its camera captures the following image:

![knn_stop_99](http://s3.amazonaws.com/assets.datacamp.com/production/course_2906/datasets/knn_stop_99.gif)

Can you apply a kNN classifier to help the car recognize this sign?

```{r}
# load packages: library(class) and library(tidyverse)
# load dataset:
signs <- read.csv("https://assets.datacamp.com/production/repositories/718/datasets/c274ea22cc3d7e12d7bb9fdc9c2bdabe9ab025f4/knn_traffic_signs.csv")
signs <- signs[, !names(signs) %in% c('id', 'sample')]
signs
# dataframe
next_sign = signs[nrow(signs), -1] 
next_sign
# Create a vector of sign labels to use with kNN by extracting the column *sign_type* from *signs*.
sign_types <- signs$sign_type
sign_types
# Classify the next sign observed
knn(signs[-1], next_sign, cl=sign_types)
```

To better understand how the knn() function was able to classify the stop sign, it 
may help to examine the training dataset it used.
Each previous observed street sign was divided into a 4x4 grid, and the red, green, and blue level for each of the 16 center pixels is recorded as illustrated here.

![knn_sign_data](http://s3.amazonaws.com/assets.datacamp.com/production/course_2906/datasets/knn_sign_data.png)

The result is a dataset that records the sign_type as well as 16x3=48 colors properties of each sign

```{r}
signs <- read.csv("https://assets.datacamp.com/production/repositories/718/datasets/c274ea22cc3d7e12d7bb9fdc9c2bdabe9ab025f4/knn_traffic_signs.csv")
signs <- signs[, !names(signs) %in% c('id', 'sample')]
signs
#Examine the structure of the signs dataset
str(signs)
# Count the number of signs of each type
table(signs$sign_type)
# Check r10's average red level by sign type
aggregate(r10 ~ sign_type, data = signs, mean)
```

Now that the autonomous vehicle has successfully stopped on its own, your team 
feels confident allowing the car to continue the test course.
The test course includes 59 additional road signs divided into three types:

![knn_stop_28](http://s3.amazonaws.com/assets.datacamp.com/production/course_2906/datasets/knn_stop_28.gif)

![knn_speed_55](http://s3.amazonaws.com/assets.datacamp.com/production/course_2906/datasets/knn_speed_55.gif)

![knn_peds_47](http://s3.amazonaws.com/assets.datacamp.com/production/course_2906/datasets/knn_peds_47.gif)

At the conclusion of the trial, you are asked to measure the car's overall performance 
at recognizeing these signs.

The class package and the dataset *signs* are already loaded in your workspace. So is the dataframe *test_signs*, which holds a set of observations you'll test your model on.
- Classify the test_signs data using knn().
    . Set train equal to the observations in signs without labels.
    . Use test_signs for the test argument, again without labels.
    . For the cl argument, use the vector of labels provided for you.
- Use table() to explore the classifier's performance at identifying the three 
  sign types (the confusion matrix).
    . Create the vector signs_actual by extracting the labels from test_signs.
    . Pass the vector of predictions and the vector of actual signs to table() 
      to cross tabulate them.
- Compute the overall accuracy of the kNN learner using the mean() function.

```{r}
signs <- read.csv("https://assets.datacamp.com/production/repositories/718/datasets/c274ea22cc3d7e12d7bb9fdc9c2bdabe9ab025f4/knn_traffic_signs.csv", header=TRUE)
(test_signs <- signs[signs$sample == "test", ])                            # remove all item when sample column has "test" value
test_signs <- test_signs [, !names(test_signs) %in% c('id','sample')]      # remove column id and sample in signs
signs <- signs[,!names(signs) %in% c('id', 'sample')]

# Use kNN to identify the test road signs
signs
sign_types <- signs$sign_type                                               # signs$sign_type is column targeted for prediction
signs_pred <- knn(train = signs[-1], test = test_signs[-1], cl = sign_types) # train=original-data, test=test-dataframe, cl=target-column-for-prediction

# Create a confusion matrix of the predicted versus actual values
test_signs
signs_actual <- test_signs$sign_type
signs_actual
table(signs_pred, signs_actual)

# Compute the accuracy
mean(signs_pred == signs_actual)
```

________________
***'k' in kNN***
________________

- The letter k is a variable that specifies the number of neighbors to consider when 
  making the classification -> determining the size of the neighborhoods.
- By default, k=1 in R, meaning only the single nearest, most similar neighbor was used to 
  classify the unlabeled example.
- Choosing the value of k may have a substantial impact on the performance of our classifier.
- Bigger 'k' is not always better
      . A small 'k' creates very small neighborhoods, the classifier is able to discover very 
        subtle pattern.
      . A larger 'k' can ignore some potentially-noisy points in an effort to discover a
        broader, more general pattern
- Choosing 'k':
      . Choosing 'k' by the square root of the number of observaions in the training data.
            ~ ex: if the car has observed 100 previous road signs, you might set k to 10.
      . Choosing 'k' by testing several different values of k and compare the performance on
        data it has not seen before.
        
**Practice**
. Testing other 'k' values .
Setting a k parameter allows the algorithm to consider additional nearby neighbors. 
This enlarges the collection of neighbors which will vote on the predicted class.

Compare k values of 1, 7, and 15 to examine the impact on traffic sign classification accuracy.
  - Compute the accuracy of the default k = 1 model using the given code, 
    then find the accuracy of the model using mean() to compare signs_actual 
    and the model's predictions.
  - Modify the knn() function call by setting k = 7 and again find accuracy value.
  - Revise the code once more by setting k = 15, plus find the accuracy value one more time.

```{r}
# library(class) to load knn() function
signs <- read.csv("https://assets.datacamp.com/production/repositories/718/datasets/c274ea22cc3d7e12d7bb9fdc9c2bdabe9ab025f4/knn_traffic_signs.csv", header=TRUE)
(test_signs <- signs[signs$sample == "test", ])                            # remove all item when sample column has "test" value
test_signs <- test_signs [, !names(test_signs) %in% c('id','sample')]      # remove column id and sample in signs
signs <- signs[,!names(signs) %in% c('id', 'sample')]
sign_types <- signs$sign_type   

# Compute the accuracy of the baseline model (default k = 1)
k_1 <- knn(train = signs[-1], test = test_signs[-1], cl = sign_types)
mean(signs_actual == k_1)

# Modify the above to set k = 7
k_7 <- knn(train = signs[-1], test = test_signs[-1], cl = sign_types, k = 7)
mean(signs_actual == k_7)

# Set k = 15 and compare to the above
k_15 <- knn(train = signs[-1], test = test_signs[-1], cl = sign_types, k = 15)
mean(signs_actual == k_15)

```
. Seeing how the neighbors voted .
When multiple nearest neighbors hold a vote, it can sometimes be useful to examine
whether the voters were unanimous or widely separated.

For example, knowing more about the voters' confidence in the classification could
allow an autonomous vehicle to use caution in the case there is any chance at all
that a stop sign is ahead.

In this exercise, you will learn how to obtain the voting results from the knn() function.

  - Build a kNN model with the prob = TRUE parameter to compute the vote
    proportions. Set k = 7.
  - Use the attr() function to obtain the vote proportions for the predicted
    class. These are stored in the attribute "prob".
  - Examine the first several vote outcomes and percentages using the head()
    function to see how the confidence varies from sign to sign.
```{r}
signs <- read.csv("https://assets.datacamp.com/production/repositories/718/datasets/c274ea22cc3d7e12d7bb9fdc9c2bdabe9ab025f4/knn_traffic_signs.csv", header=TRUE)
(test_signs <- signs[signs$sample == "test", ])                            # remove all item when sample column has "test" value
test_signs <- test_signs [, !names(test_signs) %in% c('id','sample')]      # remove column id and sample in signs
signs <- signs[,!names(signs) %in% c('id', 'sample')]
sign_types <- signs$sign_type   

# Use the prob parameter to get the proportion of votes for the winning class
sign_pred <- knn(train=signs[-1], test=test_signs[-1], cl=sign_types, prob=TRUE, k=7)

# Get the "prob" attribute from the predicted classes
sign_prob <- attr(sign_pred, "prob")

# Examine the first several predictions
head(sign_pred)

# Examine the proportion of votes for the winning class
head(sign_prob)
```

________________________________
*** Data preparation for kNN ***
________________________________
- kNN assumes your data in numeric format as it is difficult to define the data
  distance between catefories.
    . Dummy coding - using 1/10 indicators to represent the categories 
        ~ setting '1' if the category applies
        ~ setting '0' if the category does not apply
        --> categories with the same coding will be closer in distance than the others.
- When calculating distance, each  feature of the input data should be measured with the
  same range of values
- Normalizing data in R - rescale data to a given range - performing min-max normalization.
```{r}
    # define a min-max normalize() function
    normalize <- function(x) { return( (x - min(x)) / (max(x) - min(x)) ) }
    
    # normalized version of r1
    summary(normalize(signs$r1))
    
    # un-normalized version of r1
    summary(signs$r1)
```



----------------------------------------------------------------------------------------------------------

***
### PART 2: NAIVE BAYES ###
***


______________________________
Understanding Bayesian methods
______________________________
. Thomas Bayes - promoted rules for estimating probabilities in light of historic data
. Bayesian methods - a branch of statistics
          ex: some smartphones and apps predict the user's destination to offer routes and traffic estimates
              without the user even asking --> Phone keeps record of the user's past location as the data to
              forecast the user's most probable future location.
          ex2: phone records number of times at different location: home(10), restaurant(3), work(23), store(4)
              Estimating probability
                  The probability of A is denoted as P(A)
                  P(work) = 23/40 = 57.5%
                  P(store) = 4/40 = 10.0%
              Joint probability and independent events
                  The joint probability of events A and B is denoted P(A and B)
                  One event does not influence the probability of another -> independent events
                      -> One event is independent of another if knowing one doesn't give you 
                        information about how likely the other is -> Knowing the outcome of one event 
                        does not help predict the other
                  P(work and evening) = 1%
                  P(work and afternoon) = 20%
              Conditional probability and dependent events
                  When one event is predictive of another -> dependent events
                  Conditional probability expresses how one event depends on another
                  Conditional probability of events A and B is denoted as P(A | B)
                  P(A|B) = P(A and B) / P(B)
                  P(work|evening) = 1/25 = 4%
                  P(work|afternoon) = 20/25 = 80%
. 'Naive Bayes' -  applies Bayesian methods to estimate the conditional probability of an outcome.

      # building a Naive Bayes model
      library(naivebayes)
      m <- naive_bayes(location ~ time_of_day, data = location_history)

**Practice**
. The where9am data frame contains 91 days (thirteen weeks) worth of data in which Brett recorded his location
  at 9am each day as well as whether the daytype was a weekend or weekday.

. Using the conditional probability formula below, you can compute the probability that Brett is working in 
  the office, given that it is a weekday.
  
                          P(A|B) = P(A and B) / P(B)
                          
      - Find P(office) using nrow() and subset() to count rows in the dataset and save the result as p_A.
      - Find P(weekday), using nrow() and subset() again, and save the result as p_B.
      - Use nrow() and subset() a final time to find P(office and weekday). Save the result as p_AB.
      - Compute P(office | weekday) and save the result as p_A_given_B.
      - Print the value of p_A_given_B.

```{r}
location <- read.csv("https://assets.datacamp.com/production/repositories/718/datasets/571628c39048df59c40c9dcfba146a2cf7a4a0e3/locations.csv")
location
am9 <- location[location$hour == 9,]
am9
where9am <- am9[, names(am9) %in% c("daytype", "location")]
# Compute P(A) 
p_A <- nrow(subset(where9am, location == "office")) / nrow(where9am)
p_A
# Compute P(B)
p_B <- nrow(subset(where9am, daytype == "weekday")) / nrow(where9am)
p_B
# Compute the observed P(A and B)
p_AB <- nrow(subset(where9am, location == "office" & daytype == "weekday")) / nrow(where9am)
p_AB 
# Compute P(A | B) and print its value
p_A_given_B <- p_AB/p_B
p_A_given_B
```
*A simple Naive Bayes location model*
. The previous exercises showed that the probability that Brett is at work or at home at 9am is highly 
  dependent on whether it is the weekend or a weekday.

. To see this finding in action, use the where9am data frame to build a Naive Bayes model on the same data.

. You can then use this model to predict the future: 
        where does the model think that Brett will be at 9am on Thursday and at 9am on Saturday?

. The dataframe where9am is available in your workspace. This dataset contains information about Brett's
  location at 9am on different days.
    - Load the naivebayes package.
    - Use naive_bayes() with a formula like y ~ x to build a model of location as a function of daytype.
    - Forecast the Thursday 9am location using predict() with the thursday9am object as the newdata argument.
    - Do the same for predicting the saturday9am location.
```{r}
#install.package(naivebayes)
#library(naivebayes)
##thursday9amdf <- am9[am9$weekday == "thursday",]
##thursday9amdf
##thursday9am <- data.frame(daytype = thursday9amdf[,names(thursday9amdf) %in% c("daytype")][1])
##thursday9am
thursday9am <- data.frame(daytype = "weekday")  
thursday9am$daytype <- factor(thursday9am$daytype, labels= c(1, 2), levels = c("weekday", "weekend"))
## create a new dataframe: data.frame(x= c(1,2,3), y=(1,2,3)) or data.frame(x= c(1:3), y= c(1:3))
# Build the location prediction model
locmodel <- naive_bayes(location ~ daytype, data = where9am)

# Predict Thursday's 9am location
predict(locmodel, thursday9am)

# Predict Saturdays's 9am location
saturday9am <- data.frame(daytype = "weekend")
saturday9am$daytype <- factor(saturday9am$daytype, labels = c(1, 2), levels = c("weekday", "weekend"))
predict(locmodel, saturday9am)
```

*Examining "raw" probabilities*
. The naivebayes package offers several ways to peek inside a Naive Bayes model.

. Typing the name of the model object provides the a priori (overall) and conditional probabilities of 
  each of the model's predictors. If one were so inclined, you might use these for calculating posterior
  (predicted) probabilities by hand.

. Alternatively, R will compute the posterior probabilities for you if the type = "prob" parameter is
  supplied to the predict() function.

. Using these methods, examine how the model's predicted 9am location probability varies from day-to-day.
. The model locmodel that you fit in the previous exercise is in your workspace.
      - Print the locmodel object to the console to view the computed a priori and conditional
        probabilities.
      - Use the predict() function similarly to the previous exercise, but with type = "prob" to see 
        the predicted probabilities for Thursday at 9am.
      - Compare these to the predicted probabilities for Saturday at 9am.
```{r}
# The 'naivebayes' package is loaded into the workspace
# and the Naive Bayes 'locmodel' has been built

# Examine the location prediction model
locmodel <- naive_bayes(location ~ daytype, data = where9am)

# Obtain the predicted probabilities for Thursday at 9am
thursday9am <- data.frame(daytype = "weekday")  
thursday9am$daytype <- factor(thursday9am$daytype, labels= c(1, 2), levels = c("weekday", "weekend"))
predict(locmodel, thursday9am)

# Obtain the predicted probabilities for Saturday at 9am
saturday9am <- data.frame(daytype = "weekend")
saturday9am$daytype <- factor(saturday9am$daytype, labels = c(1, 2), levels = c("weekday", "weekend"))
predict(locmodel, saturday9am)
```

_______________________________
***Understand NB's 'naivety'***
_______________________________
. With a single predictor, conditional probability is based on the overlap between the two events.
. When adding more events, there're more overlaping between events and can be confusing.
--> Naive Bayes algorithm uses a shortcut to approximate the conditional probability we hope to compute.
    It assummes that the events are independent, thus, the joint probability can be computed by 
    multiplying the individual probabilities.
. The problem of the method is that if there is a zero in the multiplification chain, the result will be
  zero. To solve this problem, adding a small number in each individual multiplifications to eliminate the power (The Laplace correction).

**Practice**

. A more sophisticated location model .
- The locations dataset records Brett's location every hour for 13 weeks. Each hour, the tracking
  information includes the daytype (weekend or weekday) as well as the hourtype (morning, afternoon,
  evening, or night).

- Using this data, build a more sophisticated model to see how Brett's predicted location not only varies 
  by the day of week but also by the time of day. The dataset locations is already loaded in your workspace.

- You can specify additional independent variablloes in your formula using the + sign (e.g. y ~ x + b).

    ~ Use the R formula interface to build a model where location depends on both daytype and hourtype. 
    ~ Recall that the function naive_bayes() takes 2 arguments: formula and data.
    ~ Predict Brett's location on a weekday afternoon using the dataframe weekday_afternoon and the
      predict() function.
    ~ Do the same for a weekday_evening.

```{r}
# The 'naivebayes' package is loaded into the workspace already

# Build a NB model of location
locations <- location[,names(location) %in% c("id","daytype","hourtype","location")]
locations
locmodel <- naive_bayes(location ~ daytype + hourtype, data = locations)

# Predict Brett's location on a weekday afternoon
weekday_afternoon <- locations[13,]
predict(locmodel, weekday_afternoon)

# Predict Brett's location on a weekday evening
weekday_evening <- locations[19,]
predict(locmodel, weekday_evening)
```
. Preparing for unforeseen circumstances .
    - While Brett was tracking his location over 13 weeks, he never went into the office during the weekend.
    - Consequently, the joint probability of P(office and weekend) = 0.

    - Explore how this impacts the predicted probability that Brett may go to work on the weekend in the
      future. Additionally, you can see how using the Laplace correction will allow a small chance for 
      these types of unforeseen circumstances.

    - The model locmodel is already in your workspace, along with the dataframe weekend_afternoon.
    
        ~ Use the locmodel to output predicted probabilities for a weekend afternoon by using the predict()
          function. Remember to set the type argument.
        ~ Create a new naive Bayes model with the Laplace smoothing parameter set to 1. You can do this by
          setting the laplace argument in your call to naive_bayes(). Save this as locmodel2.
        ~ See how the new predicted probabilities compare by using the predict() function on your new model.

```{r}
# The 'naivebayes' package is loaded into the workspace already
# The Naive Bayes location model (locmodel) has already been built

# Observe the predicted probabilities for a weekend afternoon
weekend_afternoon <- locations[85,]
predict(locmodel, weekday_afternoon, type = "prob")

# Build a new model using the Laplace correction
locmodel2 <- naive_bayes(location ~ daytype + hourtype, data = locations, laplace = 1)

# Observe the new predicted probabilities for a weekend afternoon
predict(locmodel2, weekend_afternoon, type = "prob")

```

____________________________________________
***Applying Naive Bayes to other problems***
____________________________________________
. Naive Bayes makes predictions by computing conditional probabilities of events and outcomes.
. Predictors used in Naive Bayes typically comprises a set of categories.
. Numeric properties are difficult for Naive

. Binning - a simple method for creating categories from numeric data.
          - divide a range of numbers into a series of sets called "bins"
              ex1: divide a numbers into bins based on percentiles by creating a category for 
                    the bottom 25%, the next 25%, and so on
              ex2: group ranges of values into meaningful bins
                    (time -> afternoon - evening)
                    (temperature -> cold - warm - hot)
. Bag-of-words - common process to prepare text data for Naive Bayes
               - adding structure to text data
               - creates an event for each word that appears in a particular collection of text documents.
               - estimates the probability of the outcome given the evidence provided by the words in the
                  text.

--------------------------------------------------------------------------------------------------

***
### PART 3: LOGISTIC REGRESSION ###
***