---
title: 'Supervised Learning in R: Regression'
author: "Hanh Nguyen"
date: "5/7/2020"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

***
### PART 1: REGRESSION ###
***

______________________________
***Welcome and Introduction***
______________________________

- Regression: predict a numerical outcome ("dependent variable") 
              from a set of inputs ("independent variable")
        . Statistical Sense: predicting the expected value of outcome.
        . Casual Sense: predicting a numerical outcome, rather than a discrete one.
        
  --> This distinguishes regression from classification, 
        which is the task of making discrete predictions.
        
        Ex:
            1. How many units will we sell?
                --> Regression.
                
            2. Will this customerbuy our product (yes/no)?
                --> Classification.
                
            3. What price will the customer pay for our product?
                --> Regression
                
- Regression from a Machine Learning Perspective:
      . Scientific mindset: modeling to understand the data generation process.
      . Engineer mindset: *modeling to predict future event accurately
- Machine Learning: Engineer mindset


________________________________________________
***Linear regression - the fundamental method***
________________________________________________

- Linear Regression assumes that:
      . the expected outcome is the weighted sum of all the inputs.
      . the change in y is linearly proportional to the change in any x.

            $y = \beta B_{0} + \beta B_{1} * x_{1} + \beta B_{2} * x_{2} + ... $
            
            . y is linearly related to each $x_{1}$
            . Each $x_{i}$ contributes additively to y
            
            -------------------------------------------
            
            cmodel <- lm(response ~ predictor1 + predictor2 , data = )
            
            . formular: temperature ~ chirps_per_sec
            . data frame: cricket
            
            --------------------------------------------
            
            # convert a string into a formula
            
            fmla <- as.formula("string")
            
            . ex: 
                fmla <- as.formula("temperature ~ chirps_per_sec")
            
            
            # Output when printing the model: the coefficients (or betas) of the model
            
            . (Intercept): Beta-zero: the value of the model when all the inputs are zeros
            . The other coefficients are the weights for the weighted sum of the variables
            
                    ex:     
                          lm(formula = temperature ~ chirps_per_sec, data = cricket)       
                       
                       outcome:
                    1. intercept = 25.232
                    2. chirps_per_sec = 3.291
                         - meanings: 
                            1. sign of the coefficient is postive
                                  -> the temperature should increase as chirp rate increases
                            2. For every unit increase in chirp rate, temperature should
                                increase by 3.291 degrees, if everything else is constant.
            
            ----------------------------------------------------
            
            # call summary() on the model -> get the model diagnostics
            
            . Values of the coefficients
            . Standard error in their estimated value
            . Other diagnostics.
            
            -----------------------------------------------------
            
            # to get these diagnostics conveniently packaged in the dataframe
              
              broom::glance(<model>)
            
            # for the R-squared diagnostic:
            
              sigr::wrapFTest(<model>)
            
            
**Practices**

Code a simple one-variable regression

For the first coding exercise, you'll create a formula to define a one-variable modeling task, and then fit a linear model to the data. You are given the rates of male and female unemployment in the United States over several years (Source).

The task is to predict the rate of female unemployment from the observed rate of male unemployment. The outcome is female_unemployment, and the input is male_unemployment.

The sign of the variable coefficient tells you whether the outcome increases (+) or decreases (-) as the variable increases.

Recall the calling interface for lm() is:

lm(formula, data = ___)
            
The data frame unemployment is in your workspace.

    - Define a formula that expresses female_unemployment as a function of male_unemployment. 
        Assign the formula to the variable fmla and print it.
    - Then use lm() and fmla to fit a linear model to predict female unemployment from 
        male unemployment using the data set unemployment.
    - Print the model. Is the coefficent for male unemployment consistent with what you would
        expect? Does female unemployment increase as male unemployment does?          
-------------------------

> unemployment
   male_unemployment female_unemployment
1                2.9                 4.0
2                6.7                 7.4
3                4.9                 5.0
4                7.9                 7.2
5                9.8                 7.9
6                6.9                 6.1
7                6.1                 6.0
8                6.2                 5.8
9                6.0                 5.2
10               5.1                 4.2
11               4.7                 4.0
12               4.4                 4.4
13               5.8                 5.2

# unemployment is loaded in the workspace
summary(unemployment)

> summary(unemployment)
 male_unemployment female_unemployment
 Min.   :2.900     Min.   :4.000      
 1st Qu.:4.900     1st Qu.:4.400      
 Median :6.000     Median :5.200      
 Mean   :5.954     Mean   :5.569      
 3rd Qu.:6.700     3rd Qu.:6.100      
 Max.   :9.800     Max.   :7.900

# Define a formula to express female_unemployment as a function of male_unemployment
fmla <- female_unemployment ~ male_unemployment

# Print it
fmla

> fmla
female_unemployment ~ male_unemployment

# Use the formula to fit a model: unemployment_model
unemployment_model <-  lm(fmla, data = unemployment)

# Print it 
unemployment_model


> Call:
lm(formula = fmla, data = unemployment)

> Coefficients:
      (Intercept)  male_unemployment  
           1.4341             0.6945

------------------------

Examining a model
Let's look at the model unemployment_model that you have just created. There are a variety of different ways to examine a model; each way provides different information. We will use summary(), broom::glance(), and sigr::wrapFTest().

The object unemployment_model is in your workspace.

    - Print unemployment_model again. What information does it report?
    - Call summary() on unemployment_model. In addition to the coefficient values, 
        you get standard errors on the coefficient estimates, and some goodness-of-fit 
        metrics like R-squared.
    - Call glance() on the model to see the performance metrics in an orderly data frame. 
      Can you match the information from summary() to the columns of glance()?
    - Now call wrapFTest() on the model to see the R-squared again.

---------------------------

# broom and sigr are already loaded in your workspace
# Print unemployment_model
unemployment_model

> Call:
lm(formula = fmla, data = unemployment)

> Coefficients:
      (Intercept)  male_unemployment  
           1.4341             0.6945
           
# Call summary() on unemployment_model to get more details
summary(unemployment_model)

> Call:
lm(formula = fmla, data = unemployment)

> Residuals:
     Min       1Q   Median       3Q      Max 
-0.77621 -0.34050 -0.09004  0.27911  1.31254 

> Coefficients:
                  Estimate Std. Error t value Pr(>|t|)    
(Intercept)        1.43411    0.60340   2.377   0.0367 *  
male_unemployment  0.69453    0.09767   7.111 1.97e-05 ***
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1

> Residual standard error: 0.5803 on 11 degrees of freedom
Multiple R-squared:  0.8213,	Adjusted R-squared:  0.8051 
F-statistic: 50.56 on 1 and 11 DF,  p-value: 1.966e-05

# Call glance() on unemployment_model to see the details in a tidier form
glance(unemployment_model)

> r.squared adj.r.squared     sigma statistic      p.value df    logLik
1 0.8213157     0.8050716 0.5802596  50.56108 1.965985e-05  2 -10.28471
       AIC      BIC deviance df.residual
1 26.56943 28.26428 3.703714          11

# Call wrapFTest() on unemployment_model to see the most relevant details
wrapFTest(unemployment_model)

> [1] "F Test summary: (R2=0.8213, F(1,11)=50.56, p=1.966e-05)."



_____________________________________
***Predicting once you fit a model***
_____________________________________

          # call predict on model and add a column of predictions to the training dataframe.
          
                  dataframe$prediction <- predict(<model>)
                       . predict() by default returns training data predictions
          
                  ex:
                      cricket$prediction <- predict(cmodel)
                
           # looking at the predictions
           
                ggplot(dataset, aes(x=prediction, y=response)) +
                        geom_point() +
                        geom_abline() + 
                        ggtitle("title name")
              
              ex:
                ggplot(cricket, aes(x=prediction, y=temperature)) +
                      geom_point() +
                      geom_abline(color = "darkblue") +
                      ggtitle("temperature vs linear model prediction")
           
           -------------------------------------------------------------
           
            # to apply the model to newdata, use argument newdata
            
            newdataframe$prediction <- predict(<model>, newdata = <newdataframe>)
            
            ex:
                newchirps <- data.frame(chirps_per_sec = 16.5)
                newchirps$prediction <- prediction(cmodel, newdata = newchirps)
                
            outcome:
                newchirps
                    chirps_per_sec: 16.5
                    pred          : 79.53537
            
            meaning:
                the model predicts that at a chirp rate at 16.5 should correspond to 
                a temperature of almost 80 degrees (79.53537)
            
**Practices**

Predicting from the unemployment model
In this exercise, you will use your unemployment model unemployment_model to make predictions from the unemployment data, and compare predicted female unemployment rates to the actual observed female unemployment rates on the training data, unemployment. You will also use your model to predict on the new data in newrates, which consists of only one observation, where male unemployment is 5%.

The predict() interface for lm models takes the form

predict(model, newdata)
You will use the ggplot2 package to make the plots, so you will add the prediction column to the unemployment data frame. You will plot outcome versus prediction, and compare them to the line that represents perfect predictions (that is when the outcome is equal to the predicted value).

The ggplot2 command to plot a scatterplot of dframe$outcome versus dframe$pred (pred on the x axis, outcome on the y axis), along with a blue line where outcome == pred is as follows:

ggplot(dframe, aes(x = pred, y = outcome)) + 
       geom_point() +  
       geom_abline(color = "blue")
            
    - The objects unemployment, unemployment_model and newrates are in your workspace.

    - Use predict() to predict female unemployment rates from the unemployment data. 
        Assign it to a new column: prediction.
    - Use the library() command to load the ggplot2 package.
    - Use ggplot() to compare the predictions to actual unemployment rates. 
        Put the predictions on the x axis. How close are the results to the line of 
        perfect prediction?
    - Use the data frame newrates to predict expected female unemployment rate when 
        male unemployment is 5%. Assign the answer to the variable pred and print it.
            
--------------------------------

# unemployment is in your workspace
summary(unemployment)

> male_unemployment female_unemployment
 Min.   :2.900     Min.   :4.000      
 1st Qu.:4.900     1st Qu.:4.400      
 Median :6.000     Median :5.200      
 Mean   :5.954     Mean   :5.569      
 3rd Qu.:6.700     3rd Qu.:6.100      
 Max.   :9.800     Max.   :7.900

# newrates is in your workspace
newrates

> male_unemployment
1                 5

# To predict female unemployment in the unemployment data set, first call predict() on model.
unemployment$prediction <-  predict(unemployment_model)

> unemployment$prediction
 [1] 3.448245 6.087456 4.837304 6.920891 8.240497 6.226362 5.670739 5.740192
 [9] 5.601286 4.976210 4.698398 4.490039 5.462380
 
# load the ggplot2 package
library(ggplot2)

# Make a plot to compare predictions to actual (prediction on x axis)
ggplot(unemployment, aes(x = prediction, y = female_unemployment)) + 
  geom_point() +
  geom_abline(color = "blue")

# Predict female unemployment rate when male unemployment is 5%
pred <- predict(unemployment_model, newdata = newrates)

> pred
       1 
4.906757
            
--------------------------------------

Multivariate linear regression (Part 1)

In this exercise, you will work with the blood pressure dataset (Source), and model blood_pressure as a function of weight and age.
            
The data frame bloodpressure is in the workspace.

    - Define a formula that expresses blood_pressure explicitly as a function of age 
        and weight. Assign the formula to the variable fmla and print it.
    - Use fmla to fit a linear model to predict blood_pressure from age and weight in 
        the data set bloodpressure. Call the model bloodpressure_model.
    - Print the model and call summary() on bloit. Does blood pressure increase or decrease 
        with age? With weight?         
        
------------------------------------------

> bloodpressure
   blood_pressure age weight
1             132  52    173
2             143  59    184
3             153  67    194
4             162  73    211
5             154  64    196
6             168  74    220
7             137  54    188
8             149  61    188
9             159  65    207
10            128  46    167
11            166  72    217

> summary(bloodpressure)
 blood_pressure       age            weight   
 Min.   :128.0   Min.   :46.00   Min.   :167  
 1st Qu.:140.0   1st Qu.:56.50   1st Qu.:186  
 Median :153.0   Median :64.00   Median :194  
 Mean   :150.1   Mean   :62.45   Mean   :195  
 3rd Qu.:160.5   3rd Qu.:69.50   3rd Qu.:209  
 Max.   :168.0   Max.   :74.00   Max.   :220
            
# Create the formula and print it
fmla <- bloodpressure ~ age + weight

> fmla
bloodpressure ~ age + weight

# Fit the model: bloodpressure_model
bloodpressure_model <- lm(fmla, data =  bloodpressure)

> Call:
lm(formula = fmla, data = bloodpressure)

> Coefficients:
(Intercept)          age       weight  
    30.9941       0.8614       0.3349
    
# call summary() 
summary(bloodpressure_model)           

> Call:
lm(formula = fmla, data = bloodpressure)

> Residuals:
    Min      1Q  Median      3Q     Max 
-3.4640 -1.1949 -0.4078  1.8511  2.6981 

> Coefficients:
            Estimate Std. Error t value Pr(>|t|)   
(Intercept)  30.9941    11.9438   2.595  0.03186 * 
age           0.8614     0.2482   3.470  0.00844 **
weight        0.3349     0.1307   2.563  0.03351 * 
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1

>Residual standard error: 2.318 on 8 degrees of freedom
Multiple R-squared:  0.9768,	Adjusted R-squared:  0.9711 
F-statistic: 168.8 on 2 and 8 DF,  p-value: 2.874e-07

-----------------------------------------------------

Multivariate linear regression (Part 2)
Now you will make predictions using the blood pressure model bloodpressure_model that you fit in the previous exercise.

You will also compare the predictions to outcomes graphically. ggplot2 is already loaded in your workspace. Recall the plot command takes the form:

ggplot(dframe, aes(x = pred, y = outcome)) + 
     geom_point() + 
     geom_abline(color = "blue")
     
The objects bloodpressure and bloodpressure_model are in the workspace.

    - Use predict() to predict blood pressure in the bloodpressure dataset. 
        Assign the predictions to the column prediction.
    - Graphically compare the predictions to actual blood pressures. 
        Put predictions on the x axis. 
        How close are the results to the line of perfect prediction?

-----------------------------------------------------------

# bloodpressure is in your workspace
> summary(bloodpressure)
 blood_pressure       age            weight   
 Min.   :128.0   Min.   :46.00   Min.   :167  
 1st Qu.:140.0   1st Qu.:56.50   1st Qu.:186  
 Median :153.0   Median :64.00   Median :194  
 Mean   :150.1   Mean   :62.45   Mean   :195  
 3rd Qu.:160.5   3rd Qu.:69.50   3rd Qu.:209  
 Max.   :168.0   Max.   :74.00   Max.   :220
  
# bloodpressure_model is in your workspace
> bloodpressure_model

> Call:
> lm(formula = fmla, data = bloodpressure)

> Coefficients:
(Intercept)          age       weight  
    30.9941       0.8614       0.3349

# predict blood pressure using bloodpressure_model :prediction
bloodpressure$prediction <- predict(bloodpressure_model)

> bloodpressure$prediction
 [1] 133.7183 143.4317 153.6716 164.5327 151.7570 168.4078 140.4640 146.4939
 [9] 156.3019 126.5407 165.6804
 
# plot the results
> ggplot(bloodpressure, aes(x = prediction, y = blood_pressure)) + 
      geom_point() +
      geom_abline(color = "blue")



___________________________________
***Wrapping up linear regression***
___________________________________

- Pros of Linear regression:
    . Easy to fit and apply
    . Concise
    . Less prone to overfitting
        -> This means that their prediction performance on new data is usually quite similar 
            to their performance on the training data.
    . Interpretable   
    
        ex:
            > Call:
                lm(formula = blood_pressure ~ age + weight, data = bloodpressure)

            > Coefficients:
                (Intercept)          age       weight  
                      30.9941       0.8614       0.3349
        
        meanings:
            1. blood_pressure is a function of age and weight
            2. the coefficients are positive in sign telling that blood pressure tends to
                increase as both age and weight increase

- Cons of Linear regression:
    . can only express linear and additive relationship
            -> in data where the relationships are highly complex, linear regression will not
                  predict as well as other models
    . collinearity - when input (independent) variables are partially correlated.
          . coefficients might change sign - When variable are highly correlated, 
                                              the signs of coefficients may 
                                              not be as expected
                                            -> cannot interpret coefficients
                                            -> but it does not necessarily affect 
                                                the model's prediction accuracy    
          . high collinearity:
                ~ coefficients (or standard errors) can be unsually large
                -> model many unstable -> unreliable predictions.
        
        ex:
            > Call:
                lm(formula = blood_pressure ~ age + weight, data = bloodpressure)

            > Coefficients:
                (Intercept)          age       weight  
                      30.9941       0.8614       0.3349
            
        reasoning: 
            1. weight tends to increase as people age, so weight and age could be
                    partially correlated.
            2. decrease in blood pressure can result in decreasing weight.

        
- Coming next:
      . Evaluating a regression model
      . Properly training a model


----------------------------------------------------------------------------------------------
***
### PART 2: TRAINING AND EVALUATING REGRESSION MODELS ##
***

