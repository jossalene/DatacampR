---
title: 'Supervised Learning in R: Regression'
author: "Hanh Nguyen"
date: "5/7/2020"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

***
### PART 1: REGRESSION ###
***

______________________________
***Welcome and Introduction***
______________________________

- Regression: predict a numerical outcome ("dependent variable") 
              from a set of inputs ("independent variable")
        . Statistical Sense: predicting the expected value of outcome.
        . Casual Sense: predicting a numerical outcome, rather than a discrete one.
        
  --> This distinguishes regression from classification, 
        which is the task of making discrete predictions.
        
        Ex:
            1. How many units will we sell?
                --> Regression.
                
            2. Will this customerbuy our product (yes/no)?
                --> Classification.
                
            3. What price will the customer pay for our product?
                --> Regression
                
- Regression from a Machine Learning Perspective:
      . Scientific mindset: modeling to understand the data generation process.
      . Engineer mindset: *modeling to predict future event accurately
- Machine Learning: Engineer mindset


________________________________________________
***Linear regression - the fundamental method***
________________________________________________

- Linear Regression assumes that:
      . the expected outcome is the weighted sum of all the inputs.
      . the change in y is linearly proportional to the change in any x.

            $y = \beta B_{0} + \beta B_{1} * x_{1} + \beta B_{2} * x_{2} + ... $
            
            . y is linearly related to each $x_{1}$
            . Each $x_{i}$ contributes additively to y
            
            -------------------------------------------
            
            cmodel <- lm(response ~ predictor1 + predictor2 , data = )
            
            . formular: temperature ~ chirps_per_sec
            . data frame: cricket
            
            --------------------------------------------
            
            # convert a string into a formula
            
            fmla <- as.formula("string")
            
            . ex: 
                fmla <- as.formula("temperature ~ chirps_per_sec")
            
            
            # Output when printing the model: the coefficients (or betas) of the model
            
            . (Intercept): Beta-zero: the value of the model when all the inputs are zeros
            . The other coefficients are the weights for the weighted sum of the variables
            
                    ex:     
                          lm(formula = temperature ~ chirps_per_sec, data = cricket)       
                       
                       outcome:
                    1. intercept = 25.232
                    2. chirps_per_sec = 3.291
                         - meanings: 
                            1. sign of the coefficient is postive
                                  -> the temperature should increase as chirp rate increases
                            2. For every unit increase in chirp rate, temperature should
                                increase by 3.291 degrees, if everything else is constant.
            
            ----------------------------------------------------
            
            # call summary() on the model -> get the model diagnostics
            
            . Values of the coefficients
            . Standard error in their estimated value
            . Other diagnostics.
            
            -----------------------------------------------------
            
            # to get these diagnostics conveniently packaged in the dataframe
              
              broom::glance(<model>)
            
            # for the R-squared diagnostic:
            
              sigr::wrapFTest(<model>)
            
            
**Practices**

Code a simple one-variable regression

For the first coding exercise, you'll create a formula to define a one-variable modeling task, and then fit a linear model to the data. You are given the rates of male and female unemployment in the United States over several years (Source).

The task is to predict the rate of female unemployment from the observed rate of male unemployment. The outcome is female_unemployment, and the input is male_unemployment.

The sign of the variable coefficient tells you whether the outcome increases (+) or decreases (-) as the variable increases.

Recall the calling interface for lm() is:

lm(formula, data = ___)
            
The data frame unemployment is in your workspace.

    - Define a formula that expresses female_unemployment as a function of male_unemployment. 
        Assign the formula to the variable fmla and print it.
    - Then use lm() and fmla to fit a linear model to predict female unemployment from 
        male unemployment using the data set unemployment.
    - Print the model. Is the coefficent for male unemployment consistent with what you would
        expect? Does female unemployment increase as male unemployment does?          
-------------------------

> unemployment
   male_unemployment female_unemployment
1                2.9                 4.0
2                6.7                 7.4
3                4.9                 5.0
4                7.9                 7.2
5                9.8                 7.9
6                6.9                 6.1
7                6.1                 6.0
8                6.2                 5.8
9                6.0                 5.2
10               5.1                 4.2
11               4.7                 4.0
12               4.4                 4.4
13               5.8                 5.2

# unemployment is loaded in the workspace
summary(unemployment)

> summary(unemployment)
 male_unemployment female_unemployment
 Min.   :2.900     Min.   :4.000      
 1st Qu.:4.900     1st Qu.:4.400      
 Median :6.000     Median :5.200      
 Mean   :5.954     Mean   :5.569      
 3rd Qu.:6.700     3rd Qu.:6.100      
 Max.   :9.800     Max.   :7.900

# Define a formula to express female_unemployment as a function of male_unemployment
fmla <- female_unemployment ~ male_unemployment

# Print it
fmla

> fmla
female_unemployment ~ male_unemployment

# Use the formula to fit a model: unemployment_model
unemployment_model <-  lm(fmla, data = unemployment)

# Print it 
unemployment_model


> Call:
lm(formula = fmla, data = unemployment)

> Coefficients:
      (Intercept)  male_unemployment  
           1.4341             0.6945

------------------------

Examining a model
Let's look at the model unemployment_model that you have just created. There are a variety of different ways to examine a model; each way provides different information. We will use summary(), broom::glance(), and sigr::wrapFTest().

The object unemployment_model is in your workspace.

    - Print unemployment_model again. What information does it report?
    - Call summary() on unemployment_model. In addition to the coefficient values, 
        you get standard errors on the coefficient estimates, and some goodness-of-fit 
        metrics like R-squared.
    - Call glance() on the model to see the performance metrics in an orderly data frame. 
      Can you match the information from summary() to the columns of glance()?
    - Now call wrapFTest() on the model to see the R-squared again.

---------------------------

# broom and sigr are already loaded in your workspace
# Print unemployment_model
unemployment_model

> Call:
lm(formula = fmla, data = unemployment)

> Coefficients:
      (Intercept)  male_unemployment  
           1.4341             0.6945
           
# Call summary() on unemployment_model to get more details
summary(unemployment_model)

> Call:
lm(formula = fmla, data = unemployment)

> Residuals:
     Min       1Q   Median       3Q      Max 
-0.77621 -0.34050 -0.09004  0.27911  1.31254 

> Coefficients:
                  Estimate Std. Error t value Pr(>|t|)    
(Intercept)        1.43411    0.60340   2.377   0.0367 *  
male_unemployment  0.69453    0.09767   7.111 1.97e-05 ***
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1

> Residual standard error: 0.5803 on 11 degrees of freedom
Multiple R-squared:  0.8213,	Adjusted R-squared:  0.8051 
F-statistic: 50.56 on 1 and 11 DF,  p-value: 1.966e-05

# Call glance() on unemployment_model to see the details in a tidier form
glance(unemployment_model)

> r.squared adj.r.squared     sigma statistic      p.value df    logLik
1 0.8213157     0.8050716 0.5802596  50.56108 1.965985e-05  2 -10.28471
       AIC      BIC deviance df.residual
1 26.56943 28.26428 3.703714          11

# Call wrapFTest() on unemployment_model to see the most relevant details
wrapFTest(unemployment_model)

> [1] "F Test summary: (R2=0.8213, F(1,11)=50.56, p=1.966e-05)."



_____________________________________
***Predicting once you fit a model***
_____________________________________

          # call predict on model and add a column of predictions to the training dataframe.
          
                  dataframe$prediction <- predict(<model>)
                       . predict() by default returns training data predictions
          
                  ex:
                      cricket$prediction <- predict(cmodel)
                
           # looking at the predictions
           
                ggplot(dataset, aes(x=prediction, y=response)) +
                        geom_point() +
                        geom_abline() + 
                        ggtitle("title name")
              
              ex:
                ggplot(cricket, aes(x=prediction, y=temperature)) +
                      geom_point() +
                      geom_abline(color = "darkblue") +
                      ggtitle("temperature vs linear model prediction")
           
           -------------------------------------------------------------
           
            # to apply the model to newdata, use argument newdata
            
            newdataframe$prediction <- predict(<model>, newdata = <newdataframe>)
            
            ex:
                newchirps <- data.frame(chirps_per_sec = 16.5)
                newchirps$prediction <- prediction(cmodel, newdata = newchirps)
                
            outcome:
                newchirps
                    chirps_per_sec: 16.5
                    pred          : 79.53537
            
            meaning:
                the model predicts that at a chirp rate at 16.5 should correspond to 
                a temperature of almost 80 degrees (79.53537)
            
**Practices**

Predicting from the unemployment model
In this exercise, you will use your unemployment model unemployment_model to make predictions from the unemployment data, and compare predicted female unemployment rates to the actual observed female unemployment rates on the training data, unemployment. You will also use your model to predict on the new data in newrates, which consists of only one observation, where male unemployment is 5%.

The predict() interface for lm models takes the form

predict(model, newdata)
You will use the ggplot2 package to make the plots, so you will add the prediction column to the unemployment data frame. You will plot outcome versus prediction, and compare them to the line that represents perfect predictions (that is when the outcome is equal to the predicted value).

The ggplot2 command to plot a scatterplot of dframe$outcome versus dframe$pred (pred on the x axis, outcome on the y axis), along with a blue line where outcome == pred is as follows:

ggplot(dframe, aes(x = pred, y = outcome)) + 
       geom_point() +  
       geom_abline(color = "blue")
            
    - The objects unemployment, unemployment_model and newrates are in your workspace.

    - Use predict() to predict female unemployment rates from the unemployment data. 
        Assign it to a new column: prediction.
    - Use the library() command to load the ggplot2 package.
    - Use ggplot() to compare the predictions to actual unemployment rates. 
        Put the predictions on the x axis. How close are the results to the line of 
        perfect prediction?
    - Use the data frame newrates to predict expected female unemployment rate when 
        male unemployment is 5%. Assign the answer to the variable pred and print it.
            
--------------------------------

# unemployment is in your workspace
summary(unemployment)

> male_unemployment female_unemployment
 Min.   :2.900     Min.   :4.000      
 1st Qu.:4.900     1st Qu.:4.400      
 Median :6.000     Median :5.200      
 Mean   :5.954     Mean   :5.569      
 3rd Qu.:6.700     3rd Qu.:6.100      
 Max.   :9.800     Max.   :7.900

# newrates is in your workspace
newrates

> male_unemployment
1                 5

# To predict female unemployment in the unemployment data set, first call predict() on model.
unemployment$prediction <-  predict(unemployment_model)

> unemployment$prediction
 [1] 3.448245 6.087456 4.837304 6.920891 8.240497 6.226362 5.670739 5.740192
 [9] 5.601286 4.976210 4.698398 4.490039 5.462380
 
# load the ggplot2 package
library(ggplot2)

# Make a plot to compare predictions to actual (prediction on x axis)
ggplot(unemployment, aes(x = prediction, y = female_unemployment)) + 
  geom_point() +
  geom_abline(color = "blue")

# Predict female unemployment rate when male unemployment is 5%
pred <- predict(unemployment_model, newdata = newrates)

> pred
       1 
4.906757
            
--------------------------------------

Multivariate linear regression (Part 1)

In this exercise, you will work with the blood pressure dataset (Source), and model blood_pressure as a function of weight and age.
            
The data frame bloodpressure is in the workspace.

    - Define a formula that expresses blood_pressure explicitly as a function of age 
        and weight. Assign the formula to the variable fmla and print it.
    - Use fmla to fit a linear model to predict blood_pressure from age and weight in 
        the data set bloodpressure. Call the model bloodpressure_model.
    - Print the model and call summary() on bloit. Does blood pressure increase or decrease 
        with age? With weight?         
        
------------------------------------------

> bloodpressure
   blood_pressure age weight
1             132  52    173
2             143  59    184
3             153  67    194
4             162  73    211
5             154  64    196
6             168  74    220
7             137  54    188
8             149  61    188
9             159  65    207
10            128  46    167
11            166  72    217

> summary(bloodpressure)
 blood_pressure       age            weight   
 Min.   :128.0   Min.   :46.00   Min.   :167  
 1st Qu.:140.0   1st Qu.:56.50   1st Qu.:186  
 Median :153.0   Median :64.00   Median :194  
 Mean   :150.1   Mean   :62.45   Mean   :195  
 3rd Qu.:160.5   3rd Qu.:69.50   3rd Qu.:209  
 Max.   :168.0   Max.   :74.00   Max.   :220
            
# Create the formula and print it
fmla <- bloodpressure ~ age + weight

> fmla
bloodpressure ~ age + weight

# Fit the model: bloodpressure_model
bloodpressure_model <- lm(fmla, data =  bloodpressure)

> Call:
lm(formula = fmla, data = bloodpressure)

> Coefficients:
(Intercept)          age       weight  
    30.9941       0.8614       0.3349
    
# call summary() 
summary(bloodpressure_model)           

> Call:
lm(formula = fmla, data = bloodpressure)

> Residuals:
    Min      1Q  Median      3Q     Max 
-3.4640 -1.1949 -0.4078  1.8511  2.6981 

> Coefficients:
            Estimate Std. Error t value Pr(>|t|)   
(Intercept)  30.9941    11.9438   2.595  0.03186 * 
age           0.8614     0.2482   3.470  0.00844 **
weight        0.3349     0.1307   2.563  0.03351 * 
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1

>Residual standard error: 2.318 on 8 degrees of freedom
Multiple R-squared:  0.9768,	Adjusted R-squared:  0.9711 
F-statistic: 168.8 on 2 and 8 DF,  p-value: 2.874e-07

-----------------------------------------------------

Multivariate linear regression (Part 2)
Now you will make predictions using the blood pressure model bloodpressure_model that you fit in the previous exercise.

You will also compare the predictions to outcomes graphically. ggplot2 is already loaded in your workspace. Recall the plot command takes the form:

ggplot(dframe, aes(x = pred, y = outcome)) + 
     geom_point() + 
     geom_abline(color = "blue")
     
The objects bloodpressure and bloodpressure_model are in the workspace.

    - Use predict() to predict blood pressure in the bloodpressure dataset. 
        Assign the predictions to the column prediction.
    - Graphically compare the predictions to actual blood pressures. 
        Put predictions on the x axis. 
        How close are the results to the line of perfect prediction?

-----------------------------------------------------------

# bloodpressure is in your workspace
> summary(bloodpressure)
 blood_pressure       age            weight   
 Min.   :128.0   Min.   :46.00   Min.   :167  
 1st Qu.:140.0   1st Qu.:56.50   1st Qu.:186  
 Median :153.0   Median :64.00   Median :194  
 Mean   :150.1   Mean   :62.45   Mean   :195  
 3rd Qu.:160.5   3rd Qu.:69.50   3rd Qu.:209  
 Max.   :168.0   Max.   :74.00   Max.   :220
  
# bloodpressure_model is in your workspace
> bloodpressure_model

> Call:
> lm(formula = fmla, data = bloodpressure)

> Coefficients:
(Intercept)          age       weight  
    30.9941       0.8614       0.3349

# predict blood pressure using bloodpressure_model :prediction
bloodpressure$prediction <- predict(bloodpressure_model)

> bloodpressure$prediction
 [1] 133.7183 143.4317 153.6716 164.5327 151.7570 168.4078 140.4640 146.4939
 [9] 156.3019 126.5407 165.6804
 
# plot the results
> ggplot(bloodpressure, aes(x = prediction, y = blood_pressure)) + 
      geom_point() +
      geom_abline(color = "blue")



___________________________________
***Wrapping up linear regression***
___________________________________

- Pros of Linear regression:
    . Easy to fit and apply
    . Concise
    . Less prone to overfitting
        -> This means that their prediction performance on new data is usually quite similar 
            to their performance on the training data.
    . Interpretable   
    
        ex:
            > Call:
                lm(formula = blood_pressure ~ age + weight, data = bloodpressure)

            > Coefficients:
                (Intercept)          age       weight  
                      30.9941       0.8614       0.3349
        
        meanings:
            1. blood_pressure is a function of age and weight
            2. the coefficients are positive in sign telling that blood pressure tends to
                increase as both age and weight increase

- Cons of Linear regression:
    . can only express linear and additive relationship
            -> in data where the relationships are highly complex, linear regression will not
                  predict as well as other models
    . collinearity - when input (independent) variables are partially correlated.
          . coefficients might change sign - When variable are highly correlated, 
                                              the signs of coefficients may 
                                              not be as expected
                                            -> cannot interpret coefficients
                                            -> but it does not necessarily affect 
                                                the model's prediction accuracy    
          . high collinearity:
                ~ coefficients (or standard errors) can be unsually large
                -> model many unstable -> unreliable predictions.
        
        ex:
            > Call:
                lm(formula = blood_pressure ~ age + weight, data = bloodpressure)

            > Coefficients:
                (Intercept)          age       weight  
                      30.9941       0.8614       0.3349
            
        reasoning: 
            1. weight tends to increase as people age, so weight and age could be
                    partially correlated.
            2. decrease in blood pressure can result in decreasing weight.

        
- Coming next:
      . Evaluating a regression model
      . Properly training a model


----------------------------------------------------------------------------------------------
***
### PART 2: TRAINING AND EVALUATING REGRESSION MODELS ##
***

____________________________________
***Evaluating a model graphically***
____________________________________

- Plotting ground truth vs. prediction
    . Predictions: x- axis
    . Outcomes: y-axis
    . A well fitting model
        ~ x = y (diagonal) line runs through center of points
        ~ "line of perfect prediction"
    . A poorly fitting model
        ~ Points are all on one side of x =  y (diagonal) line
        ~ Systematic errors - errors that are correlated with the actual outcome
      --> This can indicate that you don't have yet all the important variables in your model.
      --> Or you need an algorithm that can find more complex relationships in the data
      
- The Residual plot - plots the residuals - the difference between outcome and predictions -
                      against the predictions.
    . A well fitting model:
        ~ Residual: actual outcome prediction
        ~ Good fit: no systematic errors.
                    --> the errors will be evenly distributed between positive and negative,
                          and have about the same magnitude above and below.
    . A poorly fitting model
        ~ Systematic errors
            --> there will be clusters of all positive or all negative residuals

- The Gain Curve plot is useful when sorting the instances is more important than 
                        predicting the exact outcome value.
          - useful for models that predict probability
          - measures how well model sorts the outcome
- The Wizard curve - the curve a perfect model would trace out.
- Reading the Gain Curve - based on how much the model curve and the wizard curve line up 
                            almost perfectly.

              
              library("WVPlots")
              GainCurvePlot(dataframe,"prediction" , "outcome-columns" , "title")

**Practices**

Graphically evaluate the unemployment model
In this exercise you will graphically evaluate the unemployment model, unemployment_model, that you fit to the unemployment data in the previous chapter. Recall that the model predicts female_unemployment from male_unemployment.

You will plot the model's predictions against the actual female_unemployment; recall the command is of the form

ggplot(dframe, aes(x = pred, y = outcome)) + 
       geom_point() +  
       geom_abline()
Then you will calculate the residuals:

residuals <- actual outcome - predicted outcome
and plot predictions against residuals. The residual graph will take a slightly different form: you compare the residuals to the horizontal line x=0 (using geom_hline()) rather than to the line x=y. The command will be provided.

The data frame unemployment and model unemployment_model are available in the workspace.

      - predict() to get the model predictions and add them to unemployment 
          as the column predictions.
      - Plot predictions (on the x-axis) versus actual female unemployment rates. 
          Are the predictions near the x=y line
          
--------------------------------------------------------

> unemployment
   male_unemployment female_unemployment predictions
1                2.9                 4.0    3.448245
2                6.7                 7.4    6.087456
3                4.9                 5.0    4.837304
4                7.9                 7.2    6.920891
5                9.8                 7.9    8.240497
6                6.9                 6.1    6.226362
7                6.1                 6.0    5.670739
8                6.2                 5.8    5.740192
9                6.0                 5.2    5.601286
10               5.1                 4.2    4.976210
11               4.7                 4.0    4.698398
12               4.4                 4.4    4.490039
13               5.8                 5.2    5.462380

> summary(unemployment)
 male_unemployment female_unemployment  predictions   
 Min.   :2.900     Min.   :4.000       Min.   :3.448  
 1st Qu.:4.900     1st Qu.:4.400       1st Qu.:4.837  
 Median :6.000     Median :5.200       Median :5.601  
 Mean   :5.954     Mean   :5.569       Mean   :5.569  
 3rd Qu.:6.700     3rd Qu.:6.100       3rd Qu.:6.087  
 Max.   :9.800     Max.   :7.900       Max.   :8.240
 
 > summary(unemployment_model)
Call:
lm(formula = fmla, data = unemployment)
Residuals:
     Min       1Q   Median       3Q      Max 
-0.77621 -0.34050 -0.09004  0.27911  1.31254 
Coefficients:
                  Estimate Std. Error t value Pr(>|t|)    
(Intercept)        1.43411    0.60340   2.377   0.0367 *  
male_unemployment  0.69453    0.09767   7.111 1.97e-05 ***
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
Residual standard error: 0.5803 on 11 degrees of freedom
Multiple R-squared:  0.8213,	Adjusted R-squared:  0.8051 
F-statistic: 50.56 on 1 and 11 DF,  p-value: 1.966e-05

# Make predictions from the model
unemployment$predictions <- predict(unemployment_model, unemployment)
> unemployment$predictions
 [1] 3.448245 6.087456 4.837304 6.920891 8.240497 6.226362 5.670739 5.740192
 [9] 5.601286 4.976210 4.698398 4.490039 5.462380

# Fill in the blanks to plot predictions (on x-axis) versus the female_unemployment rates
ggplot(unemployment, aes(x = predictions, y = female_unemployment)) + 
  geom_point() + 
  geom_abline()

-------------------------------------------------------------

    - Calculate the residuals between the predictions and actual unemployment rates. 
        Add these residuals to unemployment as the column residuals.
    - Fill in the blanks to plot predictions (on the x-axis) versus residuals (on the y-axis).
        This gives you a different view of the model's predictions as compared to ground truth.

---------------------------------------------------------------

# From previous step
unemployment$predictions <- predict(unemployment_model)

# Calculate residuals
unemployment$residuals <- unemployment$female_unemployment - unemployment$predictions
> unemployment$residuals
 [1]  0.55175451  1.31254386  0.16269612  0.27910835 -0.34049690 -0.12636236
 [7]  0.32926122  0.05980856 -0.40128612 -0.77620978 -0.69839784 -0.09003919
[13] -0.26238041

# Fill in the blanks to plot predictions (on x-axis) versus the residuals
ggplot(unemployment, aes(x = predictions, y = residuals)) + 
  geom_pointrange(aes(ymin = 0, ymax = residuals)) + 
  geom_hline(yintercept = 0, linetype = 3) + 
  ggtitle("residuals vs. linear model prediction")

---------------------------------------------------------------

The gain curve to evaluate the unemployment model
In the previous exercise you made predictions about female_unemployment and visualized the predictions and the residuals. Now, you will also plot the gain curve of the unemployment_model's predictions against actual female_unemployment using the WVPlots::GainCurvePlot() function.

For situations where order is more important than exact values, the gain curve helps you check if the model's predictions sort in the same order as the true outcome.

Calls to the function GainCurvePlot() look like:

GainCurvePlot(frame, xvar, truthvar, title)
where

frame is a data frame
xvar and truthvar are strings naming the prediction and actual outcome columns of frame
title is the title of the plot
When the predictions sort in exactly the same order, the relative Gini coefficient is 1. When the model sorts poorly, the relative Gini coefficient is close to zero, or even negative.

    - The data frame unemployment and the model unemployment_model are in the workspace.
        . Load the package WVPlots using library().
        . Plot the gain curve. Give the plot the title "Unemployment model". 
            Do the model's predictions sort correctly?

--------------------------------------------------------------------
> unemployment
   male_unemployment female_unemployment predictions   residuals
1                2.9                 4.0    3.448245 -0.55175451
2                6.7                 7.4    6.087456 -1.31254386
3                4.9                 5.0    4.837304 -0.16269612
4                7.9                 7.2    6.920891 -0.27910835
5                9.8                 7.9    8.240497  0.34049690
6                6.9                 6.1    6.226362  0.12636236
7                6.1                 6.0    5.670739 -0.32926122
8                6.2                 5.8    5.740192 -0.05980856
9                6.0                 5.2    5.601286  0.40128612
10               5.1                 4.2    4.976210  0.77620978
11               4.7                 4.0    4.698398  0.69839784
12               4.4                 4.4    4.490039  0.09003919
13               5.8                 5.2    5.462380  0.26238041

> summary(unemployment)
 male_unemployment female_unemployment  predictions      residuals       
 Min.   :2.900     Min.   :4.000       Min.   :3.448   Min.   :-1.31254  
 1st Qu.:4.900     1st Qu.:4.400       1st Qu.:4.837   1st Qu.:-0.27911  
 Median :6.000     Median :5.200       Median :5.601   Median : 0.09004  
 Mean   :5.954     Mean   :5.569       Mean   :5.569   Mean   : 0.00000  
 3rd Qu.:6.700     3rd Qu.:6.100       3rd Qu.:6.087   3rd Qu.: 0.34050  
 Max.   :9.800     Max.   :7.900       Max.   :8.240   Max.   : 0.77621

> unemployment_model
Call:
lm(formula = fmla, data = unemployment)
Coefficients:
      (Intercept)  male_unemployment  
           1.4341             0.6945

> summary(unemployment_model)
Call:
lm(formula = fmla, data = unemployment)
Residuals:
     Min       1Q   Median       3Q      Max 
-0.77621 -0.34050 -0.09004  0.27911  1.31254 
Coefficients:
                  Estimate Std. Error t value Pr(>|t|)    
(Intercept)        1.43411    0.60340   2.377   0.0367 *  
male_unemployment  0.69453    0.09767   7.111 1.97e-05 ***
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
Residual standard error: 0.5803 on 11 degrees of freedom
Multiple R-squared:  0.8213,	Adjusted R-squared:  0.8051 
F-statistic: 50.56 on 1 and 11 DF,  p-value: 1.966e-05

# Plot the Gain Curve
GainCurvePlot(unemployment, "predictions", "female_unemployment", "Unemployment model")

------------------------------------------------------------

____________________________________
***Root Mean Squared Error (RMSE)***
____________________________________

- Root Mean Squared Error (RMSE) - a key metric for evaluating the prediction performance of 
                                    a regression model.
                                 - the "typical" prediction error of the model on that data
                   RMSE = square-root(mean((pred - y)square))
        . RMSE is defined as the square root of the mean squared error of the mode; on a data
        . pred - y = the error, or residuals vector
        . mean((pred y)square) = mean value of ((pred - y)square)

- RMSE of model
                  
                  # calculating error
                  err <- dataframe$prediction - dataframe$actual
                  
                  # square the error vector
                  err2 <- err^2
                  
                  # take the mean, and sqrt it
                  (rmse <- sqrt(mean(err2))
                  
                  # the standard deviation of the outcome
                  (sdtemp <- sd(dataframe$actual))
    
    - Evaluate the RMSE by comparing it to the standard deviation outcome.
        . RMSE < standard deviation: model tends to estimate interest variable better than
                                      simply taking the average


**Practices**

Calculate RMSE
In this exercise you will calculate the RMSE of your unemployment model. In the previous coding exercises, you added two columns to the unemployment dataset:

the model's predictions (predictions column)
the residuals between the predictions and the outcome (residuals column)
You can calculate the RMSE from a vector of residuals, res, as:

RMSE=square-root(mean((res)square))
You want RMSE to be small. How small is "small"? One heuristic is to compare the RMSE to the standard deviation of the outcome. With a good model, the RMSE should be smaller.

The data frame unemployment is in your workspace.

    - Review the unemployment data from the previous exercise.
    - For convenience, assign the residuals column from unemployment to the variable res.
    - Calculate RMSE: square res, take its mean, and then square root it. 
        Assign this to the variable rmse and print it.
      . Tip: you can do this in one step by wrapping the assignment 
            in parentheses: (rmse <- ___)
    - Calculate the standard deviation of female_unemployment and assign it to the variable
        sd_unemployment. Print it. How does the rmse of the model compare to the 
        standard deviation of the data?

-------------------------------------------------

> unemployment
   male_unemployment female_unemployment predictions   residuals
1                2.9                 4.0    3.448245 -0.55175451
2                6.7                 7.4    6.087456 -1.31254386
3                4.9                 5.0    4.837304 -0.16269612
4                7.9                 7.2    6.920891 -0.27910835
5                9.8                 7.9    8.240497  0.34049690
6                6.9                 6.1    6.226362  0.12636236
7                6.1                 6.0    5.670739 -0.32926122
8                6.2                 5.8    5.740192 -0.05980856
9                6.0                 5.2    5.601286  0.40128612
10               5.1                 4.2    4.976210  0.77620978
11               4.7                 4.0    4.698398  0.69839784
12               4.4                 4.4    4.490039  0.09003919
13               5.8                 5.2    5.462380  0.26238041

> summary(unemployment)
 male_unemployment female_unemployment  predictions      residuals       
 Min.   :2.900     Min.   :4.000       Min.   :3.448   Min.   :-1.31254  
 1st Qu.:4.900     1st Qu.:4.400       1st Qu.:4.837   1st Qu.:-0.27911  
 Median :6.000     Median :5.200       Median :5.601   Median : 0.09004  
 Mean   :5.954     Mean   :5.569       Mean   :5.569   Mean   : 0.00000  
 3rd Qu.:6.700     3rd Qu.:6.100       3rd Qu.:6.087   3rd Qu.: 0.34050  
 Max.   :9.800     Max.   :7.900       Max.   :8.240   Max.   : 0.77621
 
> # For convenience put the residuals in the variable res
> res <- unemployment$residuals
> 
> # Calculate RMSE, assign it to the variable rmse and print it
> (rmse <- sqrt(mean(res^2)))
[1] 0.5337612
> 
> # Calculate the standard deviation of female_unemployment and print it
> (sd_unemployment <- sd(unemployment$female_unemployment))
[1] 1.314271

-------------------------------------------------------

_____________________
***R-Squared (R^2)***
_____________________

- R-Squared - another key metric for evaluating regression models
            - a measure of how well a model fits the data
            - a value between 0 and 1
                  . near 1: model fits well
                  . near 0: no better than guessing the average value
            - is the variance explained by the model
            - is defined as 1 minus the ratio of the residual sum of squares 
                to the total sum of squares
                
                (R^2) = 1 - (RSS/SStotal)
                  . RSS = sum((y-prediction)square)
                        -> Residual sum of squares (variance from model)
                  . SStotal = sum((y - (mean y))square)
                        -> Total sum of sqaures (variance of data)
                  
                  . If the RSS < SStotal -> R-Square is large -> the model fits data well.
                  
- Calculate R-Squared
              
              # calculate error between prediction and actual
              err <- dataframe$prediction - dataframe$actual
              
              # square it and take the sum
              rss <- sum(err^2)
              
              . actual: column of actual
              . prediction : column of prediction
              
              # take the difference of prices from the mean price
              toterr <- dataframe$actual - mean(dataframe$actual)
              
              # square it and take the sum
              sstot <- sum(toterr^2)
              
              # calculate r-squared
              r_squared <- 1 - (rss/sstottal)

- Reading R-Squared from lm() model
              # from summary(model)
              
              # from summary(model)$r.squared
              
              # from glance(model)$r.squared

- Correlation and R-squared
              
               rho <- cor(dataframe$prediction, dataframe$actual)
               
               rho^2
               
               p = cor(prediction, actual)
               p^2 = R^2
               
      -> For models that minimize squared error (like linear regression),
          the R-squared is the square of the correlation between the outcome
          and the prediction.
      -> the predictions and the true outcome are correlated

- Correlation and R-squared
    . True for modelss that minimize squared error:
        ~ Linear regression
        ~ GAM regression
        ~ Tree-based algorithms that minimize squred error
    . True for training data; NOT true for future application data

**Practices**

Calculate R-Squared
Now that you've calculated the RMSE of your model's predictions, you will examine how well the model fits the data: that is, how much variance does it explain. You can do this using R2.

Suppose y is the true outcome, p is the prediction from the model, and res=y−p are the residuals of the predictions.

Then the total sum of squares tss ("total variance") of the data is:

tss=∑(y−y¯¯¯)2
where y¯¯¯ is the mean value of y.

The residual sum of squared errors of the model, rss is:
rss=∑res2
R2 (R-Squared), the "variance explained" by the model, is then:

1−rss/tss
After you calculate R2, you will compare what you computed with the R2 reported by glance(). glance() returns a one-row data frame; for a linear regression model, one of the columns returned is the R2 of the model on the training data.

The data frame unemployment is in your workspace, with the columns predictions and residuals that you calculated in a previous exercise.

The data frame unemployment and the model unemployment_model are in the workspace.

    - Calculate the mean female_unemployment and assign it to the variable fe_mean.
    - Calculate the total sum of squares and assign it to the variable tss.
    - Calculate the residual sum of squares and assign it to the variable rss.
    - Calculate R2. Is it a good fit (R2 near 1)?
    - Use glance() to get R2 from the model. Is it the same as what you calculated?

----------------------------------------------------------------

> unemployment
   male_unemployment female_unemployment predictions   residuals
1                2.9                 4.0    3.448245 -0.55175451
2                6.7                 7.4    6.087456 -1.31254386
3                4.9                 5.0    4.837304 -0.16269612
4                7.9                 7.2    6.920891 -0.27910835
5                9.8                 7.9    8.240497  0.34049690
6                6.9                 6.1    6.226362  0.12636236
7                6.1                 6.0    5.670739 -0.32926122
8                6.2                 5.8    5.740192 -0.05980856
9                6.0                 5.2    5.601286  0.40128612
10               5.1                 4.2    4.976210  0.77620978
11               4.7                 4.0    4.698398  0.69839784
12               4.4                 4.4    4.490039  0.09003919
13               5.8                 5.2    5.462380  0.26238041

> summary(unemployment)
 male_unemployment female_unemployment  predictions      residuals       
 Min.   :2.900     Min.   :4.000       Min.   :3.448   Min.   :-1.31254  
 1st Qu.:4.900     1st Qu.:4.400       1st Qu.:4.837   1st Qu.:-0.27911  
 Median :6.000     Median :5.200       Median :5.601   Median : 0.09004  
 Mean   :5.954     Mean   :5.569       Mean   :5.569   Mean   : 0.00000  
 3rd Qu.:6.700     3rd Qu.:6.100       3rd Qu.:6.087   3rd Qu.: 0.34050  
 Max.   :9.800     Max.   :7.900       Max.   :8.240   Max.   : 0.77621

> unemployment_model
Call:
lm(formula = fmla, data = unemployment)
Coefficients:
      (Intercept)  male_unemployment  
           1.4341             0.6945

> summary(unemployment_model)
Call:
lm(formula = fmla, data = unemployment)
Residuals:
     Min       1Q   Median       3Q      Max 
-0.77621 -0.34050 -0.09004  0.27911  1.31254 
Coefficients:
                  Estimate Std. Error t value Pr(>|t|)    
(Intercept)        1.43411    0.60340   2.377   0.0367 *  
male_unemployment  0.69453    0.09767   7.111 1.97e-05 ***
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
Residual standard error: 0.5803 on 11 degrees of freedom
Multiple R-squared:  0.8213,	Adjusted R-squared:  0.8051 
F-statistic: 50.56 on 1 and 11 DF,  p-value: 1.966e-05

# Calculate mean female_unemployment: fe_mean. Print it
> (fe_mean <- mean(unemployment$female_unemployment))
[1] 5.569231

# Calculate total sum of squares: tss. Print it
> (tss <- sum((unemployment$female_unemployment - fe_mean)^2))
[1] 20.72769

# Calculate residual sum of squares: rss. Print it
> (rss <- sum(unemployment$residuals^2))
[1] 3.703714
 
# Calculate R-squared: rsq. Print it. Is it a good fit?
> (rsq <- 1 - (rss/tss))
[1] 0.8213157
 
# Get R-squared from glance. Print it
> (rsq_glance <- glance(unemployment_model)$r.squared)
[1] 0.8213157

-----------------------------------------------------------------

Correlation and R-squared
The linear correlation of two variables, x and y, measures the strength of the linear relationship between them. When x and y are respectively:

the outcomes of a regression model that minimizes squared-error (like linear regression) and
the true outcomes of the training data,
then the square of the correlation is the same as R2. You will verify that in this exercise.

    - Use cor() to get the correlation between the predictions and female unemployment. 
        Assign it to the variable rho and print it. Make sure you use Pearson correlation 
        (the default).
    - Square rho and assign it to rho2. Print it.
    - Compare rho2 to R2 from the model (using glance()). Is it the same?

---------------------------------------------------------------------

> unemployment
   male_unemployment female_unemployment predictions   residuals
1                2.9                 4.0    3.448245 -0.55175451
2                6.7                 7.4    6.087456 -1.31254386
3                4.9                 5.0    4.837304 -0.16269612
4                7.9                 7.2    6.920891 -0.27910835
5                9.8                 7.9    8.240497  0.34049690
6                6.9                 6.1    6.226362  0.12636236
7                6.1                 6.0    5.670739 -0.32926122
8                6.2                 5.8    5.740192 -0.05980856
9                6.0                 5.2    5.601286  0.40128612
10               5.1                 4.2    4.976210  0.77620978
11               4.7                 4.0    4.698398  0.69839784
12               4.4                 4.4    4.490039  0.09003919
13               5.8                 5.2    5.462380  0.26238041

# unemployment is in your workspace
> summary(unemployment)
 male_unemployment female_unemployment  predictions      residuals       
 Min.   :2.900     Min.   :4.000       Min.   :3.448   Min.   :-1.31254  
 1st Qu.:4.900     1st Qu.:4.400       1st Qu.:4.837   1st Qu.:-0.27911  
 Median :6.000     Median :5.200       Median :5.601   Median : 0.09004  
 Mean   :5.954     Mean   :5.569       Mean   :5.569   Mean   : 0.00000  
 3rd Qu.:6.700     3rd Qu.:6.100       3rd Qu.:6.087   3rd Qu.: 0.34050  
 Max.   :9.800     Max.   :7.900       Max.   :8.240   Max.   : 0.77621
 
# unemployment_model is in the workspace
> summary(unemployment_model)
Call:
lm(formula = fmla, data = unemployment)
Residuals:
     Min       1Q   Median       3Q      Max 
-0.77621 -0.34050 -0.09004  0.27911  1.31254 
Coefficients:
                  Estimate Std. Error t value Pr(>|t|)    
(Intercept)        1.43411    0.60340   2.377   0.0367 *  
male_unemployment  0.69453    0.09767   7.111 1.97e-05 ***
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
Residual standard error: 0.5803 on 11 degrees of freedom
Multiple R-squared:  0.8213,	Adjusted R-squared:  0.8051 
F-statistic: 50.56 on 1 and 11 DF,  p-value: 1.966e-05
 
# Get the correlation between the prediction and true outcome: rho and print it
> (rho <- cor(unemployment$predictions, unemployment$female_unemployment))
[1] 0.9062647
 
# Square rho: rho2 and print it
> (rho2 <- rho^2)
[1] 0.8213157

# Get R-squared from glance and print it
> (rsq_glance <- glance(unemployment_model)$r.squared)
[1] 0.8213157
 
----------------------------------------------------------------------------------

_______________________________
***Properly Training a Model***
_______________________________

- In general, a model performs better on its own training data than on data it hasn't yet seen
        ex: Training R2: 0.9
            Test R2: 0.15
            --> Overfit
- For simple models like linear regression, this upward bias is often not severe; but for 
  more complex models, or even for a linear model with too many variables, using only the
  training data to evaluate the model can produce misleading results.

- With a lot of data, it's best to split it in two:
          . one set to train the model
          . one set to test the model

- If there is not enough data to split, use cross-validation to estimate a model's out of sample performance.
    . In n-fold cross-validation, you partition the data into n-subsets.
           
        ex: 3-fold cross-validation to partition the data into 3 subsets: A, B & C
                
                1. train a model using the data from set A & B, and use that model to make
                      predictions on C
                2. train a model on B and C to predict on A
                3. train a model on A and C to predict on B
                
                None of these models has made predictions on its own training data
                All the predictions are essentially "test set" predictions.
        -> RMSE and R-squared calculated from these predictions should give an
            an biased estimate of how a model fit to all the training data will perform
            on future data.
                  
- Create a cross-validation plan
            
            library(vtreat)
            splitPlan <- kWayCrossValidation(nRows, nSplits, NULL, NULL)
                    . nRows: # of rows in the training data
                    . nSplits: # folds (partitions) in the cross-validation
                          ex: nfolds = 3 --> 3 way cross validation
                    . remaining 2 arguments not needed her.
                    
            -> returns the indices for training and testing for each fold.
                Use the data with the training indices to fit a model and
                then make prediction on the data with the app, or test, indices.
                
              # first fold (A and B to train, C to test)
              splitPlan[[1]]
              
              # train on A & B, test on C
              split <- splitPlan[[1]]
              model <- lm(fmla, data = df[split$train, ])
              df$pred.cv[split$app] <- predict(model, newdata = df[split$app, ])
              
- If the estimated model performance looks good enough, then use all the data to fit 
    a final model
- You can't evaluate this final model's future performance, because you don't have data 
    to evaluate it with
    
- Cross-validation only tests the modeling process, 
    while test/train split tests the final model.
    
**Practices**

Generating a random test/train split
For the next several exercises you will use the mpg data from the package ggplot2. The data describes the characteristics of several makes and models of cars from different years. The goal is to predict city fuel efficiency from highway fuel efficiency.

In this exercise, you will split mpg into a training set mpg_train (75% of the data) and a test set mpg_test (25% of the data). One way to do this is to generate a column of uniform random numbers between 0 and 1, using the function runif().

If you have a data set dframe of size N, and you want a random subset of approximately size 100∗X% of N (where X is between 0 and 1), then:

Generate a vector of uniform random numbers: gp = runif(N).
dframe[gp < X,] will be about the right size.
dframe[gp >= X,] will be the complement.

The data frame mpg is in the workspace.

    - Use the function nrow to get the number of rows in the data frame mpg. 
        Assign this count to the variable N and print it.
    - Calculate about how many rows 75% of N should be. 
        Assign it to the variable target and print it.
    - Use runif() to generate a vector of N uniform random numbers, called gp.
    - Use gp to split mpg into mpg_train and mpg_test (with mpg_train containing 
        approximately 75% of the data).
    - Use nrow() to check the size of mpg_train and mpg_test. 
      Are they about the right size?

------------------------------------------------------------------

# load tidyverse to get the mpg dataset
# mpg is in the workspace

summary(mpg)
 manufacturer          model               displ            year     
 Length:234         Length:234         Min.   :1.600   Min.   :1999  
 Class :character   Class :character   1st Qu.:2.400   1st Qu.:1999  
 Mode  :character   Mode  :character   Median :3.300   Median :2004  
                                       Mean   :3.472   Mean   :2004  
                                       3rd Qu.:4.600   3rd Qu.:2008  
                                       Max.   :7.000   Max.   :2008  
      cyl           trans               drv                 cty       
 Min.   :4.000   Length:234         Length:234         Min.   : 9.00  
 1st Qu.:4.000   Class :character   Class :character   1st Qu.:14.00  
 Median :6.000   Mode  :character   Mode  :character   Median :17.00  
 Mean   :5.889                                         Mean   :16.86  
 3rd Qu.:8.000                                         3rd Qu.:19.00  
 Max.   :8.000                                         Max.   :35.00  
      hwy             fl               class          
 Min.   :12.00   Length:234         Length:234        
 1st Qu.:18.00   Class :character   Class :character  
 Median :24.00   Mode  :character   Mode  :character  
 Mean   :23.44                                        
 3rd Qu.:27.00                                        
 Max.   :44.00

dim(mpg)
[1] 234  11

# Use nrow to get the number of rows in mpg (N) and print it
(N <- nrow(mpg))
>[1] 234
 
# Calculate how many rows 75% of N should be and print it
# Hint: use round() to get an integer
(target <- round(N * 0.75))
>[1] 176
 
# Create the vector of N uniform random variables: gp
gp <- runif(N)
 
# Use gp to create the training set: mpg_train (75% of data) and mpg_test (25% of data)
> mpg_train <- mpg[gp < 0.75, ]
> mpg_test <- mpg[gp >= 0.75, ]

# Use nrow() to examine mpg_train and mpg_test
> nrow(mpg_train)
[1] 179

> nrow(mpg_test)
[1] 55

--------------------------------------------------------------------------

Train a model using test/train split
Now that you have split the mpg dataset into mpg_train and mpg_test, you will use mpg_train to train a model to predict city fuel efficiency (cty) from highway fuel efficiency (hwy).

    - The data frame mpg_train is in the workspace.

    - Create a formula fmla that expresses the relationship cty as a function of hwy.
        Print it.
    - Train a model mpg_model on mpg_train to predict cty from hwy using fmla and lm().
    - Use summary() to examine the model.

-----------------------------------------------------------------------------

 # mpg_train is in the workspace
> summary(mpg_train)
 manufacturer          model               displ            year     
 Length:180         Length:180         Min.   :1.600   Min.   :1999  
 Class :character   Class :character   1st Qu.:2.500   1st Qu.:1999  
 Mode  :character   Mode  :character   Median :3.400   Median :2008  
                                       Mean   :3.558   Mean   :2004  
                                       3rd Qu.:4.600   3rd Qu.:2008  
                                       Max.   :7.000   Max.   :2008  
      cyl           trans               drv                 cty       
 Min.   :4.000   Length:180         Length:180         Min.   : 9.00  
 1st Qu.:4.000   Class :character   Class :character   1st Qu.:14.00  
 Median :6.000   Mode  :character   Mode  :character   Median :16.00  
 Mean   :6.022                                         Mean   :16.58  
 3rd Qu.:8.000                                         3rd Qu.:19.00  
 Max.   :8.000                                         Max.   :33.00  
      hwy             fl               class          
 Min.   :12.00   Length:180         Length:180        
 1st Qu.:18.00   Class :character   Class :character  
 Median :24.00   Mode  :character   Mode  :character  
 Mean   :23.11                                        
 3rd Qu.:27.00                                        
 Max.   :44.00
 
# Create a formula to express cty as a function of hwy: fmla and print it.
> (fmla <- cty ~ hwy)
cty ~ hwy

# Now use lm() to build a model mpg_model from mpg_train that predicts cty from hwy
> (mpg_model <- lm(fmla, mpg_train))
Call:
lm(formula = fmla, data = mpg_train)
Coefficients:
(Intercept)          hwy  
     1.1337       0.6683
 
# Use summary() to examine the model
> summary(mpg_model)
Call:
lm(formula = fmla, data = mpg_train)
Residuals:
    Min      1Q  Median      3Q     Max 
-2.8400 -0.8305 -0.1551  0.5865  4.8140 
Coefficients:
            Estimate Std. Error t value Pr(>|t|)    
(Intercept)  1.13375    0.38309   2.959   0.0035 ** 
hwy          0.66825    0.01608  41.564   <2e-16 ***
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
Residual standard error: 1.251 on 178 degrees of freedom
Multiple R-squared:  0.9066,	Adjusted R-squared:  0.9061 
F-statistic:  1728 on 1 and 178 DF,  p-value: < 2.2e-16

-------------------------------------------------------------------

Evaluate a model using test/train split
Now you will test the model mpg_model on the test data, mpg_test. Functions rmse() and r_squared() to calculate RMSE and R-squared have been provided for convenience:

rmse(predcol, ycol)
r_squared(predcol, ycol)
where:

predcol: The predicted values
ycol: The actual outcome
You will also plot the predictions vs. the outcome.

Generally, model performance is better on the training data than the test data (though sometimes the test set "gets lucky"). A slight difference in performance is okay; if the performance on training is significantly better, there is a problem.

The data frames mpg_train and mpg_test, and the model mpg_model are in the workspace, along with the functions rmse() and r_squared().

    - Predict city fuel efficiency from hwy on the mpg_train data. 
        Assign the predictions to the column pred.
    - Predict city fuel efficiency from hwy on the mpg_test data. 
        Assign the predictions to the column pred.
    - Use rmse() to evaluate rmse for both the test and training sets. Compare. 
        Are the performances similar?
    - Do the same with r_squared(). Are the performances similar?
    - Use ggplot2 to plot the predictions against cty on the test data.

---------------------------------------------------------------------

# Examine the objects in the workspace
ls.str()
>mpg_model : List of 12
 $ coefficients : Named num [1:2] 1.134 0.668
 $ residuals    : Named num [1:180] -2.513 0.487 -1.85 -2.508 -0.508 ...
 $ effects      : Named num [1:180] -222.414 -51.994 -1.735 -2.354 -0.354 ...
 $ rank         : int 2
 $ fitted.values: Named num [1:180] 20.5 20.5 21.8 18.5 18.5 ...
 $ assign       : int [1:2] 0 1
 $ qr           :List of 5
 $ df.residual  : int 178
 $ xlevels      : Named list()
 $ call         : language lm(formula = fmla, data = mpg_train)
 $ terms        :Classes 'terms', 'formula'  language cty ~ hwy
 $ model        :'data.frame':	180 obs. of  2 variables:
mpg_test : Classes 'tbl_df', 'tbl' and 'data.frame':	54 obs. of  11 variables:
 $ manufacturer: chr  "audi" "audi" "audi" "audi" ...
 $ model       : chr  "a4" "a4" "a4 quattro" "a4 quattro" ...
 $ displ       : num  2 3.1 1.8 2 4.2 5.3 6.5 2.4 3.7 5.2 ...
 $ year        : int  2008 2008 1999 2008 2008 2008 1999 1999 2008 1999 ...
 $ cyl         : int  4 6 4 4 8 8 8 4 6 8 ...
 $ trans       : chr  "auto(av)" "auto(av)" "auto(l5)" "auto(s6)" ...
 $ drv         : chr  "f" "f" "4" "4" ...
 $ cty         : int  21 18 16 19 16 11 14 19 15 11 ...
 $ hwy         : int  30 27 25 27 23 14 17 27 19 17 ...
 $ fl          : chr  "p" "p" "p" "p" ...
 $ class       : chr  "compact" "compact" "compact" "compact" ...
mpg_train : Classes 'tbl_df', 'tbl' and 'data.frame':	180 obs. of  11 variables:
 $ manufacturer: chr  "audi" "audi" "audi" "audi" ...
 $ model       : chr  "a4" "a4" "a4" "a4" ...
 $ displ       : num  1.8 1.8 2 2.8 2.8 1.8 2 2.8 2.8 3.1 ...
 $ year        : int  1999 1999 2008 1999 1999 1999 2008 1999 1999 2008 ...
 $ cyl         : int  4 4 4 6 6 4 4 6 6 6 ...
 $ trans       : chr  "auto(l5)" "manual(m5)" "manual(m6)" "auto(l5)" ...
 $ drv         : chr  "f" "f" "f" "f" ...
 $ cty         : int  18 21 20 16 18 18 20 15 17 17 ...
 $ hwy         : int  29 29 31 26 26 26 28 25 25 25 ...
 $ fl          : chr  "p" "p" "p" "p" ...
 $ class       : chr  "compact" "compact" "compact" "compact" ...
r_squared : function (predcol, ycol)  
rmse : function (predcol, ycol)

# predict cty from hwy for the training set
mpg_train$pred <- predict(mpg_model)

# predict cty from hwy for the test set
mpg_test$pred <- predict(mpg_model, newdata = mpg_test)

# Evaluate the rmse on both training and test data and print them
(rmse_train <- rmse(mpg_train$pred, mpg_train$cty))
> [1] 1.243958

(rmse_test <- rmse(mpg_test$pred, mpg_test$cty))
>[1] 1.277228
 
 
# Evaluate the r-squared on both training and test data.and print them
(rsq_train <- r_squared(mpg_train$pred, mpg_train$cty))
>[1] 0.9065908

(rsq_test <- r_squared(mpg_test$pred, mpg_test$cty))
>[1] 0.9251412
 
# Plot the predictions (on the x-axis) against the outcome (cty) on the test data
ggplot(mpg_test, aes(x = pred, y = cty)) + 
    geom_point() + 
    geom_abline()

------------------------------------------------------------

Create a cross validation plan
There are several ways to implement an n-fold cross validation plan. In this exercise you will create such a plan using vtreat::kWayCrossValidation(), and examine it.

kWayCrossValidation() creates a cross validation plan with the following call:

        splitPlan <- kWayCrossValidation(nRows, nSplits, dframe, y)
        
where nRows is the number of rows of data to be split, and nSplits is the desired number of cross-validation folds.

Strictly speaking, dframe and y aren't used by kWayCrossValidation; they are there for compatibility with other vtreat data partitioning functions. You can set them both to NULL.

The resulting splitPlan is a list of nSplits elements; each element contains two vectors:

train: the indices of dframe that will form the training set
app: the indices of dframe that will form the test (or application) set
In this exercise you will create a 3-fold cross-validation plan for the data set mpg.

    - Load the package vtreat.
    - Get the number of rows in mpg and assign it to the variable nRows.
    - Call kWayCrossValidation to create a 3-fold cross validation plan 
        and assign it to the variable splitPlan.
    - You can set the last two arguments of the function to NULL.
    - Call str() to examine the structure of splitPlan.
    
--------------------------------------------------------------- 

# Load the package vtreat
library(vtreat)

# mpg is in the workspace
summary(mpg)
>manufacturer          model               displ            year     
 Length:234         Length:234         Min.   :1.600   Min.   :1999  
 Class :character   Class :character   1st Qu.:2.400   1st Qu.:1999  
 Mode  :character   Mode  :character   Median :3.300   Median :2004  
                                       Mean   :3.472   Mean   :2004  
                                       3rd Qu.:4.600   3rd Qu.:2008  
                                       Max.   :7.000   Max.   :2008  
      cyl           trans               drv                 cty       
 Min.   :4.000   Length:234         Length:234         Min.   : 9.00  
 1st Qu.:4.000   Class :character   Class :character   1st Qu.:14.00  
 Median :6.000   Mode  :character   Mode  :character   Median :17.00  
 Mean   :5.889                                         Mean   :16.86  
 3rd Qu.:8.000                                         3rd Qu.:19.00  
 Max.   :8.000                                         Max.   :35.00  
      hwy             fl               class          
 Min.   :12.00   Length:234         Length:234        
 1st Qu.:18.00   Class :character   Class :character  
 Median :24.00   Mode  :character   Mode  :character  
 Mean   :23.44                                        
 3rd Qu.:27.00                                        
 Max.   :44.00
 
# Get the number of rows in mpg
nRows <- nrow(mpg)
 
# Implement the 3-fold cross-fold plan with vtreat
splitPlan <- kWayCrossValidation(nRows, 3, NULL, NULL)

# Examine the split plan
str(splitPlan)
>List of 3
 $ :List of 2
  ..$ train: int [1:156] 1 2 4 5 8 10 12 13 15 16 ...
  ..$ app  : int [1:78] 220 3 7 213 175 163 176 179 214 228 ...
 $ :List of 2
  ..$ train: int [1:156] 2 3 4 5 6 7 9 11 13 14 ...
  ..$ app  : int [1:78] 19 93 53 144 64 12 57 225 81 166 ...
 $ :List of 2
  ..$ train: int [1:156] 1 3 6 7 8 9 10 11 12 14 ...
  ..$ app  : int [1:78] 150 65 33 230 105 116 5 125 135 172 ...
 - attr(*, "splitmethod")= chr "kwaycross"
 
-------------------------------------------------------------------

Evaluate a modeling procedure using n-fold cross-validation
In this exercise you will use splitPlan, the 3-fold cross validation plan from the previous exercise, to make predictions from a model that predicts mpg$cty from mpg$hwy.

If dframe is the training data, then one way to add a column of cross-validation predictions to the frame is as follows:


# Initialize a column of the appropriate length
dframe$pred.cv <- 0 

# k is the number of folds
# splitPlan is the cross validation plan

for(i in 1:k) {
  # Get the ith split
  split <- splitPlan[[i]]

  # Build a model on the training data 
  # from this split 
  # (lm, in this case)
  model <- lm(fmla, data = dframe[split$train,])

  # make predictions on the 
  # application data from this split
  dframe$pred.cv[split$app] <- predict(model, newdata = dframe[split$app,])
          }


Cross-validation predicts how well a model built from all the data will perform on new data. As with the test/train split, for a good modeling procedure, cross-validation performance and training performance should be close.

The data frame mpg, the cross validation plan splitPlan, and the function to calculate RMSE (rmse()) from one of the previous exercises is available in your workspace.

    - Run the 3-fold cross validation plan from splitPlan and put the predictions 
      in the column mpg$pred.cv.
    - Use lm() and the formula cty ~ hwy.
    - Create a linear regression model on all the mpg data (formula cty ~ hwy) 
        and assign the predictions to mpg$pred.
    - Use rmse() to get the root mean squared error of the predictions 
        from the full model (mpg$pred). Recall that rmse() takes two arguments, 
        the predicted values, and the actual outcome.
    - Get the root mean squared error of the cross-validation predictions. 
        Are the two values about the same?

---------------------------------------------------------------------------

# mpg is in the workspace
> summary(mpg)
 manufacturer          model               displ            year     
 Length:234         Length:234         Min.   :1.600   Min.   :1999  
 Class :character   Class :character   1st Qu.:2.400   1st Qu.:1999  
 Mode  :character   Mode  :character   Median :3.300   Median :2004  
                                       Mean   :3.472   Mean   :2004  
                                       3rd Qu.:4.600   3rd Qu.:2008  
                                       Max.   :7.000   Max.   :2008  
      cyl           trans               drv                 cty       
 Min.   :4.000   Length:234         Length:234         Min.   : 9.00  
 1st Qu.:4.000   Class :character   Class :character   1st Qu.:14.00  
 Median :6.000   Mode  :character   Mode  :character   Median :17.00  
 Mean   :5.889                                         Mean   :16.86  
 3rd Qu.:8.000                                         3rd Qu.:19.00  
 Max.   :8.000                                         Max.   :35.00  
      hwy             fl               class          
 Min.   :12.00   Length:234         Length:234        
 1st Qu.:18.00   Class :character   Class :character  
 Median :24.00   Mode  :character   Mode  :character  
 Mean   :23.44                                        
 3rd Qu.:27.00                                        
 Max.   :44.00

# splitPlan is in the workspace
str(splitPlan)
> List of 3
 $ :List of 2
  ..$ train: int [1:156] 3 5 8 9 10 11 13 14 17 18 ...
  ..$ app  : int [1:78] 80 4 216 60 227 166 220 119 73 168 ...
 $ :List of 2
  ..$ train: int [1:156] 1 2 3 4 5 6 7 10 11 12 ...
  ..$ app  : int [1:78] 54 180 174 162 14 79 230 150 151 17 ...
 $ :List of 2
  ..$ train: int [1:156] 1 2 4 6 7 8 9 12 14 15 ...
  ..$ app  : int [1:78] 11 58 142 219 141 13 3 94 128 76 ...
 - attr(*, "splitmethod")= chr "kwaycross"

# Run the 3-fold cross validation plan from splitPlan
k <- 3 # Number of folds

mpg$pred.cv <- 0

for(i in 1:k) {
    split <- splitPlan[[i]]
    model <- lm(cty ~ hwy, data = mpg[split$train, ])
    mpg$pred.cv[split$app] <- predict(model, newdata = mpg[split$app, ])
  }

# Predict from a full model
mpg$pred <- predict(lm(cty ~ hwy, data = mpg))
 
# Get the rmse of the full model's predictions
rmse(mpg$pred, mpg$cty)
> [1] 1.247045

# Get the rmse of the cross-validation predictions
rmse(mpg$pred.cv, mpg$cty)
> [1] 1.260323

---------------------------------------------------------------------------------

***
### PART 3: ISSUES TO CONSIDER ###
***


________________________
***Categorical Inputs***
________________________

- In many R modeling functions, categorical variables with N possible levels are represented 
    under the covers as N-1 0/1, or indicator, variables
    
          model.matrix()
              model.maxtrix(response ~ predictor1 + predictor2, dataset)
                  . all numberical values
                  . converts categorical variable with N levels into N-1 indicator variables
                        -> called "one-hot-encoding"
                  
                  ex:
                    model.matrix(WtLoss24 ~ Diet + Age + BMI, data = diet)

- Under the covers, the lm function solves a numerical problem, with a coefficient for every
    indicator variable. Recall that for a linear model the intercept is the predicted value when
    all the variable equal 0

- Because categorical variable with K possible values encodes to K-1 variables, using variables
  like ZIP code with many many possible values can be dangerous, both for computational reasons
  and because too many variables relative to the number of datums can lead to overfit.
- Issues with one-hot-encoding:
    . Too many levels can be a problem
        ~ex: Zip code (about 40,000 codes)
    . Don't hash with geometric methods

**Practices**

Examining the structure of categorical inputs
For this exercise you will call model.matrix() to examine how R represents data with both categorical and numerical inputs for modeling. The dataset flowers (derived from the Sleuth3 package) is loaded into your workspace. It has the following columns:

Flowers: the average number of flowers on a meadowfoam plant
Intensity: the intensity of a light treatment applied to the plant
Time: A categorical variable - when (Late or Early) in the lifecycle the light treatment occurred
The ultimate goal is to predict Flowers as a function of Time and Intensity.
                  
The data frame flowers is in your workspace.

    - Call the str() function on flowers to see the types of each column.
    - Use the unique() function on the column flowers$Time to see the possible values 
        that Time takes. How many unique values are there?
    - Create a formula to express Flowers as a function of Intensity and Time. 
        Assign it to the variable fmla and print it.
    - Use fmla and model.matrix() to create the model matrix for the data frame flowers. 
        Assign it to the variable mmat.
    - Use head() to examine the first 20 lines of flowers.
    - Now examine the first 20 lines of mmat.
          . Is the numeric column Intensity different?
          . What happened to the categorical column Time from flowers?
          . How is Time == 'Early' represented? And Time == 'Late'?                  
                  
------------------------------------------------------------------------

flowers
> Flowers  Time Intensity
1     62.3  Late       150
2     77.4  Late       150
3     55.3  Late       300
4     54.2  Late       300
5     49.6  Late       450
6     61.9  Late       450
7     39.4  Late       600
8     45.7  Late       600
9     31.3  Late       750
10    44.9  Late       750
11    36.8  Late       900
12    41.9  Late       900
13    77.8 Early       150
14    75.6 Early       150
15    69.1 Early       300
16    78.0 Early       300
17    57.0 Early       450
18    71.1 Early       450
19    62.9 Early       600
20    52.2 Early       600
21    60.3 Early       750
22    45.6 Early       750
23    52.6 Early       900
24    44.4 Early       900

# Call str on flowers to see the types of each column
str(flowers)
>'data.frame':	24 obs. of  3 variables:
 $ Flowers  : num  62.3 77.4 55.3 54.2 49.6 61.9 39.4 45.7 31.3 44.9 ...
 $ Time     : chr  "Late" "Late" "Late" "Late" ...
 $ Intensity: int  150 150 300 300 450 450 600 600 750 750 ...
 
# Use unique() to see how many possible values Time takes
unique(flowers$Time)
>[1] "Late"  "Early"
 
# Build a formula to express Flowers as a function of Intensity and Time: fmla. Print it
(fmla <- as.formula("Flowers ~ Intensity + Time"))
>Flowers ~ Intensity + Time
 
# Use fmla and model.matrix to see how the data is represented for modeling
mmat <- model.matrix(fmla, flowers)

# Examine the first 20 lines of flowers
head(flowers, n = 20)
> Flowers  Time Intensity
1     62.3  Late       150
2     77.4  Late       150
3     55.3  Late       300
4     54.2  Late       300
5     49.6  Late       450
6     61.9  Late       450
7     39.4  Late       600
8     45.7  Late       600
9     31.3  Late       750
10    44.9  Late       750
11    36.8  Late       900
12    41.9  Late       900
13    77.8 Early       150
14    75.6 Early       150
15    69.1 Early       300
16    78.0 Early       300
17    57.0 Early       450
18    71.1 Early       450
19    62.9 Early       600
20    52.2 Early       600
 
# Examine the first 20 lines of mmat
head(mmat, n = 20)
> (Intercept) Intensity TimeLate
1            1       150        1
2            1       150        1
3            1       300        1
4            1       300        1
5            1       450        1
6            1       450        1
7            1       600        1
8            1       600        1
9            1       750        1
10           1       750        1
11           1       900        1
12           1       900        1
13           1       150        0
14           1       150        0
15           1       300        0
16           1       300        0
17           1       450        0
18           1       450        0
19           1       600        0
20           1       600        0                  
                  
---------------------------------------------------------------------

Modeling with categorical inputs
For this exercise you will fit a linear model to the flowers data, to predict Flowers as a function of Time and Intensity.

The model formula fmla that you created in the previous exercise is still in your workspace, as is the model matrix mmat.
                  
    - Use fmla and lm to train a linear model that predicts Flowers from Intensity and Time.
        Assign the model to the variable flower_model.
    - Use summary() to remind yourself of the structure of mmat.
    - Use summary() to examine the flower_model. Do the variables match what you saw in mmat?
    - Use flower_model to predict the number of flowers. Add the predictions to flowers as 
        the column predictions.
    - Fill in the blanks to plot predictions vs. actual flowers (predictions on the x-axis).                 
---------------------------------------------------------------------    

# flowers in is the workspace
str(flowers)
>'data.frame':	24 obs. of  3 variables:
 $ Flowers  : num  62.3 77.4 55.3 54.2 49.6 61.9 39.4 45.7 31.3 44.9 ...
 $ Time     : chr  "Late" "Late" "Late" "Late" ...
 $ Intensity: int  150 150 300 300 450 450 600 600 750 750 ...
 
# fmla is in the workspace
fmla
> Flowers ~ Intensity + Time

# Fit a model to predict Flowers from Intensity and Time : flower_model
flower_model <- lm(fmla, data =  flowers)
>Call:
lm(formula = fmla, data = flowers)
Coefficients:
(Intercept)    Intensity     TimeLate  
   83.46417     -0.04047    -12.15833 

# Use summary on mmat to remind yourself of its structure
summary(mmat)
> (Intercept)   Intensity      TimeLate  
 Min.   :1    Min.   :150   Min.   :0.0  
 1st Qu.:1    1st Qu.:300   1st Qu.:0.0  
 Median :1    Median :525   Median :0.5  
 Mean   :1    Mean   :525   Mean   :0.5  
 3rd Qu.:1    3rd Qu.:750   3rd Qu.:1.0  
 Max.   :1    Max.   :900   Max.   :1.0
 
# Use summary to examine flower_model
summary(flower_model)
> Call:
lm(formula = fmla, data = flowers)
Residuals:
   Min     1Q Median     3Q    Max 
-9.652 -4.139 -1.558  5.632 12.165 
Coefficients:
              Estimate Std. Error t value Pr(>|t|)    
(Intercept)  83.464167   3.273772  25.495  < 2e-16 ***
Intensity    -0.040471   0.005132  -7.886 1.04e-07 ***
TimeLate    -12.158333   2.629557  -4.624 0.000146 ***
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
Residual standard error: 6.441 on 21 degrees of freedom
Multiple R-squared:  0.7992,	Adjusted R-squared:   0.78 
F-statistic: 41.78 on 2 and 21 DF,  p-value: 4.786e-08
 
# Predict the number of flowers on each plant
flowers$predictions <- predict(flower_model)
>  [1] 65.23512 65.23512 59.16440 59.16440 53.09369 53.09369 47.02298 47.02298
 [9] 40.95226 40.95226 34.88155 34.88155 77.39345 77.39345 71.32274 71.32274
[17] 65.25202 65.25202 59.18131 59.18131 53.11060 53.11060 47.03988 47.03988

# Plot predictions vs actual flowers (predictions on x-axis)
> ggplot(flowers, aes(x = predictions, y = Flowers)) + 
    geom_point() +
    geom_abline(color = "blue")

---------------------------------------------------------------

__________________
***Interactions***
__________________

- Additive relationships

      ex:
          plant_height ~ bacteria + sun
          
          . Change in heights in the sum of the effects of bacteria and sunlight
                ~ Change in sunlight causes same change in height, independent of bacteria
                ~ Change in bacteria causes same change in height, independent of sunlight
                  
- Interaction - the simultaneous influence of two variables on the outcome is not additive.

      ex: 
          plant_height ~ bacteria + sun + bacteria:sun
          
          . Change in height is more (or less) than the sum of the effects due to sun/bacteria.
          . At higher level of sunlight, 1 unit change in bacteria causes more change in height.
          
          . sun: categorical {"sun", "shade"}
          . In sun, 1 unit change in bacteria causes m units change in height
          . In shade, 1 unit change in bacteria causes n units change in height
          --> 2 separat models: on for sun, one for shade.

- To fit a linear or additive model with interactions in R 
            -> must specify the interaction explicity
            
      . Interaction - Colon ( : )
                                    y ~ a : b
      . Main effects and interaction - Asterisk ( * )
                                    y ~ a * b
                              # Both mean the same
                               y ~ a + b + a:b
      . Expressing the product of two variables - I
                                    y ~ I (a * b)
      --> Using the wrong pattern of main effects and interactions in a linear regression can 
            lead to a suboptimal model, as well as incorrect interpretations of how the inputs
            affect the outcome.
            
            . The smaller the RMSE (cross-validation) the better the performance
        
**Practices**

Modeling an interaction
In this exercise you will use interactions to model the effect of gender and gastric activity on alcohol metabolism.

The data frame alcohol has columns:

Metabol: the alcohol metabolism rate
Gastric: the rate of gastric alcohol dehydrogenase activity
Sex: the sex of the drinker (Male or Female)
In the video, we fit three models to the alcohol data:

one with only additive (main effect) terms : Metabol ~ Gastric + Sex
two models, each with interactions between gastric activity and sex
We saw that one of the models with interaction terms had a better R-squared than the additive model, suggesting that using interaction terms gives a better fit. In this exercise we will compare the R-squared of one of the interaction models to the main-effects-only model.

Recall that the operator : designates the interaction between two variables. The operator * designates the interaction between the two variables, plus the main effects.

x*y = x + y + x:y

The data frame alcohol is in your workspace.

    - Write a formula that expresses Metabol as a function of Gastric 
        and Sex with no interactions.
    - Assign the formula to the variable fmla_add and print it.
    - Write a formula that expresses Metabol as a function of the interaction 
        between Gastric and Sex.
          . Add Gastric as a main effect, but not Sex.
          . Assign the formula to the variable fmla_interaction and print it.
    - Fit a linear model with only main effects: model_add to the data.
    - Fit a linear model with the interaction: model_interaction to the data.
    - Call summary() on both models. Which has a better R-squared?

---------------------------------------------------------
                  
alcohol
> Subject Metabol Gastric    Sex       Alcohol
1        1     0.6     1.0 Female     Alcoholic
2        2     0.6     1.6 Female     Alcoholic
3        3     1.5     1.5 Female     Alcoholic
4        4     0.4     2.2 Female Non-alcoholic
5        5     0.1     1.1 Female Non-alcoholic
6        6     0.2     1.2 Female Non-alcoholic
7        7     0.3     0.9 Female Non-alcoholic
8        8     0.3     0.8 Female Non-alcoholic
9        9     0.4     1.5 Female Non-alcoholic
10      10     1.0     0.9 Female Non-alcoholic
11      11     1.1     1.6 Female Non-alcoholic
12      12     1.2     1.7 Female Non-alcoholic
13      13     1.3     1.7 Female Non-alcoholic
14      14     1.6     2.2 Female Non-alcoholic
15      15     1.8     0.8 Female Non-alcoholic
16      16     2.0     2.0 Female Non-alcoholic
17      17     2.5     3.0 Female Non-alcoholic
18      18     2.9     2.2 Female Non-alcoholic
19      19     1.5     1.3   Male     Alcoholic
20      20     1.9     1.2   Male     Alcoholic
21      21     2.7     1.4   Male     Alcoholic
22      22     3.0     1.3   Male     Alcoholic
23      23     3.7     2.7   Male     Alcoholic
24      24     0.3     1.1   Male Non-alcoholic
25      25     2.5     2.3   Male Non-alcoholic
26      26     2.7     2.7   Male Non-alcoholic
27      27     3.0     1.4   Male Non-alcoholic
28      28     4.0     2.2   Male Non-alcoholic
29      29     4.5     2.0   Male Non-alcoholic
30      30     6.1     2.8   Male Non-alcoholic
31      31     9.5     5.2   Male Non-alcoholic
32      32    12.3     4.1   Male Non-alcoholic                 
                  
# alcohol is in the workspace
summary(alcohol)
>  Subject         Metabol          Gastric          Sex    
 Min.   : 1.00   Min.   : 0.100   Min.   :0.800   Female:18  
 1st Qu.: 8.75   1st Qu.: 0.600   1st Qu.:1.200   Male  :14  
 Median :16.50   Median : 1.700   Median :1.600              
 Mean   :16.50   Mean   : 2.422   Mean   :1.863              
 3rd Qu.:24.25   3rd Qu.: 2.925   3rd Qu.:2.200              
 Max.   :32.00   Max.   :12.300   Max.   :5.200              
          Alcohol  
 Alcoholic    : 8  
 Non-alcoholic:24

# Create the formula with main effects only
(fmla_add <- Metabol ~ Gastric + Sex )
> Metabol ~ Gastric + Sex

# Create the formula with interactions
(fmla_interaction <- Metabol ~ Gastric + Gastric : Sex )
> Metabol ~ Gastric + Gastric:Sex

# Fit the main effects only model
(model_add <- lm(fmla_add, alcohol))
> Call:
lm(formula = fmla_add, data = alcohol)
Coefficients:
(Intercept)      Gastric      SexMale  
     -1.947        1.966        1.617
 
# Fit the interaction model
(model_interaction <- lm(fmla_interaction, alcohol))
> Call:
lm(formula = fmla_interaction, data = alcohol)
Coefficients:
    (Intercept)          Gastric  Gastric:SexMale  
        -0.7504           1.1489           1.0422

# Call summary on both models and compare
summary(model_add)
> Call:
lm(formula = fmla_add, data = alcohol)
Residuals:
    Min      1Q  Median      3Q     Max 
-2.2779 -0.6328 -0.0966  0.5783  4.5703 
Coefficients:
            Estimate Std. Error t value Pr(>|t|)    
(Intercept)  -1.9466     0.5198  -3.745 0.000796 ***
Gastric       1.9656     0.2674   7.352 4.24e-08 ***
SexMale       1.6174     0.5114   3.163 0.003649 ** 
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
Residual standard error: 1.331 on 29 degrees of freedom
Multiple R-squared:  0.7654,	Adjusted R-squared:  0.7492 
F-statistic: 47.31 on 2 and 29 DF,  p-value: 7.41e-10


summary(model_interaction)
> Call:
lm(formula = fmla_interaction, data = alcohol)
Residuals:
    Min      1Q  Median      3Q     Max 
-2.4656 -0.5091  0.0143  0.5660  4.0668 
Coefficients:
                Estimate Std. Error t value Pr(>|t|)    
(Intercept)      -0.7504     0.5310  -1.413 0.168236    
Gastric           1.1489     0.3450   3.331 0.002372 ** 
Gastric:SexMale   1.0422     0.2412   4.321 0.000166 ***
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
Residual standard error: 1.204 on 29 degrees of freedom
Multiple R-squared:  0.8081,	Adjusted R-squared:  0.7948 
F-statistic: 61.05 on 2 and 29 DF,  p-value: 4.033e-11                  
                  
-----------------------------------------------------------------

Modeling an interaction (2)
In this exercise, you will compare the performance of the interaction model you fit in the previous exercise to the performance of a main-effects only model. Because this data set is small, we will use cross-validation to simulate making predictions on out-of-sample data.

You will begin to use the dplyr package to do calculations.

mutate() adds new columns to a tbl (a type of data frame)
group_by() specifies how rows are grouped in a tbl
summarize() computes summary statistics of a column
You will also use tidyr's gather() which takes multiple columns and collapses them into key-value pairs.

The data frame alcohol and the formulas fmla_add and fmla_interaction are in the workspace.

    - Use kWayCrossValidation() to create a splitting plan for a 3-fold cross validation.
          . The first argument is the number of rows to be split.
          . The second argument is the number of folds for the cross-validation.
          . You can set the 3rd and 4th arguments of the function to NULL.
    - Examine and run the sample code to get the 3-fold cross-validation predictions of 
        a model with no interactions and assign them to the column pred_add.
    - Get the 3-fold cross-validation predictions of the model with interactions. 
        Assign the predictions to the column pred_interaction.
          . The sample code shows you the procedure.
          . Use the same splitPlan that you already created.
    - Fill in the blanks to
          . gather the predictions into a single column pred.
          . add a column of residuals (actual outcome - predicted outcome).
          . get the RMSE of the cross-validation predictions for each model type.
    - Compare the RMSEs. Based on these results, which model should you use?

-------------------------------------------------------------------
                  
# alcohol is in the workspace
summary(alcohol)
>  Subject         Metabol          Gastric          Sex    
 Min.   : 1.00   Min.   : 0.100   Min.   :0.800   Female:18  
 1st Qu.: 8.75   1st Qu.: 0.600   1st Qu.:1.200   Male  :14  
 Median :16.50   Median : 1.700   Median :1.600              
 Mean   :16.50   Mean   : 2.422   Mean   :1.863              
 3rd Qu.:24.25   3rd Qu.: 2.925   3rd Qu.:2.200              
 Max.   :32.00   Max.   :12.300   Max.   :5.200              
          Alcohol  
 Alcoholic    : 8  
 Non-alcoholic:24
 
# Both the formulae are in the workspace
fmla_add
> Metabol ~ Gastric + Sex

fmla_interaction
> Metabol ~ Gastric + Gastric:Sex
 
# Create the splitting plan for 3-fold cross validation
set.seed(34245)  # set the seed for reproducibility
splitPlan <- kWayCrossValidation(nrow(alcohol), 3, NULL, NULL)
 
# Sample code: Get cross-val predictions for main-effects only model
alcohol$pred_add <- 0  # initialize the prediction vector

for(i in 1:3) {
    split <- splitPlan[[i]]
    model_add <- lm(fmla_add, data = alcohol[split$train, ])
    alcohol$pred_add[split$app] <- predict(model_add, newdata = alcohol[split$app, ])
  }
 
# Get the cross-val predictions for the model with interactions
alcohol$pred_interaction <- 0 # initialize the prediction vector

for(i in 1:3) {
    split <- splitPlan[[i]]
    model_interaction <- lm(fmla_interaction, data = alcohol[split$train, ])
    alcohol$pred_interaction[split$app] <- predict(model_interaction, newdata = alcohol[split$app, ])
  }
  
# Get RMSE
alcohol %>% 
    gather(key = modeltype, value = pred, pred_add, pred_interaction) %>%
    mutate(residuals = Metabol - pred) %>%
    group_by(modeltype) %>%
    summarize(rmse = sqrt(mean(residuals^2)))

> # A tibble: 2 x 2
  modeltype         rmse
  <chr>            <dbl>
1 pred_add          1.64
2 pred_interaction  1.38                
                  
--------------------------------------------------------------------------

_______________________________________________
***Transforming the response before modeling***
_______________________________________________

- Sometimes you get better models by transforming the output, rather than predicting 
    the output directly
    
- The Log Transform for monetary data (income/profits)
      . Monetary values are often lognormally distributed, which means that they tend to be
          skewed, with a long tail.
      . Lognormal distributions also typically have a wide dynamic range.
      . Wide dynamic ranges can also make prediction more difficult.
      . mean > median
      . predicting the mean wil overpredict typical values.
      . Regression algorithms usually predict the expected, or mean, value of the output.
              --> means that predicting income directly will tend to overpredict the typical income 
                    for subjects with a given set of characteristics
                  
- Normal Distribution
      . mean = median
      . more reasonable dynamic range
- Log of lognormally distributed data = normally distributed data
      -> mean tracks median.
      -> the dynamic range of the data is more modest.

- The Procedure
    
      1. Fit a model to log outcome.
            
                model <- lm(log(y) ~ x, data = train)
      
      2. Apply the data you want predictions for.
                
                logpred <- predict(model, data = test)
      
      3. Exponentiate to transform the predictions back to outcome space.
                pred <- exp(logpred)
                
- One consequence of log-transforming outcomes is that prediction errors are multiplicative in
    outcome space
        --> meaning the size of the error is relative to the size of the outcome.
        . We can define relative error as the prediction error divided by the true outcome.
        . Log transformations are useful when you want to reduce relative error, 
            rather than additive error.
            
- Root Mean Squared Relative Error

        RMS-relative error = square-root[((pred - y) / y)square]
        
    . Predicting log-outcome reduces RMS-relative error
    . But the model will often have larger RMSE

- A model that predicts log-outcome will often have lower RMS-relative error and larger RMSE than
    a model that predicts outcome directly

- We can evaluate the model's error and relative error on new data.  
-> modeling the income directly gave a smaller RMSE but modelling log income gave smaller relative error.

**Practices**

Relative error
In this exercise, you will compare relative error to absolute error. For the purposes of modeling, we will define relative error as

rel=(y−pred)y
that is, the error is relative to the true outcome. You will measure the overall relative error of a model using root mean squared relative error:

rmserel=(√rel2¯¯¯¯¯¯¯¯)
where rel2¯¯¯¯¯¯¯¯ is the mean of rel2.

The example (toy) dataset fdata is loaded in your workspace. It includes the columns:

y: the true output to be predicted by some model; imagine it is the amount of money a customer will spend on a visit to your store.
pred: the predictions of a model that predicts y.
label: categorical: whether y comes from a population that makes small purchases, or large ones.
You want to know which model does "better": the one predicting the small purchases, or the one predicting large ones.

The data frame fdata is in the workspace.

    - Fill in the blanks to examine the data. Notice that large purchases tend to be about 
        100 times larger than small ones.
    - Fill in the blanks to create error columns:
          . Define residual as y - pred.
          . Define relative error as residual / y.
    - Fill in the blanks to calculate and compare RMSE and relative RMSE.
          . How do the absolute errors compare? The relative errors?
    - Examine the plot of predictions versus outcome.
          . In your opinion, which model does "better"?

---------------------------------------------------------------------

> fdata
               y        pred           label
1      9.1496941    6.430583 small purchases
2      1.9025206    3.473332 small purchases
3     -3.8598260    1.594509 small purchases
4      2.3889703    3.764175 small purchases
5      1.5415715    9.509294 small purchases
6     13.5618785    6.931725 small purchases
7     10.1987859    8.191798 small purchases
8      1.1044627    1.514578 small purchases
9      3.9354788    8.986364 small purchases
10     9.0407978    6.149792 small purchases
11     1.7276491    8.498107 small purchases
12    15.7238410   10.941225 small purchases
13     2.2578444    6.003761 small purchases
14    -1.9750566    1.071922 small purchases
15     1.1006461    4.420715 small purchases
16    18.6282945   10.518787 small purchases
17     3.6786547    5.746765 small purchases
18     3.0863171    7.092900 small purchases
19     8.6913910    7.837105 small purchases
20     7.9086741    4.307022 small purchases
21     5.4401589    6.001545 small purchases
22    14.7850358    8.309529 small purchases
23     9.0220444    8.625899 small purchases
24     3.9766558    2.978443 small purchases
25     2.6675677    4.040108 small purchases
26     7.6815442    7.460930 small purchases
27    11.9348659    9.082721 small purchases
28     5.3087341    6.523833 small purchases
29    13.0587206   10.342933 small purchases
30     2.2285365    4.240767 small purchases
31    15.4031317    8.833006 small purchases
32    -0.8775657    1.165411 small purchases
33     7.6111939    4.743669 small purchases
34     9.8625104    8.426791 small purchases
35     4.3625009   10.191528 small purchases
36     3.8418327    4.325049 small purchases
37    11.3425371    6.389882 small purchases
38    17.1292595   10.997543 small purchases
39    16.1650339    7.524615 small purchases
40    -5.8934988    2.448673 small purchases
41    12.6387974    9.514229 small purchases
42     6.4500322    5.211945 small purchases
43     2.9682093    3.973336 small purchases
44     4.0760785    6.321008 small purchases
45     5.5235922   10.166002 small purchases
46     4.8320823    3.847532 small purchases
47     6.7230129    6.304504 small purchases
48     1.8421342    3.647446 small purchases
49     3.1992212    2.518993 small purchases
50    10.8161739    8.023910 small purchases
51  1026.4000266 1027.192723 large purchases
52   202.3892007  194.521504 large purchases
53   833.3530129  826.245477 large purchases
54  1075.4121192 1081.438809 large purchases
55    96.1198136  100.388039 large purchases
56   438.2357889  430.291572 large purchases
57   911.3332595  912.567621 large purchases
58   542.5643273  533.946165 large purchases
59   686.3323692  691.793193 large purchases
60   494.4655725  498.008558 large purchases
61   422.8093644  423.202790 large purchases
62  1033.8816216 1032.662338 large purchases
63   161.9940611  168.545765 large purchases
64   491.4299932  492.239389 large purchases
65   575.9199260  589.705853 large purchases
66   384.7702130  377.148966 large purchases
67   720.0291839  730.110724 large purchases
68   963.9350562  967.650520 large purchases
69   159.7865024  159.207010 large purchases
70   765.4002297  767.185617 large purchases
71   246.4217424  250.132558 large purchases
72  1097.9219860 1098.895762 large purchases
73  1050.4012681 1048.867067 large purchases
74  1069.6224565 1057.992036 large purchases
75   116.8100415  119.282157 large purchases
76   523.5162673  524.564769 large purchases
77   457.7430632  459.769560 large purchases
78  1060.5638650 1053.648585 large purchases
79   761.9174356  751.113173 large purchases
80   969.3237051  966.379570 large purchases
81   522.6803675  520.614066 large purchases
82   475.8733078  467.724627 large purchases
83   368.5448093  364.211669 large purchases
84  1101.6186402 1097.071859 large purchases
85  1052.9226814 1054.626093 large purchases
86   663.0351321  664.365996 large purchases
87   136.6886558  137.485828 large purchases
88   331.8120748  326.087523 large purchases
89   921.9615947  929.965382 large purchases
90   773.5812495  772.131253 large purchases
91   458.3325781  456.426629 large purchases
92   643.4683247  646.674567 large purchases
93   738.6477366  747.002823 large purchases
94   846.4213303  842.529168 large purchases
95   413.6645906  411.887935 large purchases
96   180.9942090  175.770888 large purchases
97   695.6228990  687.864916 large purchases
98   164.8980048  165.811990 large purchases
99   106.5062642  108.484750 large purchases
100  358.3557044  363.866527 large purchases
          
# fdata is in the workspace
summary(fdata)
>     y                 pred                      label   
 Min.   :  -5.894   Min.   :   1.072   small purchases:50  
 1st Qu.:   5.407   1st Qu.:   6.373   large purchases:50  
 Median :  57.374   Median :  55.693                       
 Mean   : 306.204   Mean   : 305.905                       
 3rd Qu.: 550.903   3rd Qu.: 547.886                       
 Max.   :1101.619   Max.   :1098.896
 
# Examine the data: generate the summaries for the groups large and small:
> fdata %>% 
      group_by(label) %>%        # group by small/large purchases
      summarize(min  = min(y),   # min of y
                mean = mean(y),  # mean of y
                max  = max(y))   # max of y
# A tibble: 2 x 4
  label             min   mean    max
  <fct>           <dbl>  <dbl>  <dbl>
1 small purchases -5.89   6.48   18.6
2 large purchases 96.1  606.   1102.
 
# Fill in the blanks to add error columns
> fdata2 <- fdata %>% 
           group_by(label) %>%               # group by label
             mutate(residual = y - pred,     # Residual
                    relerr   = residual/y)   # Relative error

# Compare the rmse and rmse.rel of the large and small groups:
> fdata2 %>% 
    group_by(label) %>% 
    summarize(rmse     = sqrt(mean(residual^2)),  # RMSE
              rmse.rel = sqrt(mean(relerr^2)))    # Root mean squared relative error
# A tibble: 2 x 3
  label            rmse rmse.rel
  <fct>           <dbl>    <dbl>
1 small purchases  4.01   1.25  
2 large purchases  5.54   0.0147
 
# Plot the predictions for both groups of purchases
> ggplot(fdata2, aes(x = pred, y = y, color = label)) + 
    geom_point() + 
    geom_abline() + 
    facet_wrap(~ label, ncol = 1, scales = "free") + 
    ggtitle("Outcome vs prediction")         

-------------------------------------------------------------------------------------

Modeling log-transformed monetary output
In this exercise, you will practice modeling on log-transformed monetary output, and then transforming the "log-money" predictions back into monetary units. The data loaded into your workspace records subjects' incomes in 2005 (Income2005), as well as the results of several aptitude tests taken by the subjects in 1981:

Arith
Word
Parag
Math
AFQT (Percentile on the Armed Forces Qualifying Test)
The data have already been split into training and test sets (income_train and income_test respectively) and are in the workspace. You will build a model of log(income) from the inputs, and then convert log(income) back into 

    - Call summary() on income_train$Income2005 to see the summary statistics of income 
        in the training set.
    - Write a formula to express log(Income2005) as a function of the five tests 
        as the variable fmla.log. Print it.
    - Fit a linear model of log(Income2005) to the income_train data: model.log.
    - Use model.log to predict income on the income_test dataset. Put it in the column logpred.
            . Check summary() of logpred to see that the magnitudes are much different from 
                those of Income2005.
    - Reverse the log transformation to put the predictions into "monetary units":
                        exp(income_test$logpred).
            . Check summary() of pred.income and see that the magnitudes are now 
                similar to Income2005 magnitudes.
    - Fill in the blanks to plot a scatter plot of predicted income vs income on the test set.

---------------------------------------------------------------------------------------

head(income_train)
> Subject Arith Word Parag Math   AFQT Income2005
1       2     8   15     6    6  6.841       5500
2       6    30   35    15   23 99.393      65000
4       8    13   35    12    4 44.022      36000
5       9    21   28    10   13 59.683      65000
7      16    17   30    12   17 50.283      71000
8      17    29   33    13   21 89.669      43000

# Examine Income2005 in the training set
summary(income_train$Income2005)
>   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. 
     63   23000   39000   49894   61500  703637
 
# Write the formula for log income as a function of the tests and print it
(fmla.log <- log(Income2005) ~ Arith + Word + Parag + Math + AFQT)
> log(Income2005) ~ Arith + Word + Parag + Math + AFQT
 
# Fit the linear model
model.log <-  lm(fmla.log, income_train)
 
# Make predictions on income_test
income_test$logpred <- predict(model.log, income_test)

summary(income_test$logpred)
>  Min. 1st Qu.  Median    Mean 3rd Qu.    Max. 
  9.766  10.133  10.423  10.419  10.705  11.006

# Convert the predictions to monetary units
income_test$pred.income <- exp(income_test$logpred)

summary(income_test$pred.income)
>  Min. 1st Qu.  Median    Mean 3rd Qu.    Max. 
  17432   25167   33615   35363   44566   60217

#  Plot predicted income (x axis) vs income
ggplot(income_test, aes(x = pred.income, y = Income2005)) + 
    geom_point() + 
    geom_abline(color = "blue")
          
--------------------------------------------------------------------------

Comparing RMSE and root-mean-squared Relative Error
In this exercise, you will show that log-transforming a monetary output before modeling improves mean relative error (but increases RMSE) compared to modeling the monetary output directly. You will compare the results of model.log from the previous exercise to a model (model.abs) that directly fits income.

The income_train and income_test datasets are loaded in your workspace, along with your model, model.log.

Also in the workspace:

model.abs: a model that directly fits income to the inputs using the formula

Income2005 ~ Arith + Word + Parag + Math + AFQT

    - Fill in the blanks to add predictions from the models to income_test.
          . Don’t forget to take the exponent of the predictions from model.log 
              to undo the log transform!
    - Fill in the blanks to gather() the predictions and calculate the residuals and relative error.
    - Fill in the blanks to calculate the RMSE and relative RMSE for predictions.
          . Which model has larger absolute error? Larger relative error?
          
--------------------------------------------------------------------------------

# fmla.abs is in the workspace
fmla.abs
> Income2005 ~ Arith + Word + Parag + Math + AFQT

# model.abs is in the workspace
summary(model.abs)
> Call:
lm(formula = fmla.abs, data = income_train)
Residuals:
   Min     1Q Median     3Q    Max 
-78728 -24137  -6979  11964 648573 
Coefficients:
            Estimate Std. Error t value Pr(>|t|)    
(Intercept)  17516.7     6420.1   2.728  0.00642 ** 
Arith         1552.3      303.4   5.116 3.41e-07 ***
Word          -132.3      265.0  -0.499  0.61754    
Parag        -1155.1      618.3  -1.868  0.06189 .  
Math           725.5      372.0   1.950  0.05127 .  
AFQT           177.8      144.1   1.234  0.21734    
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
Residual standard error: 45500 on 2063 degrees of freedom
Multiple R-squared:  0.1165,	Adjusted R-squared:  0.1144 
F-statistic:  54.4 on 5 and 2063 DF,  p-value: < 2.2e-16
 
# Add predictions to the test set
income_test <- income_test %>%
    mutate(pred.absmodel = predict(model.abs, income_test),      # predictions from model.abs
           pred.logmodel = exp(predict(model.log, income_test))) # predictions from model.log
 
# Gather the predictions and calculate residuals and relative error
income_long <- income_test %>% 
    gather(key = modeltype, value = pred, pred.absmodel, pred.logmodel) %>%
    mutate(residual = pred - Income2005,     # residuals
           relerr   = residual / Income2005) # relative error
 
# Calculate RMSE and relative RMSE and compare
income_long %>% 
    group_by(modeltype) %>%                       # group by modeltype
    summarize(rmse     = sqrt(mean(residual^2)),  # RMSE
              rmse.rel = sqrt(mean(relerr^2)))    # Root mean squared relative error

> # A tibble: 2 x 3
  modeltype       rmse rmse.rel
  <chr>          <dbl>    <dbl>
1 pred.absmodel 37448.     3.18
2 pred.logmodel 39235.     2.22
          
-----------------------------------------------------------------------------------------


_________________________________________
***Transforming inputs before modeling***
_________________________________________

- There are many reasons why you might want to transform input variables before modeling. 
      . Having domain knowledge that tells you a transformed variable may be more informative
          than the variables that you have.
      . Pragmatic reasons
          ~ Log transform to reduce dynamic range
          ~ Logtransform because meaningful changes in variable are multiplicative
          ~ to meet modeling assumptions, like linearity
                  y approximately linear in f(x) rather than in x

- Since there're differnet possible fits, if we have a domain knowledge for prefering one, then
    that's how we should pick. But if we don't know, and are mostly concerned with accurate prediction,
    then we should pick the one that seems to give us the lowest prediction error
          . Compare different models (linear, quadratic and cubic models), the one that has 
              the best r-squared is the largest.
          . Use cross-validation to evaluate the models, the one that has the best RMSE is the smallest
          
**Practices**

Input transforms: the "hockey stick"
In this exercise, we will build a model to predict price from a measure of the house's size (surface area). The data set houseprice has the columns:

price : house price in units of $1000
size: surface area

A scatterplot of the data shows that the data is quite non-linear: a sort of "hockey-stick" where price is fairly flat for smaller houses, but rises steeply as the house gets larger. Quadratics and tritics are often good functional forms to express hockey-stick like relationships. Note that there may not be a "physical" reason that price is related to the square of the size; a quadratic is simply a closed form approximation of the observed relationship.

scatterplot

You will fit a model to predict price as a function of the squared size, and look at its fit on the training data.

Because ^ is also a symbol to express interactions, use the function I() to treat the expression x^2 “as is”: that is, as the square of x rather than the interaction of x with itself.

exampleFormula = y ~ I(x^2)

The data set houseprice is in the workspace.

    - Write a formula, fmla_sqr, to express price as a function of squared size. Print it.
    - Fit a model model_sqr to the data using fmla_sqr
    - For comparison, fit a linear model model_lin to the data using the formula price ~ size.
    - Fill in the blanks to
          . make predictions from the training data from the two models
          . gather the predictions into a single column pred
          . graphically compare the predictions of the two models to the data. Which fits better?
          
--------------------------------------------------------------------------------------

 houseprice
> size price
1    72   156
2    98   153
3    92   230
4    90   152
5    44    42
6    46   157
7    90   113
8   150   573
9    94   202
10   90   261
11   90   175
12   66   212
13  142   486
14   74   109
15   86   220
16   46   186
17   54   133
18  130   360
19  122   283
20  118   380
21  100   185
22   74   186
23  146   459
24   92   167
25  100   171
26  140   547
27   94   170
28   90   286
29  120   293
30   70   109
31  100   205
32  132   514
33   58   175
34   92   249
35   76   234
36   90   242
37   66   177
38  134   399
39  140   511
40   64   107
          
# houseprice is in the workspace
summary(houseprice)
>    size           price      
 Min.   : 44.0   Min.   : 42.0  
 1st Qu.: 73.5   1st Qu.:164.5  
 Median : 91.0   Median :203.5  
 Mean   : 94.3   Mean   :249.2  
 3rd Qu.:118.5   3rd Qu.:287.8  
 Max.   :150.0   Max.   :573.0
 
# Create the formula for price as a function of squared size
(fmla_sqr <- price ~ I(size^2))
> price ~ I(size^2)

# Fit a model of price as a function of squared size (use fmla_sqr)
model_sqr <- lm(fmla_sqr, houseprice)
> Call:
lm(formula = fmla_sqr, data = houseprice)
Coefficients:
(Intercept)    I(size^2)  
   45.03408      0.02097

# Fit a model of price as a linear function of size
model_lin <- lm(price ~ size, houseprice)
> Call:
lm(formula = price ~ size, data = houseprice)
Coefficients:
(Intercept)         size  
   -124.723        3.966 

# Make predictions and compare
houseprice %>% 
      mutate(pred_lin = predict(model_lin),       # predictions from linear model
             pred_sqr = predict(model_sqr)) %>%   # predictions from quadratic model 
      gather(key = modeltype, value = pred, pred_lin, pred_sqr) %>% # gather the predictions
      ggplot(aes(x = size)) + 
         geom_point(aes(y = price)) +                  # actual prices
         geom_line(aes(y = pred, color = modeltype)) + # the predictions
         scale_color_brewer(palette = "Dark2")         
          
----------------------------------------------------------------------------------------

Input transforms: the "hockey stick" (2)
In the last exercise you saw that a quadratic model seems to fit the houseprice data better than a linear model. In this exercise you will confirm whether the quadratic model would perform better on out-of-sample data. Since this data set is small, you will use cross-validation. The quadratic formula fmla_sqr that you created in the last exercise is in your workspace.

For comparison, the sample code will calculate cross-validation predictions from a linear model price ~ size.

The data frame houseprice and the formula fmla_sqr from the last exercise are in the workspace.

    - Use kWayCrossValidation() to create a splitting plan for a 3-fold cross validation.
        . You can set the 3rd and 4th arguments of the function to NULL.
    - Examine and run the sample code to get the 3-fold cross-validation predictions of 
        the model price ~ size and add them to the column pred_lin.
    - Get the cross-validation predictions for price as a function of squared size. 
        Assign them to the column pred_sqr.
            . The sample code gives you the procedure.
            . You can use the splitting plan you already created.
    - Fill in the blanks to gather the predictions and calculate the residuals.
    - Fill in the blanks to compare the RMSE for the two models. Which one fits better?
    
--------------------------------------------------------------------------------------------

# houseprice is in the workspace
summary(houseprice)
>    size           price      
 Min.   : 44.0   Min.   : 42.0  
 1st Qu.: 73.5   1st Qu.:164.5  
 Median : 91.0   Median :203.5  
 Mean   : 94.3   Mean   :249.2  
 3rd Qu.:118.5   3rd Qu.:287.8  
 Max.   :150.0   Max.   :573.0
 
# fmla_sqr is in the workspace
fmla_sqr
> price ~ I(size^2)

# Create a splitting plan for 3-fold cross validation
set.seed(34245)  # set the seed for reproducibility
splitPlan <- kWayCrossValidation(nrow(houseprice), 3, NULL, NULL)
> splitPlan
[[1]]
[[1]]$train
 [1]  1  2  3  4  7  9 10 11 15 16 18 19 21 22 23 24 25 26 27 28 30 32 33 36 37
[26] 38 40

> [[1]]$app
 [1] 34 17  6 13 39  5 29 12  8 14 35 31 20

> [[2]]
[[2]]$train
 [1]  4  5  6  8  9 10 11 12 13 14 16 17 20 21 22 24 26 29 30 31 32 34 35 36 37
[26] 39

> [[2]]$app
 [1] 38  1 28 25  2 15 33 23 27  3 19  7 18 40

> [[3]]
[[3]]$train
 [1]  1  2  3  5  6  7  8 12 13 14 15 17 18 19 20 23 25 27 28 29 31 33 34 35 38
[26] 39 40

> [[3]]$app
 [1]  9 11 37 10 26 16 24  4 32 30 36 22 21

> attr(,"splitmethod")
[1] "kwaycross"

# Sample code: get cross-val predictions for price ~ size
houseprice$pred_lin <- 0  # initialize the prediction vector

for(i in 1:3) {
    split <- splitPlan[[i]]
    model_lin <- lm(price ~ size, data = houseprice[split$train, ])
    houseprice$pred_lin[split$app] <- predict(model_lin, newdata = houseprice[split$app, ])
  }
 
# Get cross-val predictions for price as a function of size^2 (use fmla_sqr)
houseprice$pred_sqr <- 0 # initialize the prediction vector

for(i in 1:3) {
    split <- splitPlan[[i]]
    model_sqr <- lm(fmla_sqr, data = houseprice[split$train, ])
    houseprice$pred_sqr[split$app] <- predict(model_sqr, newdata = houseprice[split$app, ])
  }
 
# Gather the predictions and calculate the residuals
houseprice_long <- houseprice %>%
    gather(key = modeltype, value = pred, pred_lin, pred_sqr) %>%
    mutate(residuals = pred - price)
 
# Compare the cross-validated RMSE for the two models
houseprice_long %>% 
    group_by(modeltype) %>%
    summarize(rmse = sqrt(mean(residuals^2)))

># A tibble: 2 x 2
  modeltype  rmse
  <chr>     <dbl>
1 pred_lin   74.3
2 pred_sqr   63.7

-------------------------------------------------------------------------------------

***
### PART 4: DEALING WITH NON-LINEAR RESPONSES ###
***

__________________________________________________
***Logistic regression to predict probabilities***
__________________________________________________

Predicting Probabilites
  - Predicting whether an even occurs (yes/no): classification
  - Predicting the probability that an even occur: regression
  - Linear regression: predicts values in [-infinity, +infinity]
  - Probabilities: limited to [0, 1] interval
      . So we'll call it non-linear

We cannot use linear model here because it can predict invalid probability which lies
    outside of the valid range.

Logistic Regression

            model <- glm(formula, data, family = binomial)
            
                . Generalized linear model
                . Assumes inputs additive, linear in log-odds: log(p/(1-p))
                . family: describes error distribution of the model
                    - logistic regression: family = binomial
                    
glm() also assumes that there are two possible outcomes, a and b. The model returns the
  probability of event b.  To make the model easier to understand, we recommend that you
  encode the two outcome as 0/1 or FALSE and TRUE
      . If the coefficient is positive, then the even becomes more probable as that
          value increases, if everything else is held constant

Predict with glm() model

              predict(model, newdata, type = "response")
              
              . newdata: by default, training data
              . To get probabilities: use type = "response"
                  - By default: returns log-odds

Evaluating a logistic regression model: pseudo-R2
  - Squared error and RMSE are not good measure for logistic regression models. 
      Instead, use deviance and pseudo R-squared
              
              R2 = 1 - (RSS / SStotal)
              
        pseudoR2 = 1 - (deviance / null.deviance)
        
              . Deviance: analogous to variance (RSS)
              . Null deviance: similiar to SStot
              . pseudo R^2: Deviance explained - analogous to R-squared.
                            It compares the deviance of a model to the null deviance of data
            -> A good fit gives psedo-R-squared near 1

For training performance, we can calculate pseudo-R-squared using the deviance and
  null deviance from glance(), or call wrapChiSqTest() on the model
  
                broom::glance()
                glance(model) %>%
                        summarize(pR2 = 1 - deviance/null.deviance)
                
                <OR>
                
                sigr::wrapChiSqTest()
                wrapChiSqTest(model)
                        
For test performance, call wrapChiSqTest on a data frame that has both the predictions 
    and the true outcomes.
                
         test %>%
                mutate(pred = predict(model, newdata = test, type = "response")) %>%
                wrapChiSqTest("pred", "has_dmd", TRUE)
                
                . dataframe
                . prediction column name
                . outcome column name
                . target value (target event)
                
The Gain Curve Plot is great for evaluating logistic regression models.
          
      GainCurvePlot(test, "pred", "has_dmd", "DMD model on test")
      
      . dataframe
      . prediction column name
      . outcome column name
      . title of the plot
      -> The wizard curve (in green) shows how a perfect model would sort the events.
      -> The blue curve shows how the model's probability scores would sort the events.
      -> The closer the blue curve is to the green one, the better the model's
            probability estimates are for identifying positive instances.
            
**Practices**

Fit a model of sparrow survival probability
In this exercise, you will estimate the probability that a sparrow survives a severe winter storm, based on physical characteristics of the sparrow. The dataset sparrow is loaded into your workspace. The outcome to be predicted is status ("Survived", "Perished"). The variables we will consider are:

    - total_length: length of the bird from tip of beak to tip of tail (mm)
    - weight: in grams
    - humerus : length of humerus 
                ("upper arm bone" that connects the wing to the body) (inches)
    - Remember that when using glm() to create a logistic regression model, 
              you must explicitly specify that family = binomial:

                  glm(formula, data = data, family = binomial)
                  
You will call summary(), broom::glance() to see different functions for examining a logistic regression model. One of the diagnostics that you will look at is the analog to R2, called pseudo-R2.

                    pseudoR2=1−deviancenull.deviance
                    
You can think of deviance as analogous to variance: it is a measure of the variation in categorical data. The pseudo-R2 is analogous to R2 for standard regression: R2 is a measure of the "variance explained" of a regression model. The pseudo-R2 is a measure of the "deviance explained".

The data frame sparrow and the package broom are loaded in the workspace.

    - As suggested in the video, you will predict on the outcomes TRUE and FALSE. 
        Create a new column survived in the sparrow data frame that is TRUE 
        when status == "Survived".
    - Create the formula fmla that expresses survived as a function of the variables 
        of interest. Print it.
    - Fit a logistic regression model to predict the probability of sparrow survival. 
        Assign the model to the variable sparrow_model.
    - Call summary() to see the coefficients of the model, the deviance and 
        the null deviance.
    - Call glance() on the model to see the deviances and other diagnostics in 
        a data frame. Assign the output from glance() to the variable perf.
    - Calculate the pseudo-R2.

---------------------------------------------------------------------

head(sparrow, 5)
>  status   age total_length wingspan weight beak_head humerus femur legbone
1 Survived adult          154      241   24.5      31.2    0.69  0.67    1.02
2 Survived adult          160      252   26.9      30.8    0.74  0.71    1.18
3 Survived adult          155      243   26.9      30.6    0.73  0.70    1.15
4 Survived adult          154      245   24.3      31.7    0.74  0.69    1.15
5 Survived adult          156      247   24.1      31.5    0.71  0.71    1.13
  skull sternum
1  0.59    0.83
2  0.60    0.84
3  0.60    0.85
4  0.58    0.84
5  0.57    0.82

# sparrow is in the workspace
summary(sparrow)
>    status       age             total_length      wingspan    
 Perished:36   Length:87          Min.   :153.0   Min.   :236.0  
 Survived:51   Class :character   1st Qu.:158.0   1st Qu.:245.0  
               Mode  :character   Median :160.0   Median :247.0  
                                  Mean   :160.4   Mean   :247.5  
                                  3rd Qu.:162.5   3rd Qu.:251.0  
                                  Max.   :167.0   Max.   :256.0  
     weight       beak_head        humerus           femur       
 Min.   :23.2   Min.   :29.80   Min.   :0.6600   Min.   :0.6500  
 1st Qu.:24.7   1st Qu.:31.40   1st Qu.:0.7250   1st Qu.:0.7000  
 Median :25.8   Median :31.70   Median :0.7400   Median :0.7100  
 Mean   :25.8   Mean   :31.64   Mean   :0.7353   Mean   :0.7134  
 3rd Qu.:26.7   3rd Qu.:32.10   3rd Qu.:0.7500   3rd Qu.:0.7300  
 Max.   :31.0   Max.   :33.00   Max.   :0.7800   Max.   :0.7600  
    legbone          skull           sternum      
 Min.   :1.010   Min.   :0.5600   Min.   :0.7700  
 1st Qu.:1.110   1st Qu.:0.5900   1st Qu.:0.8300  
 Median :1.130   Median :0.6000   Median :0.8500  
 Mean   :1.131   Mean   :0.6032   Mean   :0.8511  
 3rd Qu.:1.160   3rd Qu.:0.6100   3rd Qu.:0.8800  
 Max.   :1.230   Max.   :0.6400   Max.   :0.9300

# Create the survived column
sparrow$survived <- sparrow$status == "Survived"
sparrow$survived
>[1]  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE
[13]  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE
[25]  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE FALSE
[37] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE
[49] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE  TRUE
[61]  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE
[73]  TRUE  TRUE  TRUE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE
[85] FALSE FALSE FALSE

# Create the formula
(fmla <- survived ~ total_length + weight + humerus)
> survived ~ total_length + weight + humerus

# Fit the logistic regression model
sparrow_model <- glm(fmla, data = sparrow, family = binomial)
parrow_model
> Call:  glm(formula = fmla, family = binomial, data = sparrow)
Coefficients:
 (Intercept)  total_length        weight       humerus  
     46.8813       -0.5435       -0.5689       75.4610  
Degrees of Freedom: 86 Total (i.e. Null);  83 Residual
Null Deviance:	    118 
Residual Deviance: 75.09 	AIC: 83.09

# Call summary
summary(sparrow_model)
> Call:
glm(formula = fmla, family = binomial, data = sparrow)
Deviance Residuals: 
    Min       1Q   Median       3Q      Max  
-2.1117  -0.6026   0.2871   0.6577   1.7082  
Coefficients:
             Estimate Std. Error z value Pr(>|z|)    
(Intercept)   46.8813    16.9631   2.764 0.005715 ** 
total_length  -0.5435     0.1409  -3.858 0.000115 ***
weight        -0.5689     0.2771  -2.053 0.040060 *  
humerus       75.4610    19.1586   3.939 8.19e-05 ***
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
(Dispersion parameter for binomial family taken to be 1)
    Null deviance: 118.008  on 86  degrees of freedom
Residual deviance:  75.094  on 83  degrees of freedom
AIC: 83.094
Number of Fisher Scoring iterations: 5

# Call glance
(perf <- glance(sparrow_model))
> null.deviance df.null    logLik      AIC      BIC deviance df.residual
1      118.0084      86 -37.54718 83.09436 92.95799 75.09436          83

# Calculate pseudo-R-squared
(pseudoR2 <- 1 - perf$deviance/perf$null.deviance)
> [1] 0.3636526

--------------------------------------------------------------------------

Predict sparrow survival
In this exercise you will predict the probability of survival using the sparrow survival model from the previous exercise.

Recall that when calling predict() to get the predicted probabilities from a glm() model, you must specify that you want the response:

predict(model, type = "response")
Otherwise, predict() on a logistic regression model returns the predicted log-odds of the event, not the probability.

You will also use the GainCurvePlot() function to plot the gain curve from the model predictions. If the model's gain curve is close to the ideal ("wizard") gain curve, then the model sorted the sparrows well: that is, the model predicted that sparrows that actually survived would have a higher probability of survival. The inputs to the GainCurvePlot() function are:

frame: data frame with prediction column and ground truth column
xvar: the name of the column of predictions (as a string)
truthVar: the name of the column with actual outcome (as a string)
title: a title for the plot (as a string)
GainCurvePlot(frame, xvar, truthVar, title)

The dataframe sparrow and the model sparrow_model are in the workspace.

    - Create a new column in sparrow called pred that contains the predictions 
        on the training data.
    - Call GainCurvePlot() to create the gain curve of predictions. Does the model 
        do a good job of sorting the sparrows by whether or not they actually survived?
        
---------------------------------------------------------------------------------

# sparrow_model is in the workspace
summary(sparrow_model)

>Call:
glm(formula = fmla, family = binomial, data = sparrow)

>Deviance Residuals: 
    Min       1Q   Median       3Q      Max  
-2.1117  -0.6026   0.2871   0.6577   1.7082  

>Coefficients:
             Estimate Std. Error z value Pr(>|z|)    
(Intercept)   46.8813    16.9631   2.764 0.005715 ** 
total_length  -0.5435     0.1409  -3.858 0.000115 ***
weight        -0.5689     0.2771  -2.053 0.040060 *  
humerus       75.4610    19.1586   3.939 8.19e-05 ***
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1

>(Dispersion parameter for binomial family taken to be 1)

>    Null deviance: 118.008  on 86  degrees of freedom
Residual deviance:  75.094  on 83  degrees of freedom
AIC: 83.094

>Number of Fisher Scoring iterations: 5
 
# Make predictions
sparrow$pred <- predict(sparrow_model, type = "response")
sparrow$pred
> [1] 0.789036450 0.614461920 0.918996722 0.994546078 0.877501566 0.959636891
 [7] 0.967857632 0.857041795 0.601541688 0.937371038 0.703563162 0.944677672
[13] 0.324481862 0.822286417 0.718617304 0.903408324 0.618340102 0.397224639
[19] 0.969845946 0.922173701 0.826454929 0.629641202 0.845483122 0.307055350
[25] 0.470574430 0.948008055 0.246927802 0.919665642 0.726996421 0.641040514
[31] 0.873931544 0.843720877 0.937898478 0.762027690 0.991567960 0.116743511
[37] 0.715288904 0.659752796 0.487020282 0.595505449 0.145090567 0.013235225
[43] 0.742768753 0.849758638 0.129828241 0.555641091 0.452362860 0.599450015
[49] 0.076293572 0.167753063 0.117645568 0.171332260 0.025449140 0.787581457
[55] 0.340148037 0.110218800 0.009516223 0.044176504 0.055384873 0.981078787
[61] 0.992935586 0.507594196 0.305139525 0.933573177 0.713994815 0.731487800
[67] 0.799632479 0.278257831 0.232472954 0.871964174 0.884138105 0.969093551
[73] 0.881648015 0.922606434 0.811276335 0.892435257 0.254222635 0.772659696
[79] 0.006315099 0.277989396 0.235117651 0.395066969 0.204666252 0.046373805
[85] 0.731750328 0.164275126 0.420683268

# Look at gain curve
GainCurvePlot(sparrow, "pred", "survived", "sparrow survival model")

--------------------------------------------------------------------------------

___________________________________________________________
***Poisson and quasipoisson regression to predict counts***
___________________________________________________________

Predicting counts is a non-linear problem, because counts are restricted to being 
    non-negative and integral
        . Linear regression: predicts values in [-infinity, +infinity]
        . Counts: integers in range [0, +infinity]
        
The counterpart to linear regression for count data is poisson or quasipoisson regression.

Poisson regression is also a generalized linear model. it assumes that the inputs 
  are additive and linear with respect to the log of the outcome.

        glm(formula, data, family)
        
        . family: either poisson <or> quasipoisson
        . inputs additive and linear in log(count)
        . outcome: is an integer
            - counts: eg, number of traffic tickets a driver gets
            - rates: eg, number of website hits/day
        .  prediction: expected rate or intensity/(not integral)
            - expected # traffic tickets; expected hits/day
            
Poisson vs. Quasipoisson
    . Poisson assumes that mean(y) = var(y)
    . If var(y) much different from mean(y) - quasipoisson
    . Generally requires a large sample size
    . If rates/counts >> 0 - regular regression is fine.
Poisson and quasipoisson regression work better on larger data sets.
If the counts we want to predict are much larger than zero, doing regular regression will
  often be fine, as well.
            
Evaluate count model by RMSE    

----------------------------------------------------------------------------------------

Fit a model to predict bike rental counts
In this exercise you will build a model to predict the number of bikes rented in an hour as a function of the weather, the type of day (holiday, working day, or weekend), and the time of day. You will train the model on data from the month of July.

The data frame has the columns:

cnt: the number of bikes rented in that hour (the outcome)
hr: the hour of the day (0-23, as a factor)
holiday: TRUE/FALSE
workingday: TRUE if neither a holiday nor a weekend, else FALSE
weathersit: categorical, "Clear to partly cloudy"/"Light Precipitation"/"Misty"
temp: normalized temperature in Celsius
atemp: normalized "feeling" temperature in Celsius
hum: normalized humidity
windspeed: normalized windspeed
instant: the time index -- number of hours since beginning of data set (not a variable)
mnth and yr: month and year indices (not variables)
Remember that you must specify family = poisson or family = quasipoisson when using glm() to fit a count model.

Since there are a lot of input variables, for convenience we will specify the outcome and the inputs in variables, and use paste() to assemble a string representing the model formula.   

The data frame bikesJuly is in the workspace. The names of the outcome variable and the input variables are also in the workspace as the variables outcome and vars respectively.

    - Fill in the blanks to create the formula fmla expressing cnt as 
        a function of the inputs. Print it.
    - Calculate the mean (mean()) and variance (var()) of bikesJuly$cnt.
    - Should you use poisson or quasipoisson regression?
    - Use glm() to fit a model to the bikesJuly data: bike_model.
    - Use glance() to look at the model's fit statistics. 
        Assign the output of glance() to the variable perf.
    - Calculate the pseudo-R-squared of the model.
    
---------------------------------------------------------------------------------------

head(bikesJuly, 5)
> hr holiday workingday             weathersit temp  atemp  hum windspeed cnt
1  0   FALSE      FALSE Clear to partly cloudy 0.76 0.7273 0.66    0.0000 149
2  1   FALSE      FALSE Clear to partly cloudy 0.74 0.6970 0.70    0.1343  93
3  2   FALSE      FALSE Clear to partly cloudy 0.72 0.6970 0.74    0.0896  90
4  3   FALSE      FALSE Clear to partly cloudy 0.72 0.7121 0.84    0.1343  33
5  4   FALSE      FALSE Clear to partly cloudy 0.70 0.6667 0.79    0.1940   4
  instant mnth yr
1   13004    7  1
2   13005    7  1
3   13006    7  1
4   13007    7  1
5   13008    7  1

outcome
>[1] "cnt"

vars
>[1] "hr"         "holiday"    "workingday" "weathersit" "temp"      
[6] "atemp"      "hum"        "windspeed"            

# bikesJuly is in the workspace
str(bikesJuly)
> 'data.frame':	744 obs. of  12 variables:
 $ hr        : Factor w/ 24 levels "0","1","2","3",..: 1 2 3 4 5 6 7 8 9 10 ...
 $ holiday   : logi  FALSE FALSE FALSE FALSE FALSE FALSE ...
 $ workingday: logi  FALSE FALSE FALSE FALSE FALSE FALSE ...
 $ weathersit: chr  "Clear to partly cloudy" "Clear to partly cloudy" "Clear to partly cloudy" "Clear to partly cloudy" ...
 $ temp      : num  0.76 0.74 0.72 0.72 0.7 0.68 0.7 0.74 0.78 0.82 ...
 $ atemp     : num  0.727 0.697 0.697 0.712 0.667 ...
 $ hum       : num  0.66 0.7 0.74 0.84 0.79 0.79 0.79 0.7 0.62 0.56 ...
 $ windspeed : num  0 0.1343 0.0896 0.1343 0.194 ...
 $ cnt       : int  149 93 90 33 4 10 27 50 142 219 ...
 $ instant   : int  13004 13005 13006 13007 13008 13009 13010 13011 13012 13013 ...
 $ mnth      : int  7 7 7 7 7 7 7 7 7 7 ...
 $ yr        : int  1 1 1 1 1 1 1 1 1 1 ...
 
# The outcome column
outcome
> [1] "cnt"

# The inputs to use
vars
>[1] "hr"         "holiday"    "workingday" "weathersit" "temp"      
>[6] "atemp"      "hum"        "windspeed"
 
# Create the formula string for bikes rented as a function of the inputs
(fmla <- paste(outcome, "~", paste(vars, collapse = " + ")))
> [1] "cnt ~ hr + holiday + workingday + weathersit + temp + atemp + hum + windspeed"

# Calculate the mean and variance of the outcome
(mean_bikes <- mean(bikesJuly$cnt))
> [1] 273.6653

(var_bikes <- var(bikesJuly$cnt))
> [1] 45863.84

# Fit the model
bike_model <- glm(fmla, data = bikesJuly, family = quasipoisson)

# Call glance
(perf <- glance(bike_model))
>  null.deviance df.null logLik AIC BIC deviance df.residual
1      133364.9     743     NA  NA  NA  28774.9         712
 
# Calculate pseudo-R-squared
(pseudoR2 <- 1 - perf$deviance/perf$null.deviance)
> [1] 0.7842393            
            
----------------------------------------------------------------------------------

Predict bike rentals on new data
In this exercise you will use the model you built in the previous exercise to make predictions for the month of August. The data set bikesAugust has the same columns as bikesJuly.

Recall that you must specify type = "response" with predict() when predicting counts from a glm poisson or quasipoisson model.

The model bike_model and the data frame bikesAugust are in the workspace.

    - Use predict to predict the number of bikes per hour on the bikesAugust data. 
        Assign the predictions to the column bikesAugust$pred.
    - Fill in the blanks to get the RMSE of the predictions on the August data.
    - Fill in the blanks to generate the plot of predictions to actual counts.
    - Do any of the predictions appear negative?

------------------------------------------------------------------------------------

# bikesAugust is in the workspace
str(bikesAugust)
> 'data.frame':	744 obs. of  12 variables:
 $ hr        : Factor w/ 24 levels "0","1","2","3",..: 1 2 3 4 5 6 7 8 9 10 ...
 $ holiday   : logi  FALSE FALSE FALSE FALSE FALSE FALSE ...
 $ workingday: logi  TRUE TRUE TRUE TRUE TRUE TRUE ...
 $ weathersit: chr  "Clear to partly cloudy" "Clear to partly cloudy" "Clear to partly cloudy" "Clear to partly cloudy" ...
 $ temp      : num  0.68 0.66 0.64 0.64 0.64 0.64 0.64 0.64 0.66 0.68 ...
 $ atemp     : num  0.636 0.606 0.576 0.576 0.591 ...
 $ hum       : num  0.79 0.83 0.83 0.83 0.78 0.78 0.78 0.83 0.78 0.74 ...
 $ windspeed : num  0.1642 0.0896 0.1045 0.1045 0.1343 ...
 $ cnt       : int  47 33 13 7 4 49 185 487 681 350 ...
 $ instant   : int  13748 13749 13750 13751 13752 13753 13754 13755 13756 13757 ...
 $ mnth      : int  8 8 8 8 8 8 8 8 8 8 ...
 $ yr        : int  1 1 1 1 1 1 1 1 1 1 ...

# bike_model is in the workspace
summary(bike_model)

> Call:
glm(formula = fmla, family = quasipoisson, data = bikesJuly)

> Deviance Residuals: 
     Min        1Q    Median        3Q       Max  
-21.6117   -4.3121   -0.7223    3.5507   16.5079  

> Coefficients:
                               Estimate Std. Error t value Pr(>|t|)    
(Intercept)                    5.934986   0.439027  13.519  < 2e-16 ***
hr1                           -0.580055   0.193354  -3.000 0.002794 ** 
hr2                           -0.892314   0.215452  -4.142 3.86e-05 ***
hr3                           -1.662342   0.290658  -5.719 1.58e-08 ***
hr4                           -2.350204   0.393560  -5.972 3.71e-09 ***
hr5                           -1.084289   0.230130  -4.712 2.96e-06 ***
hr6                            0.211945   0.156476   1.354 0.176012    
hr7                            1.211135   0.132332   9.152  < 2e-16 ***
hr8                            1.648361   0.127177  12.961  < 2e-16 ***
hr9                            1.155669   0.133927   8.629  < 2e-16 ***
hr10                           0.993913   0.137096   7.250 1.09e-12 ***
hr11                           1.116547   0.136300   8.192 1.19e-15 ***
hr12                           1.282685   0.134769   9.518  < 2e-16 ***
hr13                           1.273010   0.135872   9.369  < 2e-16 ***
hr14                           1.237721   0.136386   9.075  < 2e-16 ***
hr15                           1.260647   0.136144   9.260  < 2e-16 ***
hr16                           1.515893   0.132727  11.421  < 2e-16 ***
hr17                           1.948404   0.128080  15.212  < 2e-16 ***
hr18                           1.893915   0.127812  14.818  < 2e-16 ***
hr19                           1.669277   0.128471  12.993  < 2e-16 ***
hr20                           1.420732   0.131004  10.845  < 2e-16 ***
hr21                           1.146763   0.134042   8.555  < 2e-16 ***
hr22                           0.856182   0.138982   6.160 1.21e-09 ***
hr23                           0.479197   0.148051   3.237 0.001265 ** 
holidayTRUE                    0.201598   0.079039   2.551 0.010961 *  
workingdayTRUE                 0.116798   0.033510   3.485 0.000521 ***
weathersitLight Precipitation -0.214801   0.072699  -2.955 0.003233 ** 
weathersitMisty               -0.010757   0.038600  -0.279 0.780572    
temp                          -3.246001   1.148270  -2.827 0.004833 ** 
atemp                          2.042314   0.953772   2.141 0.032589 *  
hum                           -0.748557   0.236015  -3.172 0.001581 ** 
windspeed                      0.003277   0.148814   0.022 0.982439    
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1

> (Dispersion parameter for quasipoisson family taken to be 38.98949)

>  Null deviance: 133365  on 743  degrees of freedom
Residual deviance:  28775  on 712  degrees of freedom
AIC: NA

>Number of Fisher Scoring iterations: 5
 
# Make predictions on August data
bikesAugust$pred <- predict(bike_model, newdata = bikesAugust, type = "response")
 
# Calculate the RMSE
bikesAugust %>% 
    mutate(residual = pred - cnt) %>%
    summarize(rmse  = sqrt(mean(residual^2)))
>      rmse
1 112.5815
 
# Plot predictions vs cnt (pred on x-axis)
ggplot(bikesAugust, aes(x = pred, y = cnt)) +
    geom_point() + 
    geom_abline(color = "darkblue")

---------------------------------------------------------------------------------

Visualize the Bike Rental Predictions
In the previous exercise, you visualized the bike model's predictions using the standard "outcome vs. prediction" scatter plot. Since the bike rental data is time series data, you might be interested in how the model performs as a function of time. In this exercise, you will compare the predictions and actual rentals on an hourly basis, for the first 14 days of August.

To create the plot you will use the function tidyr::gather() to consolidate the predicted and actual values from bikesAugust in a single column. gather() takes as arguments:

The "wide" data frame to be gathered (implicit in a pipe)
The name of the key column to be created - contains the names of the gathered columns.
The name of the value column to be created - contains the values of the gathered columns.
The names of the columns to be gathered into a single column.
You'll use the gathered data frame to compare the actual and predicted rental counts as a function of time. The time index, instant counts the number of observations since the beginning of data collection. The sample code converts the instants to daily units, starting from 0

The data frame bikesAugust, with the predictions (bikesAugust$pred) is in the workspace.

    - Fill in the blanks to plot the predictions and actual counts by hour for the 
        first 14 days of August.
    - convert instant to be in day units, rather than hour
    - gather() the cnt and pred columns into a column called value, 
        with a key called valuetype.
    - filter() for the first two weeks of August
    - Plot value as a function of instant (day).
    - Does the model see the general time patterns in bike rentals?
            
----------------------------------------------------------------------------------

# Plot predictions and cnt by date/time
bikesAugust %>% 
    # set start to 0, convert unit to days 
    mutate(instant = (instant - min(instant))/24) %>%  
    # gather cnt and pred into a value column
    gather(key = valuetype, value = value, cnt, pred) %>%
    filter(instant < 14) %>% # restrict to first 14 days
    # plot value by instant
    ggplot(aes(x = instant, y = value, color = valuetype, linetype = valuetype)) + 
    geom_point() + 
    geom_line() + 
    scale_x_continuous("Day", breaks = 0:14, labels = 0:14) + 
    scale_color_brewer(palette = "Dark2") + 
    ggtitle("Predicted August bike rentals, Quasipoisson model")
            
--------------------------------------------------------------------------------

_____________________________________________
***GAM to learn non-linear transformations***
_____________________________________________

With Generalized Additive Models (GAMs), the outcome depends additively on 
  unkown smooth functions of the input variables.
    - A GAM learns the functions s_u and the intercept b0 that best fit the data.
          . If we aren't sure if the relationship is quadratic, cubic, or something else,
              we can try GAM to learn the relationship

            library(mgcv)
            gam(formula, family, data)
            
            . family
              ~ gaussian (default): "regular" regression
              ~ binomial: probabilities
              ~ poisson/quasipoisson: counts
            . GAM is more complex than linear models, they are more likely to overfit
                -> best for larger data sets
            
The s() function
            
            # model response as a non-linear function of predictor
            
              fmla <- response ~ s(predictor)
              model <- gam(fmla, data=training data, family = )
            
              . s() designates that variables should be non-linear
            . Use s() with continuous variables
                ~ More than about 10 unique values
                
            # calling plot on the model to see the plot of the variable transformations
              that the algorithm learned
              
              plot(model)
            
            # call predict() to get the y values of these plots
            
              y values: predict(model, type = "terms")
              
            # call predict() with type = "response" to make predictions with the model
            
              predict(model, newdata= trainig-data, type = "response")
    
    --> GAM's performance is better than linear model, though not quite as good as 
          the quadratic or cubic models
          
    --> using GAM to learn the transformations is useful when we don't have 
          the domain knowledge to tell you the correct transform

**Practice**

Writing formulas for GAM models
When using gam() to model outcome as an additive function of the inputs, you can use the s() function inside formulas to designate that you want to use a spline to model the non-linear relationship of a continuous variable to the outcome.

Suppose that you want to predict how much weight (Wtloss) a dieter will lose over a 2-year diet plan as a function of:

Diet type (categorical)
Sex (categorical)
Age at beginning of diet (continuous)
BMI (body mass index) at beginning of diet (continuous)
You do not want to assume that any of the relationships are linear.

Which is the most appropriate formula?

>   Wtloss ~ Diet + Sex + s(Age) + s(BMI)

-----------------------------------------------------------------------

Writing formulas for GAM models (2)
Suppose that in the diet problem from the previous exercise, you now also want to take into account

the dieter's resting metabolic rate (BMR -- continuous) and
the dieter's average number hours of aerobic exercise per day (E -- continuous) at the beginning of the study.
You have reason to believe that the relationship between BMR and weight loss is linear (and you want to model it that way), but not necessarily the relationship between aerobic exercise and weight loss.

Which is the most appropriate formula?
            
> Wtloss ~ Diet + Sex + s(Age) + s(BMI) + BMR + s(E)
> because the formular wants Age, BMI and E as non-linear but BMR as linear

-----------------------------------------------------------------------

Model soybean growth with GAM
In this exercise you will model the average leaf weight on a soybean plant as a function of time (after planting). As you will see, the soybean plant doesn't grow at a steady rate, but rather has a "growth spurt" that eventually tapers off. Hence, leaf weight is not well described by a linear model.

Recall that you can designate which variable you want to model non-linearly in a formula with the s() function:

y ~ s(x)
Also remember that gam() from the package mgcv has the calling interface

gam(formula, family, data)
For standard regression, use family = gaussian (the default).

The soybean training data, soybean_train is loaded into your workspace. It has two columns: the outcome weight and the variable Time. For comparison, the linear model model.lin, which was fit using the formula weight ~ Time has already been loaded into the workspace as well.

    - Fill in the blanks to plot weight versus Time (Time on x-axis). 
        Does the relationship look linear?
            
------------------------------------------------------------        

# soybean_train is in the workspace
> summary(soybean_train)
      Plot     Variety   Year          Time           weight       
 1988F6 : 10   F:161   1988:124   Min.   :14.00   Min.   : 0.0290  
 1988F7 :  9   P:169   1989:102   1st Qu.:27.00   1st Qu.: 0.6663  
 1988P1 :  9           1990:104   Median :42.00   Median : 3.5233  
 1988P8 :  9                      Mean   :43.56   Mean   : 6.1645  
 1988P2 :  9                      3rd Qu.:56.00   3rd Qu.:10.3808  
 1988F3 :  8                      Max.   :84.00   Max.   :27.3700  
 (Other):276
 
# Plot weight vs Time (Time on x axis)
ggplot(soybean_train, aes(x = Time, y = weight)) + 
    geom_point()            

-------------------------------------------------------------            

    - Load the package mgcv.
    - Create the formula fmla.gam to express weight as a non-linear function of Time. 
        Print it.
    -  a generalized additive model on soybean_train using fmla.gam.        
            
--------------------------------------------------------------

# Load the package mgcv
library(mgcv)

# Create the formula
(fmla.gam <- weight ~ s(Time) )
> weight ~ s(Time)

# Fit the GAM Model
> model.gam <- gam(fmla.gam, soybean_train, family = gaussian)
            
--------------------------------------------------------------

    - Call summary() on the linear model model.lin (already in your workspace). 
        What is the R2?
    - Call summary() on 'model.gam. The "deviance explained" reports 
        the model's unadjusted R2. What is the R2? Which model appears to be 
        a better fit to the training data?
    - Call plot() on model.gam to see the derived relationship between Time and weight

--------------------------------------------------------------

(model.gam <- gam(fmla.gam, data = soybean_train, family = gaussian))

>Family: gaussian 
Link function: identity 

>Formula:
weight ~ s(Time)

>Estimated degrees of freedom:
8.5  total = 9.5 

>GCV score: 4.439466
 
# Call summary() on model.lin and look for R-squared
summary(model.lin)

>Call:
>lm(formula = fmla.lin, data = soybean_train)

>Residuals:
    Min      1Q  Median      3Q     Max 
-9.3933 -1.7100 -0.3909  1.9056 11.4381 

>Coefficients:
             Estimate Std. Error t value Pr(>|t|)    
(Intercept) -6.559283   0.358527  -18.30   <2e-16 ***
Time         0.292094   0.007444   39.24   <2e-16 ***
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1

>Residual standard error: 2.778 on 328 degrees of freedom
Multiple R-squared:  0.8244,	Adjusted R-squared:  0.8238 
F-statistic:  1540 on 1 and 328 DF,  p-value: < 2.2e-16
 
# Call summary() on model.gam and look for R-squared
summary(model.gam)

>Family: gaussian 
Link function: identity 

>Formula:
weight ~ s(Time)

>Parametric coefficients:
            Estimate Std. Error t value Pr(>|t|)    
(Intercept)   6.1645     0.1143   53.93   <2e-16 ***
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1

>Approximate significance of smooth terms:
          edf Ref.df     F p-value    
s(Time) 8.495   8.93 338.2  <2e-16 ***
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1

>R-sq.(adj) =  0.902   Deviance explained = 90.4%
GCV = 4.4395  Scale est. = 4.3117    n = 330
 
# Call plot() on model.gam
plot(model.gam)

-------------------------------------------------------------------------------------            
 Predict with the soybean model on test data
In this exercise you will apply the soybean models from the previous exercise (model.lin and model.gam, already in your workspace) to new data: soybean_test.           

The data frame soybean_test and the models model.lin and model.gam are in the workspace.

    - Create a column soybean_test$pred.lin with predictions from 
        the linear model model.lin.
    - Create a column soybean_test$pred.gam with predictions from the gam model model.gam.
          . For GAM models, the predict() method returns a matrix, so use as.numeric() 
              to convert the matrix to a vector.
    - Fill in the blanks to gather() the prediction columns into a single value 
        column pred with key column modeltype. Call the long dataframe soybean_long.
    - Calculate and compare the RMSE of both models.
          . Which model does better?
    - Run the code to compare the predictions of each model against 
        the actual average leaf weights.
          . A scatter plot of weight as a function of Time.
          . Point-and-line plots of the predictions (pred) as a function of Time.
          . Notice that the linear model sometimes predicts negative weights! 
              Does the gam model?

-----------------------------------------------------------------------------------------            
# soybean_test is in the workspace
summary(soybean_test)
>    Plot    Variety   Year         Time           weight       
 1988F8 : 4   F:43    1988:32   Min.   :14.00   Min.   : 0.0380  
 1988P7 : 4   P:39    1989:26   1st Qu.:23.00   1st Qu.: 0.4248  
 1989F8 : 4           1990:24   Median :41.00   Median : 3.0025  
 1990F8 : 4                     Mean   :44.09   Mean   : 7.1576  
 1988F4 : 3                     3rd Qu.:69.00   3rd Qu.:15.0113  
 1988F2 : 3                     Max.   :84.00   Max.   :30.2717  
 (Other):60
 
# Get predictions from linear model
 soybean_test$pred.lin <- predict(model.lin, newdata = soybean_test)
 
# Get predictions from gam model
soybean_test$pred.gam <- as.numeric(predict(model.gam, newdata = soybean_test))
 
# Gather the predictions into a "long" dataset
soybean_long <- soybean_test %>%
    gather(key = modeltype, value = pred, pred.lin, pred.gam)

# Calculate the rmse
soybean_long %>%
    mutate(residual = weight - pred) %>%     # residuals
    group_by(modeltype) %>%                  # group by modeltype
    summarize(rmse = sqrt(mean(residual^2))) # calculate the RMSE
> # A tibble: 2 x 2
  modeltype  rmse
  <chr>     <dbl>
1 pred.gam   2.29
2 pred.lin   3.19

# Compare the predictions against actual weights on the test data
soybean_long %>%
    ggplot(aes(x = Time)) +                         # the column for the x axis
    geom_point(aes(y = weight)) +                   # the y-column for the scatterplot
    geom_point(aes(y = pred, color = modeltype)) +  # the y-column for the point-and-line plot
    geom_line(aes(y = pred, color = modeltype, linetype = modeltype)) + # the y-column for the point-and-line plot
    scale_color_brewer(palette = "Dark2")

---------------------------------------------------------------------------

***
### PART 5: TREE-BASED METHODS ###
***
            
            
        









