---
title: "Unsupervised Learning in R"
author: "Hanh Nguyen"
date: "6/1/2020"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```


##UNSUPERVISED LEARNING IN R##


__Overview__
. Unsupervised learning
. Three major types of machine learning
. Execute one type of unsupervised learning using R

__Types of machine learning__

    1. Unsupervised learning
        - Finding structure in unlabeled data, which is data w/o a target, w/o labeled responses
        
    2. Supervised learning
        -  Making predictions based on labeled data, which is data w/ a target
        - Types of predictions includes 
              . Regression or predicting how much sth there is or could be.
              . Classification predicting what kind/class that sth is or could be.
              
    3. Reinforcement learning
        -  Computer learning from feedback by operating a real and synthetic environment

__Goals of Unsupervised learning__

    1. Finding homogenous subgroups within larger group
        - Clustering - the process of finding homogenous subgroups
                . ex1: People have features such as income, education attaiment, and gender 
                      --> one might find homogenous subgroups or groups for members have similar features
                      --> label group A, group B, ...
                . ex2: Market segmentation - grouping customers based on geopgraphic subgroup
                . ex3: Movies - grouping movie based on features/reviews of the movies 
                              --> find movies most like another mover
                              
    2. Finding patterns in the features of the data
        - Dimensionality reduction - the method to decrease the number of features to describe an observation
                                      while maintaining the maximum information content under the constraint
                                      of dimensionality
                                      
__Dimensionality Reduction__
    
    1. Finding patterns in the features of the data
    
    2. Allowing visualization of high dimensional data while maintaining much of the data variability
        --> This is done because visually representing and understanding data with more than 3 or 4 features
              can be difficult for both the producer and consumer of the visualization
              
    3. Pre-processing before supervised learning.

__Challenges and benefits__
    
    1. No single goal of analysis
        ex: find the pattern of the data
        
    2. Requires more creativity
    
    3. Much more unlabeled data available than cleanly labeled data
        -> more opportunity to supply unsupervised learning

--------------------------------------------------------------------------------

**Identify clustering problems**

Which of the following are clustering problems?

1. Determining how many features it takes to describe most of the variability in data
2. Determining the natural groupings of houses for sale based on size, number of bedrooms, etc.
3. Visualizing 13 dimensional data (data with 13 features)
4. Determining if there are common patterns in the demographics of people at a commerce site
5. Predicting if someone will click on a web advertisement

          ---> 2 and 4
               (1 is not a clustering problems because it is dimensionality reduction)
               
------------------------------------------------------------------------------------



####################################
## CHAPTER 1 : k-MEANS CLUSTERING ##
####################################

_______________________________________
## Introduction to k-mean clustering ##
_______________________________________

__k-means clustering algorithm__

    - An algorithm to find homogenous subgroups within a population.
    - Works by first assuming the number of subgroups clustering in the data and then assigning each
        observation to one of the subgroups
        (ie, breaks observations into pre-defined number of clusters)
        
__k-means in R__

      # k-means algorithm with 5 centers, run 20 times
      
          kmeans(x, centers = 5, nstart = 20)
          
              . x : the data
                  - one observation per row, one feature per column
              . centers : the number of pre-determined groups/clusters
              . nstart :  the number of time k-means will be repeated
                  - k-means has a random component, thus a single run of k-means may not find the optimal solution.
                    --> Therefore, k-means can be run multiple times to improve odds of the best model
                    --> best outcome will be selected.

-----------------------------------------------------------------------------------------

**k-means clustering**

We have created some two-dimensional data and stored it in a variable called x in your workspace. 
The scatter plot on the right is a visual representation of the data.

In this exercise, your task is to create a k-means model of the x data using 3 clusters, 
then to look at the structure of the resulting model using the summary() function.

- Fit a k-means model to x using 3 centers and run the k-means algorithm 20 times. Store the result in km.out.
- Inspect the result with the summary() function.

----------------------------------------------------------------------------------------------

head(x)
>     [,1]     [,2]
[1,] 3.370958 1.995379
[2,] 1.435302 2.760242
[3,] 2.363128 2.038991
[4,] 2.632863 2.735072
[5,] 2.404268 1.853527
[6,] 1.893875 1.942113
                  
# Create the k-means model: km.out
km.out <- kmeans(x, centers = 3, nstart = 20)
 
# Inspect the result
summary(km.out)
>            Length Class  Mode   
cluster      300    -none- numeric
centers        6    -none- numeric
totss          1    -none- numeric
withinss       3    -none- numeric
tot.withinss   1    -none- numeric
betweenss      1    -none- numeric
size           3    -none- numeric
iter           1    -none- numeric
ifault         1    -none- numeric

-----------------------------------------------------------------------------------------------------

**Results of kmeans()**

The kmeans() function produces several outputs. In the video, we discussed one output of modeling, 
the cluster membership.

In this exercise, you will access the cluster component directly. This is useful anytime you need the cluster
membership for each observation of the data used to build the clustering model. A future exercise will show 
an example of how this cluster membership might be used to help communicate the results of k-means modeling.

k-means models also have a print method to give a human friendly output of basic modeling results. 
This is available by using print() or simply typing the name of the model.

The k-means model you built in the last exercise, km.out, is still available in your workspace.

- Print a list of the cluster membership to the console.
- Use a print method to print out the km.out model.

------------------------------------------------------------------------------------------------------

# Print the cluster membership component of the model
print(km.out$cluster)
> [1] 1 1 1 1 1 1 1 1 1 1 1 1 3 1 1 1 1 3 3 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 3
 [38] 3 3 1 1 1 1 3 1 1 1 1 1 1 1 1 1 1 1 1 1 1 3 1 1 1 1 1 1 1 1 1 1 1 1 1 1 3
 [75] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 2 2 2 2 2 2 2 2 2 2 2
[112] 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2
[149] 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2
[186] 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2
[223] 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 3 3 3 3 1 3 3 3 3
[260] 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 1 1 3 3 1 3 3 3 3 3 3 1 3 3 3 3 3 3 1 3 3
[297] 3 1 3 3
 
# Print the km.out object
print(km.out)
> K-means clustering with 3 clusters of sizes 98, 150, 52

> Cluster means:
        [,1]        [,2]
1  2.2171113  2.05110690
2 -5.0556758  1.96991743
3  0.6642455 -0.09132968

> Clustering vector:
  [1] 1 1 1 1 1 1 1 1 1 1 1 1 3 1 1 1 1 3 3 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 3
 [38] 3 3 1 1 1 1 3 1 1 1 1 1 1 1 1 1 1 1 1 1 1 3 1 1 1 1 1 1 1 1 1 1 1 1 1 1 3
 [75] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 2 2 2 2 2 2 2 2 2 2 2
[112] 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2
[149] 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2
[186] 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2
[223] 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 3 3 3 3 1 3 3 3 3
[260] 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 1 1 3 3 1 3 3 3 3 3 3 1 3 3 3 3 3 3 1 3 3
[297] 3 1 3 3

> Within cluster sum of squares by cluster:
[1] 148.64781 295.16925  95.50625
 (between_SS / total_SS =  87.2 %)

> Available components:
[1] "cluster"      "centers"      "totss"        "withinss"     "tot.withinss"
[6] "betweenss"    "size"         "iter"         "ifault"

-------------------------------------------------------------------------------------------------

**Visualizing and interpreting results of kmeans()**

One of the more intuitive ways to interpret the results of k-means models is by plotting the data as a scatter plot
and using color to label the samples' cluster membership. 

In this exercise, you will use the standard plot() function to accomplish this.

To create a scatter plot, you can pass data with two features (i.e. columns) to plot() with an extra argument col = km.out$cluster, which sets the color of each point in the scatter plot according to its cluster membership.

x and km.out are available in your workspace. Using the plot() function to create a scatter plot of data x:

- Color the dots on the scatterplot by setting the col argument to the cluster component in km.out.
- Title the plot "k-means with 3 clusters" using the main argument to plot().
- Ensure there are no axis labels by specifying "" for both the xlab and ylab arguments to plot()

----------------------------------------------------------------------------------------------------

# Scatter plot of x
plot(x, col = km.out$cluster, main = "k-means with 3 clusters", xlab = "", ylab = "")

---------------------------------------------------------------------------------------------------

______________________________________________
## How kmeans() works and practical matters ##
______________________________________________

__Ojectives__
. Explain how k-means algorithm is implemented visually
. Model selection: determine number of clusters



__How kmeans() works__

Example: 
        Examining a visualizing data with 2 features --> 2 subgroubs
        
        1. Randomly assign each to one of the two clusters
        2. Calculate the center of each of the two subgroups
            The center of each of the subgroups is the average position of all the points in that subgroup.
        3. Each point is assigned to the cluster of the nearest center
    --Complete 1 iteration of the k-means algorithm--
        1. New clustered center of each subgroup will be calculated.
            The new center of each subgroup will be the average position of all points in that subgroup
        2. Each point is assigned tot he cluster of the nearest center
    --Complete another iteration--
    
        Some point of change may happen after each iteration
        When no observation has changed assignment, the k-means algorithm stops

There're other stopping criteria, such as:
    . stopping after a number of iteration
    . the cluster center moves less than some distance
    

__Model selection__

. Because the k-means has the random component, it is run multiple times and the best solution is selected from
    multiple runs.
--> The k-means() algorithm needs the measurement of the model quality to determine the best outcome of multiple runs.
--> The best outcome is based on total within cluster sum of squares as that measurement.

. The k-means() run with the MINIMUM total within cluster sum of squares is considered the best model.

. Total within cluster sum of squares is calculated as:
    - for each cluster
        - for each for each observation in the cluster
            - calculate the squared distance from the observation to the cluster center
            - then, sum all of the square distances
        --> And that is the total within cluster sum of squares
        
. Running algorithm multiple times helps find the global minimum total within cluster sum of squares

. It is possible to find identical groupings and Total Within sum of squares but different cluster labels

__Determining the number of clusters__

. Trial and error is not the best approach
. k-means is a better approach to run in one or some number of clusters, recording the total within sum of squares (y)
    for each number of cluster (x) --< scree plot
          - Elbow in scree plot: the place for the total within sum of squares decreases with much lower with another
              cluster -- The elbow value is used to approximate the number of cluster
              
-----------------------------------------------------------------------------------------------------------------

**Handling random algorithms**

In the video, you saw how kmeans() randomly initializes the centers of clusters. This random initialization can 
result in assigning observations to different cluster labels. Also, the random initialization can result in 
finding different local minima for the k-means algorithm. This exercise will demonstrate both results.

At the top of each plot, the measure of model quality—total within cluster sum of squares error—will be plotted.
Look for the model(s) with the lowest error to find models with the better model results.

Because kmeans() initializes observations to random clusters, it is important to set the random number generator 
seed for reproducibility.

The data, x, is still available in your workspace. Your task is to generate six kmeans() models on the data, 
plotting the results of each, in order to see the impact of random initializations on model results.

- Set the random number seed to 1 with set.seed().
- For each iteration of the for loop, run kmeans() on x. 
    Assume the number of clusters is 3 and number of starts (nstart) is 1.
- Visualize the cluster memberships using the col argument to plot(). 
    Observe how the measure of quality and cluster assignments vary among the six model runs.
    
------------------------------------------------------------------------------------------------------------------

# Set up 2 x 3 plotting grid
par(mfrow = c(2, 3))
 
# Set seed
set.seed(1)
 
for(i in 1:6) {
    # Run kmeans() on x with three clusters and one start
    km.out <- kmeans(x, centers = 3, nstart = 1)
    
    # Plot clusters
    plot(x, col = km.out$cluster, 
         main = km.out$tot.withinss, 
         xlab = "", ylab = "")
  }     

--------------------------------------------------------------------------------------------------------------------

**Selecting number of clusters**

The k-means algorithm assumes the number of clusters as part of the input. If you know the number of clusters
in advance (e.g. due to certain business constraints) this makes setting the number of clusters easy. However, 
as you saw in the video, if you do not know the number of clusters and need to determine it, you will need to 
run the algorithm multiple times, each time with a different number of clusters. From this, you can observe 
how a measure of model quality changes with the number of clusters.

In this exercise, you will run kmeans() multiple times to see how model quality changes as the number of 
clusters changes. Plots displaying this information help to determine the number of clusters and are often
referred to as scree plots.

The ideal plot will have an elbow where the quality measure improves more slowly as the number of clusters increases. This indicates that the quality of the model is no longer improving substantially as the model complexity 
(i.e. number of clusters) increases. In other words, the elbow indicates the number of clusters inherent in the data.

The data, x, is still available in your workspace.

- Build 15 kmeans() models on x, each with a different number of clusters (ranging from 1 to 15). 
  Set nstart = 20 for all model runs and save the total within cluster sum of squares for each model to the 
  ith element of wss.
- Run the code provided to create a scree plot of the wss for all 15 models.
- Take a look at your scree plot. How many clusters are inherent in the data? 
  Set k equal to the number of clusters at the location of the elbow.

----------------------------------------------------------------------------------------------------------------

# Initialize total within sum of squares error: wss
wss <- 0
 
# For 1 to 15 cluster centers
 for (i in 1:15) {
    km.out <- kmeans(x, centers = i, nstart = 20)
    # Save total within sum of squares to wss variable
    wss[i] <- km.out$tot.withins
  }
 
# Plot total within sum of squares vs. number of clusters
 plot(1:15, wss, type = "b", 
       xlab = "Number of Clusters", 
       ylab = "Within groups sum of squares")
 
# Set k equal to the number of clusters corresponding to the elbow location
 k <- 2
------------------------------------------------------------------------------------------------------------------


__________________________________
## Introduction to Pokemon data ##
__________________________________

. 6 features:
    - Hitpoints
    - Attack
    - Defense
    - SpecialAttack
    - SpecialDefense
    - Speed
    
__Data Challenges__

    1. Determining which variable to use for clustering
        - Which features should be used for clustering
        - Sometimes try multiple subsets of features is an important step to find the pattern of the data
        
    2. Scaling the data
    
    3. Determining the number of clusters
          - In real world data, clean "elbow" in scree plot is rarely exist
          
    4. Visualizing the result for interpretation
    
-----------------------------------------------------------------------------------------------------------------

**Practical matters: working with real data**

Dealing with real data is often more challenging than dealing with synthetic data. 
Synthetic data helps with learning new concepts and techniques, but the next few exercises will deal with data 
that is closer to the type of real data you might find in your professional or academic pursuits.

The first challenge with the Pokemon data is that there is no pre-determined number of clusters. 
You will determine the appropriate number of clusters, keeping in mind that in real data the elbow in the scree plot
might be less of a sharp elbow than in synthetic data. 
Use your judgement on making the determination of the number of clusters.

The second part of this exercise includes plotting the outcomes of the clustering on two dimensions, or features, 
of the data. These features were chosen somewhat arbitrarily for this exercise. 
Think about how you would use plotting and clustering to communicate interesting groups of Pokemon to other people.

An additional note: this exercise utilizes the iter.max argument to kmeans(). As you've seen, kmeans() is an 
iterative algorithm, repeating over and over until some stopping criterion is reached. The default number of
iterations for kmeans() is 10, which is not enough for the algorithm to converge and reach its stopping criterion, 
so we'll set the number of iterations to 50 to overcome this issue. To see what happens when kmeans() does not
converge, try running the example with a lower number of iterations (e.g. 3). 
This is another example of what might happen when you encounter real data and use real cases.

The pokemon dataset, which contains observations of 800 Pokemon characters on 6 dimensions (i.e. features),
is available in your workspace.

- Using kmeans() with nstart = 20, determine the total within sum of square errors for different numbers of 
    clusters (between 1 and 15).
- Pick an appropriate number of clusters based on these results from the first instruction and assign that 
    number to k.
- Create a k-means model using k clusters and assign it to the km.out variable.
- Create a scatter plot of Defense vs. Speed, showing cluster membership for each observation.

-----------------------------------------------------------------------------------------------------------

head(pokemon)
>     HitPoints Attack Defense SpecialAttack SpecialDefense Speed
[1,]        45     49      49            65             65    45
[2,]        60     62      63            80             80    60
[3,]        80     82      83           100            100    80
[4,]        80    100     123           122            120    80
[5,]        39     52      43            60             50    65
[6,]        58     64      58            80             65    80

# Initialize total within sum of squares error: wss
wss <- 0

# Look over 1 to 15 possible clusters
for (i in 1:15) {
    # Fit the model: km.out
    km.out <- kmeans(pokemon, centers = i, nstart = 20, iter.max = 50)
    # Save the within cluster sum of squares
    wss[i] <- km.out$tot.withins
  }
 
# Produce a scree plot
plot(1:15, wss, type = "b", 
       xlab = "Number of Clusters", 
       ylab = "Within groups sum of squares")
 
# Select number of clusters
k <- 2
 
# Build model with k clusters: km.out
km.out <- kmeans(pokemon, centers = 2, nstart = 20, iter.max = 50)

# View the resulting model
km.out
> K-means clustering with 2 clusters of sizes 426, 374

> Cluster means:
  HitPoints   Attack  Defense SpecialAttack SpecialDefense    Speed
1  81.88263 97.49061 89.94836      90.80751       88.11033 80.77465
2  54.87968 57.94118 55.49733      52.33155       53.44118 54.04278

> Clustering vector:
  [1] 2 2 1 1 2 2 1 1 1 2 2 1 1 2 2 2 2 2 2 1 2 2 1 1 2 2 2 1 2 1 2 1 2 1 2 2 1
 [38] 2 2 1 2 1 2 1 2 2 2 1 2 2 1 2 2 2 1 2 2 2 2 2 1 2 1 2 1 2 2 1 2 2 1 1 2 2
 [75] 1 2 2 1 2 1 2 2 1 2 1 2 1 1 2 1 2 2 1 2 1 2 1 2 1 2 2 1 1 2 2 1 2 1 2 1 2
[112] 1 2 1 1 1 2 2 1 2 1 2 1 1 1 2 1 2 1 2 1 1 1 1 1 1 1 1 1 2 1 1 1 2 2 1 1 1
[149] 2 2 1 2 1 1 1 1 1 1 1 2 2 1 1 1 1 1 2 2 1 2 2 1 2 2 1 2 2 2 1 2 2 2 2 1 2
[186] 1 2 2 2 2 2 2 1 2 2 1 1 1 2 2 2 1 2 2 1 2 2 1 2 2 1 1 1 2 1 1 2 2 1 2 1 2
[223] 2 1 1 2 1 2 1 1 1 1 1 2 2 1 2 2 2 1 2 2 1 2 1 1 2 1 1 1 2 1 1 1 2 2 1 2 2
[260] 2 1 1 1 1 1 2 2 1 1 1 1 1 2 2 1 1 2 2 1 1 2 2 1 1 2 2 2 2 2 2 2 2 2 2 2 1
[297] 2 2 1 2 2 2 1 2 2 1 1 2 2 2 1 2 1 1 2 1 2 2 2 1 2 1 2 2 2 2 2 1 2 1 2 1 1
[334] 1 2 2 1 2 1 1 2 2 2 2 2 2 1 2 1 1 2 1 2 1 1 1 2 1 2 2 2 1 2 1 2 1 1 1 1 1
[371] 1 2 1 2 1 2 1 2 1 2 1 2 1 2 1 2 1 1 2 1 1 2 1 1 2 2 1 1 2 2 1 2 1 1 1 2 2
[408] 2 1 1 2 2 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 2 2 1 2 2 1 2 2 1 2 2 1
[445] 2 2 2 2 2 2 1 2 1 2 1 2 1 2 1 1 1 1 2 1 2 2 1 2 1 2 1 1 2 1 2 1 1 1 1 2 1
[482] 2 2 1 2 1 2 2 2 2 1 2 2 1 1 2 2 1 1 2 1 2 1 2 1 1 2 1 2 2 1 1 1 1 1 1 1 1
[519] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 2
[556] 2 1 2 2 1 2 2 1 2 2 2 2 1 2 1 2 1 2 1 2 1 2 1 2 2 1 2 1 2 2 1 2 2 2 1 1 1
[593] 2 2 1 2 2 1 1 1 2 2 1 2 2 1 2 1 2 1 1 2 2 1 2 1 1 1 2 1 2 1 1 2 1 2 1 2 1
[630] 2 1 2 1 2 1 2 2 1 2 2 1 2 1 2 2 1 2 1 2 2 1 2 1 2 1 1 2 1 2 1 2 1 1 2 2 1
[667] 2 1 2 2 1 2 2 1 2 1 1 2 1 1 2 1 1 2 1 2 1 1 2 1 2 1 1 1 2 2 1 2 1 1 1 1 1
[704] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 2 2 1 2 2 1 2 2 1 2 2 2 2 1 2 2 2 2 1 2 2 1
[741] 2 1 2 1 1 2 1 1 2 1 1 1 2 1 2 1 2 1 2 1 2 1 2 1 2 1 2 1 2 1 1 1 2 1 2 1 1
[778] 1 2 1 2 2 2 2 1 1 1 1 2 1 2 1 1 1 1 1 1 1 1 1

> Within cluster sum of squares by cluster:
[1] 2007145.9  911966.2
 (between_SS / total_SS =  31.9 %)

> Available components:
[1] "cluster"      "centers"      "totss"        "withinss"     "tot.withinss"
[6] "betweenss"    "size"         "iter"         "ifault"

# Plot of Defense vs. Speed by cluster membership
plot(pokemon[, c("Defense", "Speed")],
       col = km.out$cluster,
       main = paste("k-means clustering of Pokemon with", k, "clusters"),
       xlab = "Defense", ylab = "Speed")
       
__________________________________________________________________________________________________________



____________________________________________
## Introduction to hierachical clustering ##
____________________________________________

. Hierarchical clustering is used when a number of clusters is not known ahead of time
      (different from k-means which first requires clustering identification before algorithm execution)

. 2 kinds of hierarchical clustering : *bottom-up* and *top-down*


#################################################
# CHAPTER 2 : BOTTOM-UP HIERARCHICAL CLUSTERING #
#################################################

__Bottom-up hierarchical clustering__

    1. First starts off assigning each point to its own cluster 
        --> 5 point 5 clusters, color coded for reference
    2. Find the closest 2 clusters and to join them together into a single cluster
        --> reduce the number of cluster, increase the number of point in each cluster
    3. This process continues iteratively finding the next pair of clusters that are close to each other
        and combining them into a single cluster.
    4. This process continues until there is only one cluster.
        Once there is only a single cluster the hierarchical clustering stops
        
__Hierarchical clustering in R__

. Performing hierarchical clustering in R requires only 1 paramter, which is the distance between observations.

    # Calculates similarity as Euclidean distance between observations
    
      dist_matrix <- dist(x)
          . x is a data matrix (1 observation per row and 1 feature per column)
          
    # Returns hierarchical clustering model
      
      hclust(d = dist_matrix)
      
    # Outcome
    
      Call:
      hclust(d = s)
      
      Cluster method    : complete
      Distance          : euclidean
      Number of objects : 50
      
----------------------------------------------------------------------------------------------------------

**Hierarchical clustering with results**

In this exercise, you will create your first hierarchical clustering model using the hclust() function.

We have created some data that has two dimensions and placed it in a variable called x. Your task is 
to create a hierarchical clustering model of x. Remember` from the video that the first step to hierarchical
clustering is determining the similarity between observations, which you will do with the dist() function.

You will look at the structure of the resulting model using the summary() function.

- Fit a hierarchical clustering model to x using the hclust() function. 
    Store the result in hclust.out.
- Inspect the result with the summary() function.

------------------------------------------------------------------------------------------------------------

# Create hierarchical clustering model: hclust.out
hclust.out <- hclust(d = dist(x))
 
# Inspect the result
summary(hclust.out)
>            Length Class  Mode     
merge       98     -none- numeric  
height      49     -none- numeric  
order       50     -none- numeric  
labels       0     -none- NULL     
method       1     -none- character
call         2     -none- call     
dist.method  1     -none- character

--------------------------------------------------------------------------------------------------------------


__________________________________
## Selecting number of clusters ##
__________________________________

__Interpreting results__

. Using summary() to interpret the results is not useful as it contain only technical information

__Dendrogram__

. Dendrogram is the tree shaped structure used to interpret hierarchical clustering model

  1. Every single point is a cluster
  2. The two closest cluster are joined together into a single cluster
        The distance between the cluster represents the height of the horizontal line on the dendrogram
  3. The next iteration is then joined the next two closest clusters.
  4. The algorithm continues until there's only one cluster remaining
  Complete the tree representation

__Dendrogram plotting in R__

        # Draws a dendrogram
          
          plot(hclust.out)
          abline(h = 6, col = "red")
          
          . h = height
          . col = "<color>"
          
. Determine the number of clusters wanted in the model
    -> drawing a cut line at a particular height or distance between the clusters
    -> The number clusters that are separated after cut will be the number of clusters wanted in the model
    
__Tree 'cutting' in R__

. Use cutree() to make cluster assignment to each observation in the cluster

          # Cut by height h
          
            cutree(hclust.out, h = 6)
            
          # Cut by number of clusters k
          
            cutree(hclust.out, k = 2)
          
              
              . hclust.out : the clustering model
              . h : the height wanted to cut
              . k : number of cluster wanted to maintain
              
              . The result is vector with numeric cluster assigment for each observation
              
----------------------------------------------------------------------------------------------

**Cutting the tree**

Remember from the video that cutree() is the R function that cuts a hierarchical model. The h and k arguments
to cutree() allow you to cut the tree based on a certain height h or a certain number of clusters k.

In this exercise, you will use cutree() to cut the hierarchical model you created earlier based on each of these two criteria.


The hclust.out model you created earlier is available in your workspace.

- Cut the hclust.out model at height 7.
- Cut the hclust.out model to create 3 clusters.

---------------------------------------------------------------------------------------------------------

# Cut by height
cutree(hclust.out, h = 7)
> [1] 1 1 1 1 1 1 1 1 2 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 3 3 3 3 3 3 3 3 3 3 2 2 2
[39] 2 2 2 2 2 2 2 2 2 2 2 2
 
# Cut by number of clusters
cutree(hclust.out, k = 3)
> [1] 1 1 1 1 1 1 1 1 2 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 3 3 3 3 3 3 3 3 3 3 2 2 2
[39] 2 2 2 2 2 2 2 2 2 2 2 2

----------------------------------------------------------------------------------------------------------

______________________________________________
## Clustering linkage and practical matters ##
______________________________________________

__Linking clusters in hierarchical clustering__

. How is distance between clusters betermined? Rules?

    As soon as the first two observations are combined to a cluster, the cluster needs rules of 
    how to mearsure the distance between cluster
    
    4 methods to to determine whic cluster should be linked
    
      1. Complete method (default) - pairwise similarity/distance between all observations 
                                      in cluster 1 and cluster 2, and uses *largest* similarities/ distances
                                      between the clusters
                                   - tend to be used when observations are fused in one at a time
                                        --> un-balanced tree
      
      2. Single method - same as above but uses *smallest* of similarities
      
      3. Average method - same as above but uses *average* of similarities
      
      4. Centroid method - find centroid of cluster 1 and centroid of cluster 2, and uses similarity/distance
                            between two centroids
                         - creates inversion for clusters are fused in either clusters
                              --> undesired behavior --> used much less often than others

Complete and Average linking method tends to produce more balance tree

__Linkage in R__

        # Fitting hierarchical clustering models using different methods
        
          hclust.complete <- hclust(d, method = "complete")
          hclust.average <- hclust(d, method = "average")
          hclust.single <- hclust(d, method = "single")
          
__Practical matters__

  - Problem: Data on different scales/unit of measurement can cause undesirable results in clustering methods.
  
  - Solution: Scale data so that features have same mean and standard deviation
              (ie, data is transform through a linear transformation before performing clustering)
      --> Normalization:
        . Subtract mean of a feature from each of the observations.
        . Divide each feature by the standard deviation of the feature by the standard deviation of the feature
        . Normalized features have a mean of zero and a standard deviation of one
        
            - If any of the features are on different scales/unit of measurement, it is customary to normalize
                all features
            - Even when the same scales/unit of measuremnt is used, it is necessary to check the variability 
                of the mean and standard deviation of the features. If the mean and standard deviation 
                varies across the features, scaling is in order
                
        # check if scaling is necessary
        
        # check the mean
            
            colMeans(x)                  # x is data matrix
          [1] -0.1337828   0.0594019    
        
        # check the standard deviation
        
            apply(x, 2, sd)
          [1] 1.974376    2.112357
          
        # Produce new matrix with column of mean of 0 and sd of 1  --> normalized features
        
            scaled_x <- scale(x)
            
            colMeans(scaled_x)
          [1]  2.775558e-17    3.330669e-17
          
            apply(scaled_x, 2, sd)
          [1]  1   1
          
------------------------------------------------------------------------------------------------------------

**Linkage methods**

In this exercise, you will produce hierarchical clustering models using different linkages and plot the dendrogram for each, observing the overall structure of the trees.

You'll be asked to interpret the results in the next exercise.

- Produce three hierarchical clustering models on x using the "complete", "average", and "single" 
  linkage methods, respectively.
- Plot a dendrogram for each model, using titles of "Complete", "Average", and "Single", respectively.

-----------------------------------------------------------------------------------------------------------

# Cluster using complete linkage: hclust.complete
hclust.complete <- hclust(dist(x), method = "complete")

# Cluster using average linkage: hclust.average
hclust.average <- hclust(dist(x), method = "average")

# Cluster using single linkage: hclust.single
hclust.single <- hclust(dist(x), method = "single")

# Plot dendrogram of hclust.complete
plot(hclust.complete, main = "Complete")

# Plot dendrogram of hclust.average
plot(hclust.average, main = "Average")

# Plot dendrogram of hclust.single
plot(hclust.single, main = "Single")

---------------------------------------------------------------------------------------------------------

**Practical matters: scaling**

Recall from the video that clustering real data may require scaling the features if they have different distributions. So far in this chapter, you have been working with synthetic data that did not need scaling.

In this exercise, you will go back to working with "real" data, the pokemon dataset introduced in the first chapter. You will observe the distribution (mean and standard deviation) of each feature, scale the data accordingly, then produce a hierarchical clustering model using the complete linkage method.

The data is stored in the pokemon object in your workspace.

- Observe the mean of each variable in pokemon using the colMeans() function.
- Observe the standard deviation of each variable using the apply() and sd() functions. Since the variables 
    are the columns of your matrix, make sure to specify 2 as the MARGIN argument to apply().
- Scale the pokemon data using the scale() function and store the result in pokemon.scaled.
- Create a hierarchical clustering model of the pokemon.scaled data using the complete linkage method. 
    Manually specify the method argument and store the result in hclust.pokemon.
    
-------------------------------------------------------------------------------------------------------

 # View column means
colMeans(pokemon)
>   HitPoints         Attack        Defense  SpecialAttack SpecialDefense 
      69.25875       79.00125       73.84250       72.82000       71.90250 
         Speed 
      68.27750

# View column standard deviations
apply(pokemon, 2, sd)
>  HitPoints         Attack        Defense  SpecialAttack SpecialDefense 
      25.53467       32.45737       31.18350       32.72229       27.82892 
         Speed 
      29.06047

# Scale the data
pokemon.scaled <- scale(pokemon)
 
# Create hierarchical clustering model: hclust.pokemon
hclust.pokemon <- hclust(dist(pokemon.scaled), method = "complete")

# View column means
colMeans(pokemon)
>   HitPoints         Attack        Defense  SpecialAttack SpecialDefense 
      69.25875       79.00125       73.84250       72.82000       71.90250 
         Speed 
      68.27750
 
# View column standard deviations
apply(pokemon, 2, sd)
>   HitPoints         Attack        Defense  SpecialAttack SpecialDefense 
      25.53467       32.45737       31.18350       32.72229       27.82892 
         Speed 
      29.06047
 
# Scale the data
pokemon.scaled <- scale(pokemon)
 
# Create hierarchical clustering model: hclust.pokemon
hclust.pokemon <- hclust(dist(pokemon.scaled), method = "complete")

------------------------------------------------------------------------------------------------------

**Comparing kmeans() and hclust()**

Comparing k-means and hierarchical clustering, you'll see the two methods produce different cluster memberships. This is because the two algorithms make different assumptions about how the data is generated. In a more advanced course, we could choose to use one model over another based on the quality of the models' assumptions, but for now, it's enough to observe that they are different.

This exercise will have you compare results from the two models on the pokemon dataset to see how they differ.

The results from running k-means clustering on the pokemon data (for 3 clusters) are stored as km.pokemon. The hierarchical clustering model you created in the previous exercise is still available as hclust.pokemon.

- Using cutree() on hclust.pokemon, assign cluster membership to each observation.
    Assume three clusters and assign the result to a vector called cut.pokemon.
- Using table(), compare cluster membership between the two clustering methods. 
    Recall that the different components of k-means model objects can be accessed with the $ operator.

------------------------------------------------------------------------------------------------------

# Apply cutree() to hclust.pokemon: cut.pokemon
> (cut.pokemon <- cutree(hclust.pokemon, k = 3))
  [1] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1
 [38] 1 1 1 1 1 1 1 1 2 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1
 [75] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1
[112] 1 1 1 1 1 1 1 1 1 1 2 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1
[149] 1 1 1 1 1 1 1 2 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1
[186] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 2 1 1 1 1
[223] 1 1 1 1 1 1 1 1 3 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1
[260] 1 1 2 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1
[297] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 2 1 1 1 1 1 1 1 1 1 1 1
[334] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 2 2 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1
[371] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1
[408] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1
[445] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 2 1 1 1 1 1 1 1
[482] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 2 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1
[519] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1
[556] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1
[593] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1
[630] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 2 1 1 1 1 1 1 1 1 1 1
[667] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1
[704] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1
[741] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1
[778] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1

# Compare methods
table(km.pokemon$cluster, cut.pokemon)
>  cut.pokemon
      1   2   3
  1 204   9   1
  2 342   1   0
  3 242   1   0

---------------------------------------------------------------------------------------------------------


#####################################
##DIMENSIONALITY REDUCTION WITH PCA##
#####################################


_________________________
## Introduction to PCA ##
_________________________

. Dimensionality reduction is another type of Unsupervised learning
    - 2 main goals
        1. Find structure within features
        2. Aid in visualization
        
. A popular method of Dimensionality reduction is *Principal Component Analysis (PCA)*
    - 3 goals when finding lower dimensional representation of features:
        1. Find the linear combination of the original features/variables. Linear combination just means
            it takes some fractions of some or all feartures and add them together. These new features 
            are called principal components.
        2. In those new features, PCA will maintain as much variance as they can from the original data
            for a given number of principal components
        3. The new features are uncorellated (ie, orthogonal to each other)
        
__PCA intuition__

  2 dimensions: x and y ---- PCA ------> 1 dimension: One principal component
  
  PCA will help map the data from 2 features to 1 features while maintaining as much as data variability
  as possible.
  
  1. Fit the regression line to the data. This line is the firt principal component.
  2. Each point is then mapped and projected on the line. This projecting value on the principal component
      is called component scores or factor scores
      
__PCA in R__

    pr.iris <- prcomp(x = iris[-5], scale = FALSE, center = TRUE)
    
          . x : the original data
          . scale : indicates the data should be scaled the standard deviation of 1 before performing PCA
          . center : indicates the data should be centered around zero before performing PCA
                      reccomend leaving it as TRUE
                      
           . True or False meanings perform or not program
                      
    summary(pr.iris)

------------------------------------------------------------------------------------------------------

**PCA using prcomp()**

In this exercise, you will create your first PCA model and observe the diagnostic results.

We have loaded the Pokemon data from earlier, which has four dimensions, and placed it in a variable called pokemon. Your task is to create a PCA model of the data, then to inspect the resulting model using the summary() function.

- Create a PCA model of the data in pokemon, setting scale to TRUE. Store the result in pr.out.
- Inspect the result with the summary() function.

------------------------------------------------------------------------------------------------------

# Perform scaled PCA: pr.out
pr.out <- prcomp(x = pokemon, scale = TRUE, center = TRUE)

# Inspect model output
summary(pr.out)
> Importance of components:
                          PC1    PC2    PC3     PC4
Standard deviation     1.4420 1.0013 0.7941 0.53595
Proportion of Variance 0.5199 0.2507 0.1577 0.07181
Cumulative Proportion  0.5199 0.7705 0.9282 1.00000

------------------------------------------------------------------------------------------------

**Results of PCA**

This exercise will check your understanding of the summary() of a PCA model. Your model from the last exercise, pr.out, and the pokemon dataset are still available in your workspace.

What is the minimum number of principal components that are required to describe at least 75% of the cumulative variance in this dataset?

--> The first two principal components describe around 77% of the variance.

Importance of components:
                          PC1    PC2    PC3     PC4
Standard deviation     1.4420   1.0013 0.7941 0.53595
Proportion of Variance 0.5199   0.2507 0.1577 0.07181
Cumulative Proportion  0.5199 **0.7705** 0.9282 1.00000

--------------------------------------------------------------------------------------------------------

**Additional results of PCA**

PCA models in R produce additional diagnostic and output components:

. center: the column means used to center to the data, or FALSE if the data weren't centered
. scale: the column standard deviations used to scale the data, or FALSE if the data weren't scaled
. rotation: the directions of the principal component vectors in terms of the original features/variables. 
            This information allows you to define new data in terms of the original principal components
. x: the value of each observation in the original dataset projected to the principal components

You can access these the same as other model components. For example, use pr.out$rotation to access the rotation component.

Which of the following statements is not correct regarding the pr.out model fit on the pokemon data?

-->The directions of the principal component vectors are presented in a table with the same dimensions as the original data.

--------------------------------------------------------------------------------------------------

_____________________________________________
## Visualizing and interpreting PCA result ##
_____________________________________________

__Biplot__

. This plot shows all of the original observations is point plot in the first 2 principal components.
. Biplot also shows the original features' vectors mapped onto the first principal components.

    #creating a biplot
    
      pr.iris <- prcomp(x = iris[-5], scale = FALSE, center = TRUE)
      
      biplot (pr.iris)

__Scree plot__

. Scree plot for PCA either shows the proportion of variance explained by each principal component or the cumulative percentage of variance as the number of principal components as the number of principal component increases.

. When number of PCs and number of original features are the same, the cummulative proportion of variance
  explained is 1.
  
      # getting proportion of variance for a scree plot
      
          pr.var <- pr.iris$sdev^2
          
              . sdev : the standard deviation of each principal component
              . Because we want variance --> sdev^2 
              
          pve <- pr.var / sum(pr.var)
        
      # plot variance explained for each principal componet
      
          plot(pve, xlab = "Principal Componet", 
                    ylab = "Proportion of Variance Explained" ,
                    ylim = c(0, 1), type = "b")

-------------------------------------------------------------------------------------------------------

**Interpreting biplots (1)**

As stated in the video, the biplot() function plots both the principal components loadings and 
the mapping of the observations to their first two principal component values. The next couple of 
exercises will check your interpretation of the biplot() visualization.

Using the biplot() of the pr.out model, which two original variables have approximately the same loadings in the first two principal components?

--> biplot(pr.out)
--> Attack and HitPoints

------------------------------------------------------------------------------------------------------

**Interpreting biplots (2)**

In the last exercise, you saw that Attack and HitPoints have approximately the same loadings in the 
first two principal components.

Again using the biplot() of the pr.out model, which two Pokemon are the least similar in terms of the second principal component?

--> Kadabra and Torkoal

------------------------------------------------------------------------------------------------------

**Variance explained**

The second common plot type for understanding PCA models is a scree plot. A scree plot shows the 
variance explained as the number of principal components increases. Sometimes the cumulative variance
explained is plotted as well.

In this and the next exercise, you will prepare data from the pr.out model you created at the beginning of the chapter for use in a scree plot. Preparing the data for plotting is required because there is not a built-in function in R to create this type of plot.


pr.out and the pokemon data are still available in your workspace.

- Assign to the variable pr.var the square of the standard deviations of the principal components 
    (i.e. the variance). The standard deviation of the principal components is available in the sdev
    component of the PCA model object.
- Assign to the variable pve the proportion of the variance explained, calculated by dividing pr.var 
    by the total variance explained by all principal components.

-------------------------------------------------------------------------------------------

# Variability of each principal component: pr.var
(pr.var <- pr.out$sdev^2)
> [1] 2.0795013 1.0026331 0.6306284 0.2872372

# Variance explained by each principal component: pve
(pve <- pr.var/ sum(pr.var))
> [1] 0.51987532 0.25065826 0.15765710 0.07180931

--------------------------------------------------------------------------------------------

**Visualize variance explained**

Now you will create a scree plot showing the proportion of variance explained by each principal component,
  as well as the cumulative proportion of variance explained.

Recall from the video that these plots can help to determine the number of principal components to retain.
  One way to determine the number of principal components to retain is by looking for an elbow in the 
  scree plot showing that as the number of principal components increases, the rate at which variance 
  is explained decreases substantially. In the absence of a clear elbow, you can use the scree plot as
  a guide for setting a threshold.

The proportion of variance explained is still available in the pve object you created in the last exercise.

- Use plot() to plot the proportion of variance explained by each principal component.
- Use plot() and cumsum() (cumulative sum) to plot the cumulative proportion of variance explained as 
    a function of the number principal components.
    
---------------------------------------------------------------------------------------------

# Plot variance explained for each principal component
plot(pve, xlab = "Principal Component",
       ylab = "Proportion of Variance Explained",
       ylim = c(0, 1), type = "b")
 
# Plot cumulative proportion of variance explained
plot(cumsum(pve), xlab = "Principal Component",
       ylab = "Cumulative Proportion of Variance Explained",
       ylim = c(0, 1), type = "b")

------------------------------------------------------------------------------------------------

_______________________________
## Practical issues with PCA ##
_______________________________

. Scaling the data
. Missing values:
    - Drop observations with missing values
    - Impute/estimate missing value
. Categorical data
    - Do not include the categorical data features
    - Encode categorical features as number
    
-----------------------------------------------------------------------------------------------------

**Practical issues: scaling**

You saw in the video that scaling your data before doing PCA changes the results of the PCA modeling. 
Here, you will perform PCA with and without scaling, then visualize the results using biplots.

Sometimes scaling is appropriate when the variances of the variables are substantially different. 
This is commonly the case when variables have different units of measurement, for example, 
degrees Fahrenheit (temperature) and miles (distance). Making the decision to use scaling is 
an important step in performing a principal component analysis.

The same Pokemon dataset is available in your workspace as pokemon, but one new variable has been added:
Total.

- There is some code at the top of the editor to calculate the mean and standard deviation of 
    each variable in the model. Run this code to see how the scale of the variables differs in 
    the original data.
- Create a PCA model of pokemon with scaling, assigning the result to pr.with.scaling.
- Create a PCA model of pokemon without scaling, assigning the result to pr.without.scaling.
- Use biplot() to plot both models (one at a time) and compare their outputs.

-----------------------------------------------------------------------------------------------------


# Mean of each variable
colMeans(pokemon)
>  Total HitPoints    Attack   Defense     Speed 
   448.82     71.08     81.22     78.44     66.58
 
# Standard deviation of each variable
apply(pokemon, 2, sd)
>  Total HitPoints    Attack   Defense     Speed 
119.32321  25.62193  33.03078  32.05809  27.51036
 
# PCA model with scaling: pr.with.scaling
> pr.with.scaling <- prcomp(pokemon, scale = TRUE)
 
# PCA model without scaling: pr.without.scaling
pr.without.scaling <- prcomp(pokemon, scale = FALSE)
 
# Create biplots of both for comparison
biplot(pr.with.scaling)
biplot(pr.without.scaling)

------------------------------------------------------------------------------------------------------

